---
title: "Generative AI Models"
subtitle: A non-technical introduction to GenAI models, their capabilities, and their shortcomings 
---

## What Happens When You Prompt

When you type a question into ChatGPT or Claude, here's what actually happens:

1. **Tokenization:** Your prompt is converted into a sequence of numbers. Text is split into "tokens"—not exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.

2. **Context assembly:** Your prompt is combined with any previous messages in the conversation, plus a "system prompt" (instructions the model provider includes automatically).
    - **Context Window**: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.

3. **Forward pass:** The tokenized input goes through the model's neural network. Each layer does mathematical operations—essentially matrix multiplications—transforming the input through the billions of parameters.

4. **Output generation:** The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by "temperature"), adds that token to the sequence, and repeats until it ends up sampling a "stop" token.
    - **Temperature**: Controls randomness in selection:
        - Temperature = 0: Always pick the highest-probability token (deterministic)
        - Temperature = 1: Sample proportionally to probabilities  
        - Temperature > 1: Flatten the distribution (more random/creative)

5. **Detokenization:** Once a "stop" token is reached, the output tokens are converted back to human-readable text.

```{mermaid}
flowchart LR
    subgraph Input
        A["User Prompt"]
        S["System Prompt"]
    end
    
    subgraph Tokenization
        B["Tokenizer"]
        C["Token IDs"]
    end
    
    subgraph Processing
        D["Context Window"]
        E["LLM"]
    end
    
    subgraph Output
        F{"Probability\nDistribution"}
        G["Temperature"]
        H["Selected Token"]
    end
    
    A --> B
    S --> B
    B --> C --> D --> E --> F --> G --> H
    H -->|"Append & repeat"| D
    linkStyle 8 stroke-dasharray: 5 5
```

## What is an LLM?

**A large language model is just a file full of numbers.**

When companies like OpenAI, Anthropic, or Google train a model, they're creating a very large file—hundreds of gigabytes for frontier models—that contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.

That is what the model "knows"—statistical patterns about how language works and what tends to follow what.

### How the Numbers Are Obtained

The training process happens in stages:

**Pre-training:** The model reads massive amounts of text from the internet, books, code repositories, academic papers—essentially a substantial portion of human written output. During this phase, it learns to predict "what word comes next?" billions of times. The parameters adjust to get better at prediction. This takes months and costs tens to hundreds of millions of dollars in compute.

**Post-training (Fine-tuning):** The base model is then adjusted using:

- **Supervised fine-tuning (SFT):** Humans write example conversations showing how the model should respond
- **Reinforcement learning from human feedback (RLHF):** Humans rate model outputs, and the model learns to generate outputs that get higher ratings
- **Constitutional AI (CAI):** Used by Anthropic, where the model learns to critique and revise its own outputs based on principles

**Continuous refinement:** Models are updated regularly to improve performance, fix issues, and add capabilities.

Key terms you might hear: "transformer architecture" (the mathematical structure these models use), "attention mechanism" (how the model decides what parts of the input to focus on), "parameters" or "weights" (the numbers in the file).

---

## The Context Window

The context window is the maximum amount of text (and other inputs) the model can consider at once:  
**(system instructions + your prompt + chat history + retrieved snippets + tool outputs + file excerpts) + the model’s output.**

If the total exceeds the limit:

- the system must drop, summarize, or “compact” something,
- or it returns an error (depending on provider).

### What providers may automatically include in the context window (even if you don’t see it)

Many modern assistants can add extra context such as:

- system safety instructions,
- your custom instructions,
- project instructions,
- “memory” items (facts it saved about your preferences),
- snippets retrieved from uploaded files or connected tools.

This is why two people can ask the “same prompt” and get slightly different results.

### Uploading files with your prompt

In most systems, uploading a file does **not** mean the entire file is placed into the context window verbatim.

A common approach is **retrieval**:

- the file is chunked,
- the system builds embeddings (a searchable representation) from each chunk 
- and only relevant chunks are pulled into the context window when needed (based on what you asked about the file)

### What happens when you near the limit of the context window 

Different providers handle this differently:

- Some UIs summarize older messages.
- Some drop the earliest conversation turns (“truncation”).
- Some run a compaction step to preserve key details.

### Output limits
Even with a very large **input** window, models have **output caps**. When you ask for “write 30 pages,” you usually get:

- a truncated response,
- or a refusal,
- or “here is an outline; ask me to expand section by section.”

**Recommendation:**  
For long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2…  
This yields better control and fewer errors.

## Frontier GenAI Models are Multimodal 

Models like GPT 5.2 and Gemini 3.0 are multimodal: they can input and output text, audio, images, and videos.

TODO:

- explain that they still input and output tokens (just with a different representation)
- mention names of specialized models like Nano Banana Pro, and Sora (anything for speach?) and how they are integrated with the main models (GPT 5.2, Gemini 3.0 etc.) 

## Main Shortcomings 

- Long-term memory (context window is short-term memory)
- Halucinations
- Inability to act (exert change on external systems)

The "Agents" pages describes modern advances in addressing these shortcomings.