---
title: "Generative AI Models"
subtitle: A non-technical introduction to GenAI models, their capabilities, and their shortcomings 
---


## What is an LLM?

**A large language model is just a file full of numbers.**

When companies like OpenAI, Anthropic, or Google train a model, they're creating a very large file—hundreds of gigabytes for frontier models—that contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.

That is what the model "knows"—statistical patterns about how language works and what tends to follow what.

### How the Numbers Are Obtained

Modern frontier models like GPT 5.2 and Claude Opus 4.5 are built through a multi-stage training pipeline. Each stage shapes the model's capabilities in different ways.

#### Stage 1: Pre-training

The foundation of every LLM is **next-token prediction** on massive text datasets. The model reads text from the internet, books, code repositories, academic papers, and other sources—a substantial portion of human written output. For each position in the text, it tries to predict what token comes next. When it guesses wrong, the parameters adjust slightly (via **stochastic gradient descent**) to make a better prediction next time. This process repeats trillions of times.

Pre-training is expensive—months of compute time on thousands of GPUs, costing hundreds of millions of dollars. But the result is a "base model" that has absorbed vast knowledge about language, facts, reasoning patterns, and code.

#### Stage 2: Post-training

The base model can complete text, but it doesn't know how to be a helpful assistant. Post-training shapes the model's behavior:

**Supervised fine-tuning (SFT):** Humans write thousands of example conversations demonstrating ideal assistant behavior—how to respond to questions, handle ambiguity, refuse harmful requests, and maintain a helpful tone. The model learns to imitate these examples.

**Reinforcement learning from human feedback (RLHF):** Humans compare pairs of model responses and indicate which is better. These preferences train a "reward model" that can score any response. The LLM then optimizes to produce responses that score highly—learning to be more helpful, accurate, and aligned with human values.

#### Stage 3: Reasoning Enhancement (for Thinking Models)

Standard post-training produces capable assistants, but they still struggle with complex reasoning. Models like GPT 5.2 and Claude Opus 4.5 undergo additional training specifically for multi-step reasoning:

**Reinforcement learning on reasoning tasks:** The model is given problems with verifiable answers (math, logic, coding challenges). It generates chain-of-thought reasoning, and the training process rewards chains that lead to correct answers. Over many iterations, the model learns *how to think*—which reasoning strategies work, when to reconsider, and how to break down complex problems.

**Process reward models:** Rather than just rewarding correct final answers, these models learn to evaluate each step of the reasoning process. This teaches the model to build sound arguments step-by-step, not just pattern-match to answers.

**Synthetic data and self-play:** Models generate their own training data—producing reasoning traces, solutions, and even new problems. The best outputs become training examples, creating a flywheel of improvement.

#### Continuous Refinement

Deployed models continue to improve through ongoing fine-tuning, bug fixes, and capability additions. The model you use today may be subtly different from last month's version.

<!--
OLD VERSION:

The training process happens in stages:

**Pre-training:** The model reads massive amounts of text from the internet, books, code repositories, academic papers—essentially a substantial portion of human written output. During this phase, it learns to predict "what word comes next?" billions of times. The parameters adjust to get better at prediction. This takes months and costs hundreds of millions of dollars in compute.

**Post-training (Fine-tuning):** The base model is then adjusted using:

- **Supervised fine-tuning (SFT):** Humans write example conversations showing how the model should respond
- **Reinforcement learning from human feedback (RLHF):** Humans rate model outputs, and the model learns to generate outputs that get higher ratings
- **Constitutional AI (CAI):** Used by Anthropic, where the model learns to critique and revise its own outputs based on principles

**Continuous refinement:** Models are updated regularly to improve performance, fix issues, and add capabilities.

Key terms you might hear: "transformer architecture" (the mathematical structure these models use), "attention mechanism" (how the model decides what parts of the input to focus on), "parameters" or "weights" (the numbers in the file).
-->


## What Happens When You Prompt

When you type a question into ChatGPT or Claude, here's what actually happens:

1. **Tokenization:** Your prompt is converted into a sequence of numbers. Text is split into "tokens"—not exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.

2. **Context assembly:** Your prompt is combined with any previous messages in the conversation, plus a "system prompt" (instructions the model provider includes automatically).
    - **Context Window**: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.

3. **Forward pass:** The tokenized input goes through the model's neural network. Each layer does mathematical operations—essentially matrix multiplications—transforming the input through the billions of parameters.

4. **Output generation:** The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by "temperature"), adds that token to the sequence, and repeats until it ends up sampling a "stop" token.
    - **Temperature**: Controls randomness in selection:
        - Temperature = 0: Always pick the highest-probability token (deterministic)
        - Temperature = 1: Sample proportionally to probabilities  
        - Temperature > 1: Flatten the distribution (more random/creative)

5. **Detokenization:** Once a "stop" token is reached, the output tokens are converted back to human-readable text.

```{mermaid}
flowchart LR
    subgraph Input
        A["User Prompt"]
        S["System Prompt"]
    end
    
    subgraph Tokenization
        B["Tokenizer"]
        C["Token IDs"]
    end
    
    subgraph Processing
        D["Context Window"]
        E["LLM"]
    end
    
    subgraph Output
        F{"Probability\nDistribution"}
        G["Temperature"]
        H["Selected Token"]
    end
    
    A --> B
    S --> B
    B --> C --> D --> E --> F --> G --> H
    H -->|"Append & repeat"| D
    linkStyle 8 stroke-dasharray: 5 5
```

## The Context Window

The context window is the maximum amount of text (and other inputs) the model can consider at once. This includes more than just your prompt:  

- system instructions 
- your prompt 
- chat history 
- retrieved snippets 
- tool outputs 
- file excerpts
- the model’s output

If the total exceeds the limit:

- the system must drop, summarize, or “compact” something,
- or it returns an error (depending on provider).

### What may be automatically include in the context window

Many modern assistants can add extra context such as:

- system safety instructions
- your custom instructions
- project instructions
- “memory” items (facts it saved about your preferences)
- snippets retrieved from uploaded files or connected tools

This is why two people can ask the “same prompt” and get slightly different results.

### Uploading files with your prompt

In most systems, uploading a file does **not** mean the entire file is placed into the context window verbatim.

A common approach is **retrieval** (see "RAG" on [Agents](03_agents.qmd) page):

- the file is chunked,
- the system builds embeddings (a searchable representation) from each chunk 
- and only relevant chunks are pulled into the context window when needed (based on what you asked about the file)

### What happens when you near the limit of the context window 

Different providers handle this differently:

- Some UIs summarize older messages
- Some drop the earliest conversation turns (“truncation”)
- Some run a compaction step to preserve key details

### Output limits

Even with a very large **input** window, models have **output caps**. When you ask for “write 30 pages,” you usually get a truncated response, a refusal, or something like “here is an outline; ask me to expand section by section.”

For long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2 etc. This yields better control and fewer errors.

## Multimodality 

Frontier models like GPT 5.2 and Gemini 3.0 are multimodal: **they can process text, audio, images, and videos.**


### It's still tokens in, tokens out

Even when processing images, audio, or video, these models still work with tokens—just not word tokens. Different modalities get converted into numerical representations:

- **Images** are divided into patches (small grid sections), and each patch becomes one or more tokens
- **Audio** is converted into spectrograms or waveform segments, then tokenized
- **Video** combines image tokens (frames) with temporal information

The model processes all these token types through the same transformer architecture. This is why you can ask questions about an image in natural language—the image tokens and text tokens flow through the same neural network.

### Specialized model orchestration

The frontier models you interact with often orchestrate multiple specialized models behind the scenes:

- image generation
    - GPT Image 1.5, Nano Banana Pro 
- video generation
    - Sora 2, Veo 3.1 
- speach generation (text-to-speach)
    - GPT-4o Mini TTS, Gemini 2.5 Pro Preview TTS 

When you ask GPT 5.2 to "generate an image of a sunset over mountains," the language model interprets your request, formulates a detailed prompt, and hands it off to GPT Image 1.5. The result is passed back and presented as part of the conversation.

This modular architecture allows each component to be optimized for its specific task while providing a unified conversational interface.

## Main Shortcomings 

Despite their impressive capabilities, current LLMs have fundamental limitations:

### Knowledge Cutoff

LLMs are trained on data up to a certain date. They don't know about recent events, updated policies, or new research.

### Limited long-term memory

As we discussed above, models have limited context windows (short-term memory). Some systems like ChatGPT implement "memory" features that save key facts about your conversations to a database and inject them into future prompts. But this is bolted-on storage, not true learning—the model's parameters don't change based on your interactions.

### Hallucinations

LLMs can generate confident, fluent text that is factually incorrect. Models don't "know" things. They are optimized for producing probable sounding text, not verified truth. This means they can:

- Invent citations that don't exist
- State incorrect facts with complete confidence
- Mix accurate and inaccurate information seamlessly

### Inability to act 

GenAI models ingest tokens and output tokens. The can not exert change on external systems ("act"). They cannot send emails, execute code, query databases, browse the web, modify files. The model can *describe* how to do these things, but it cannot actually perform them without external systems.

**This is where agents come in:** By connecting LLMs to tools (APIs, code interpreters, file systems), we can create systems that can take real-world actions. The [Agents](03_agents.qmd) section describes how modern frameworks address these limitations by giving models the ability to plan, use tools, and affect external systems.