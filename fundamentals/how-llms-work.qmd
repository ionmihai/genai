---
title: "How GenAI Models Work"
subtitle: A non-technical introduction
---

## What Happens When You Prompt

When you type a question into ChatGPT or Claude, here's what actually happens:

1. **Tokenization:** Your prompt is converted into a sequence of numbers. Text is split into "tokens"—not exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.

2. **Context assembly:** Your prompt is combined with any previous messages in the conversation, plus a "system prompt" (instructions the model provider includes automatically).
    - **Context Window**: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.

3. **Forward pass:** The tokenized input goes through the model's neural network. Each layer does mathematical operations—essentially matrix multiplications—transforming the input through the billions of parameters.

4. **Output generation:** The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by "temperature"), adds that token to the sequence, and repeats until it ends up sampling a "stop" token.
    - **Temperature**: Controls randomness in selection:
        - Temperature = 0: Always pick the highest-probability token (deterministic)
        - Temperature = 1: Sample proportionally to probabilities  
        - Temperature > 1: Flatten the distribution (more random/creative)

5. **Detokenization:** Once a "stop" token is reached, the output tokens are converted back to human-readable text.

```{mermaid}
flowchart LR
    subgraph Input
        A["User Prompt"]
        S["System Prompt"]
    end
    
    subgraph Tokenization
        B["Tokenizer"]
        C["Token IDs"]
    end
    
    subgraph Processing
        D["Context Window"]
        E["LLM"]
    end
    
    subgraph Output
        F{"Probability\nDistribution"}
        G["Temperature"]
        H["Selected Token"]
    end
    
    A --> B
    S --> B
    B --> C --> D --> E --> F --> G --> H
    H -->|"Append & repeat"| D
    linkStyle 8 stroke-dasharray: 5 5
```

## The Model

**A large language model is just a file full of numbers.**

When companies like OpenAI, Anthropic, or Google train a model, they're creating a very large file—hundreds of gigabytes for frontier models—that contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.

That is what the model "knows"—statistical patterns about how language works and what tends to follow what.

### How the Numbers Are Obtained

The training process happens in stages:

**Pre-training:** The model reads massive amounts of text from the internet, books, code repositories, academic papers—essentially a substantial portion of human written output. During this phase, it learns to predict "what word comes next?" billions of times. The parameters adjust to get better at prediction. This takes months and costs tens to hundreds of millions of dollars in compute.

**Post-training (Fine-tuning):** The base model is then adjusted using:

- **Supervised fine-tuning (SFT):** Humans write example conversations showing how the model should respond
- **Reinforcement learning from human feedback (RLHF):** Humans rate model outputs, and the model learns to generate outputs that get higher ratings
- **Constitutional AI (CAI):** Used by Anthropic, where the model learns to critique and revise its own outputs based on principles

**Continuous refinement:** Models are updated regularly to improve performance, fix issues, and add capabilities.

Key terms you might hear: "transformer architecture" (the mathematical structure these models use), "attention mechanism" (how the model decides what parts of the input to focus on), "parameters" or "weights" (the numbers in the file).

---

## The Context Window

The context window is the maximum amount of text (and other inputs) the model can consider at once:  
**(system instructions + your prompt + chat history + retrieved snippets + tool outputs + file excerpts) + the model’s output.**

If the total exceeds the limit:

- the system must drop, summarize, or “compact” something,
- or it returns an error (depending on provider).

### What providers may automatically include in the context window (even if you don’t see it)

Many modern assistants can add extra context such as:

- system safety instructions,
- your custom instructions,
- project instructions,
- “memory” items (facts it saved about your preferences),
- snippets retrieved from uploaded files or connected tools.

This is why two people can ask the “same prompt” and get slightly different results.

### Uploading files with your prompt

In most systems, uploading a file does **not** mean the entire file is placed into the context window verbatim.

A common approach is **retrieval**:

- the file is chunked,
- the system builds embeddings (a searchable representation) from each chunk 
- and only relevant chunks are pulled into the context window when needed (based on what you asked about the file)

### What happens when you near the limit of the context window 

Different providers handle this differently:

- Some UIs summarize older messages.
- Some drop the earliest conversation turns (“truncation”).
- Some run a compaction step to preserve key details.

### Output limits
Even with a very large **input** window, models have **output caps**. When you ask for “write 30 pages,” you usually get:

- a truncated response,
- or a refusal,
- or “here is an outline; ask me to expand section by section.”

**Recommendation:**  
For long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2…  
This yields better control and fewer errors.

## "Thinking" (Planning) Models and Tool Use

### Thinking Models

Standard LLMs generate output token-by-token in a single pass—they don't "stop and think." **Thinking models** (like OpenAI's o1/o3, Claude with extended thinking, or DeepSeek R1) add an explicit reasoning phase before producing their final answer.

How it works:

1. The model receives your prompt
2. Before answering, it generates a **chain of thought**—a series of reasoning steps that break down the problem
    - In modern systems, this thinking is visible to you and it's a good idea to inspect it and see if you can spot issues with it
    - For some producs like ChatGPT Pro Deep Research, you can **pause** the thinking/execution loop if you want to correct some reasoning errors or want to add more context  
4. The model can reconsider, catch errors, and refine its approach during this phase
5. Only after thinking does it produce the final response

**Trade-offs:** Thinking models are better at complex reasoning (math, logic, multi-step problems) but are slower and more expensive because they generate many more tokens internally.

### Tool Use

LLMs can only generate tokens—they can't browse the web, run code, or access databases. "Tool use" (also called "function calling") extends models to interact with external systems.

How it works:

1. The system prompt tells the model what tools are available (e.g., "You can search the web" or "You can run Python code")
2. Instead of generating text, the model can output a structured **tool call** (e.g., `search("current weather in NYC")`, where `search` is a Python function somewhere on an OpenAI server)
3. The system executes the tool and returns the result to the model
4. The model incorporates the result into its context and continues generating
5. This loop can repeat—models can chain multiple tool calls to accomplish complex tasks

**Why it matters:** Tool use lets models access current information (web search), verify calculations (code execution), and take actions in the real world (APIs). It transforms LLMs from text generators into agents that can accomplish tasks.

```{mermaid}
flowchart LR
    subgraph Input
        A["User Prompt"]
        S["System Prompt\n+ Available Tools"]
    end
    
    subgraph Processing
        D["Context Window"]
        E["LLM"]
    end
    
    subgraph "LLM Output"
        F{"Output\nType?"}
        T["Thinking\nTokens"]
        G["Tool Call"]
        H["Final\nResponse"]
    end
    
    subgraph Tools
        X["Execute Tool"]
        R["Tool Result"]
    end
    
    A --> D
    S --> D
    D --> E --> F
    F -->|"Reasoning"| T
    T -->|"Append"| D
    F -->|"Tool"| G --> X --> R
    R -->|"Append"| D
    F -->|"Done"| H
    linkStyle 5 stroke-dasharray: 5 5
    linkStyle 9 stroke-dasharray: 5 5
```

The key difference from the basic pipeline (at the top of this page): thinking models generate reasoning tokens as *output* that gets appended to the context, creating a loop where the model "thinks out loud" before producing its final answer. Tool-enabled models can similarly pause, call external tools, and incorporate results before continuing.

