<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>GenAI in Academia - Generative AI Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="mermaid-theme" content="neutral">
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../fundamentals/01_ecosystem.html">Fundamentals of GenAI</a></li><li class="breadcrumb-item"><a href="../fundamentals/02_models.html">Models</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">GenAI in Academia</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Fundamentals of GenAI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/01_ecosystem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Ecosystem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/02_models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/03_agents.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Agents</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/04_products.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Products</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Resources</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-an-llm" id="toc-what-is-an-llm" class="nav-link active" data-scroll-target="#what-is-an-llm">What is an LLM?</a>
  <ul class="collapse">
  <li><a href="#how-the-numbers-are-obtained" id="toc-how-the-numbers-are-obtained" class="nav-link" data-scroll-target="#how-the-numbers-are-obtained">How the Numbers Are Obtained</a></li>
  </ul></li>
  <li><a href="#what-happens-when-you-prompt" id="toc-what-happens-when-you-prompt" class="nav-link" data-scroll-target="#what-happens-when-you-prompt">What Happens When You Prompt</a></li>
  <li><a href="#the-context-window" id="toc-the-context-window" class="nav-link" data-scroll-target="#the-context-window">The Context Window</a>
  <ul class="collapse">
  <li><a href="#what-may-be-automatically-include-in-the-context-window" id="toc-what-may-be-automatically-include-in-the-context-window" class="nav-link" data-scroll-target="#what-may-be-automatically-include-in-the-context-window">What may be automatically include in the context window</a></li>
  <li><a href="#uploading-files-with-your-prompt" id="toc-uploading-files-with-your-prompt" class="nav-link" data-scroll-target="#uploading-files-with-your-prompt">Uploading files with your prompt</a></li>
  <li><a href="#what-happens-when-you-near-the-limit-of-the-context-window" id="toc-what-happens-when-you-near-the-limit-of-the-context-window" class="nav-link" data-scroll-target="#what-happens-when-you-near-the-limit-of-the-context-window">What happens when you near the limit of the context window</a></li>
  <li><a href="#output-limits" id="toc-output-limits" class="nav-link" data-scroll-target="#output-limits">Output limits</a></li>
  </ul></li>
  <li><a href="#multimodality" id="toc-multimodality" class="nav-link" data-scroll-target="#multimodality">Multimodality</a>
  <ul class="collapse">
  <li><a href="#its-still-tokens-in-tokens-out" id="toc-its-still-tokens-in-tokens-out" class="nav-link" data-scroll-target="#its-still-tokens-in-tokens-out">It’s still tokens in, tokens out</a></li>
  <li><a href="#specialized-model-orchestration" id="toc-specialized-model-orchestration" class="nav-link" data-scroll-target="#specialized-model-orchestration">Specialized model orchestration</a></li>
  </ul></li>
  <li><a href="#main-shortcomings" id="toc-main-shortcomings" class="nav-link" data-scroll-target="#main-shortcomings">Main Shortcomings</a>
  <ul class="collapse">
  <li><a href="#knowledge-cutoff" id="toc-knowledge-cutoff" class="nav-link" data-scroll-target="#knowledge-cutoff">Knowledge Cutoff</a></li>
  <li><a href="#limited-long-term-memory" id="toc-limited-long-term-memory" class="nav-link" data-scroll-target="#limited-long-term-memory">Limited long-term memory</a></li>
  <li><a href="#hallucinations" id="toc-hallucinations" class="nav-link" data-scroll-target="#hallucinations">Hallucinations</a></li>
  <li><a href="#inability-to-act" id="toc-inability-to-act" class="nav-link" data-scroll-target="#inability-to-act">Inability to act</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Generative AI Models</h1>
<p class="subtitle lead">A non-technical introduction to GenAI models, their capabilities, and their shortcomings</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="what-is-an-llm" class="level2">
<h2 class="anchored" data-anchor-id="what-is-an-llm">What is an LLM?</h2>
<p><strong>A large language model is just a file full of numbers.</strong></p>
<p>When companies like OpenAI, Anthropic, or Google train a model, they’re creating a very large file—hundreds of gigabytes for frontier models—that contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.</p>
<p>That is what the model “knows”—statistical patterns about how language works and what tends to follow what.</p>
<section id="how-the-numbers-are-obtained" class="level3">
<h3 class="anchored" data-anchor-id="how-the-numbers-are-obtained">How the Numbers Are Obtained</h3>
<p>Modern frontier models like GPT 5.2 and Claude Opus 4.5 are built through a multi-stage training pipeline. Each stage shapes the model’s capabilities in different ways.</p>
<section id="stage-1-pre-training" class="level4">
<h4 class="anchored" data-anchor-id="stage-1-pre-training">Stage 1: Pre-training</h4>
<p>The foundation of every LLM is <strong>next-token prediction</strong> on massive text datasets. The model reads text from the internet, books, code repositories, academic papers, and other sources—a substantial portion of human written output. For each position in the text, it tries to predict what token comes next. When it guesses wrong, the parameters adjust slightly (via <strong>stochastic gradient descent</strong>) to make a better prediction next time. This process repeats trillions of times.</p>
<p>Pre-training is expensive—months of compute time on thousands of GPUs, costing hundreds of millions of dollars. But the result is a “base model” that has absorbed vast knowledge about language, facts, reasoning patterns, and code.</p>
</section>
<section id="stage-2-post-training" class="level4">
<h4 class="anchored" data-anchor-id="stage-2-post-training">Stage 2: Post-training</h4>
<p>The base model can complete text, but it doesn’t know how to be a helpful assistant. Post-training shapes the model’s behavior:</p>
<p><strong>Supervised fine-tuning (SFT):</strong> Humans write thousands of example conversations demonstrating ideal assistant behavior—how to respond to questions, handle ambiguity, refuse harmful requests, and maintain a helpful tone. The model learns to imitate these examples.</p>
<p><strong>Reinforcement learning from human feedback (RLHF):</strong> Humans compare pairs of model responses and indicate which is better. These preferences train a “reward model” that can score any response. The LLM then optimizes to produce responses that score highly—learning to be more helpful, accurate, and aligned with human values.</p>
</section>
<section id="stage-3-reasoning-enhancement-for-thinking-models" class="level4">
<h4 class="anchored" data-anchor-id="stage-3-reasoning-enhancement-for-thinking-models">Stage 3: Reasoning Enhancement (for Thinking Models)</h4>
<p>Standard post-training produces capable assistants, but they still struggle with complex reasoning. Models like GPT 5.2 and Claude Opus 4.5 undergo additional training specifically for multi-step reasoning:</p>
<p><strong>Reinforcement learning on reasoning tasks:</strong> The model is given problems with verifiable answers (math, logic, coding challenges). It generates chain-of-thought reasoning, and the training process rewards chains that lead to correct answers. Over many iterations, the model learns <em>how to think</em>—which reasoning strategies work, when to reconsider, and how to break down complex problems.</p>
<p><strong>Process reward models:</strong> Rather than just rewarding correct final answers, these models learn to evaluate each step of the reasoning process. This teaches the model to build sound arguments step-by-step, not just pattern-match to answers.</p>
<p><strong>Synthetic data and self-play:</strong> Models generate their own training data—producing reasoning traces, solutions, and even new problems. The best outputs become training examples, creating a flywheel of improvement.</p>
</section>
<section id="continuous-refinement" class="level4">
<h4 class="anchored" data-anchor-id="continuous-refinement">Continuous Refinement</h4>
<p>Deployed models continue to improve through ongoing fine-tuning, bug fixes, and capability additions. The model you use today may be subtly different from last month’s version.</p>
<!--
OLD VERSION:

The training process happens in stages:

**Pre-training:** The model reads massive amounts of text from the internet, books, code repositories, academic papers—essentially a substantial portion of human written output. During this phase, it learns to predict "what word comes next?" billions of times. The parameters adjust to get better at prediction. This takes months and costs hundreds of millions of dollars in compute.

**Post-training (Fine-tuning):** The base model is then adjusted using:

- **Supervised fine-tuning (SFT):** Humans write example conversations showing how the model should respond
- **Reinforcement learning from human feedback (RLHF):** Humans rate model outputs, and the model learns to generate outputs that get higher ratings
- **Constitutional AI (CAI):** Used by Anthropic, where the model learns to critique and revise its own outputs based on principles

**Continuous refinement:** Models are updated regularly to improve performance, fix issues, and add capabilities.

Key terms you might hear: "transformer architecture" (the mathematical structure these models use), "attention mechanism" (how the model decides what parts of the input to focus on), "parameters" or "weights" (the numbers in the file).
-->
</section>
</section>
</section>
<section id="what-happens-when-you-prompt" class="level2">
<h2 class="anchored" data-anchor-id="what-happens-when-you-prompt">What Happens When You Prompt</h2>
<p>When you type a question into ChatGPT or Claude, here’s what actually happens:</p>
<ol type="1">
<li><p><strong>Tokenization:</strong> Your prompt is converted into a sequence of numbers. Text is split into “tokens”—not exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.</p></li>
<li><p><strong>Context assembly:</strong> Your prompt is combined with any previous messages in the conversation, plus a “system prompt” (instructions the model provider includes automatically).</p>
<ul>
<li><strong>Context Window</strong>: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.</li>
</ul></li>
<li><p><strong>Forward pass:</strong> The tokenized input goes through the model’s neural network. Each layer does mathematical operations—essentially matrix multiplications—transforming the input through the billions of parameters.</p></li>
<li><p><strong>Output generation:</strong> The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by “temperature”), adds that token to the sequence, and repeats until it ends up sampling a “stop” token.</p>
<ul>
<li><strong>Temperature</strong>: Controls randomness in selection:
<ul>
<li>Temperature = 0: Always pick the highest-probability token (deterministic)</li>
<li>Temperature = 1: Sample proportionally to probabilities<br>
</li>
<li>Temperature &gt; 1: Flatten the distribution (more random/creative)</li>
</ul></li>
</ul></li>
<li><p><strong>Detokenization:</strong> Once a “stop” token is reached, the output tokens are converted back to human-readable text.</p></li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph Input
        A["User Prompt"]
        S["System Prompt"]
    end
    
    subgraph Tokenization
        B["Tokenizer"]
        C["Token IDs"]
    end
    
    subgraph Processing
        D["Context Window"]
        E["LLM"]
    end
    
    subgraph Output
        F{"Probability\nDistribution"}
        G["Temperature"]
        H["Selected Token"]
    end
    
    A --&gt; B
    S --&gt; B
    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H
    H --&gt;|"Append &amp; repeat"| D
    linkStyle 8 stroke-dasharray: 5 5
</pre>
</div>
</div>
</div>
</div>
</section>
<section id="the-context-window" class="level2">
<h2 class="anchored" data-anchor-id="the-context-window">The Context Window</h2>
<p>The context window is the maximum amount of text (and other inputs) the model can consider at once. This includes more than just your prompt:</p>
<ul>
<li>system instructions</li>
<li>your prompt</li>
<li>chat history</li>
<li>retrieved snippets</li>
<li>tool outputs</li>
<li>file excerpts</li>
<li>the model’s output</li>
</ul>
<p>If the total exceeds the limit:</p>
<ul>
<li>the system must drop, summarize, or “compact” something,</li>
<li>or it returns an error (depending on provider).</li>
</ul>
<section id="what-may-be-automatically-include-in-the-context-window" class="level3">
<h3 class="anchored" data-anchor-id="what-may-be-automatically-include-in-the-context-window">What may be automatically include in the context window</h3>
<p>Many modern assistants can add extra context such as:</p>
<ul>
<li>system safety instructions</li>
<li>your custom instructions</li>
<li>project instructions</li>
<li>“memory” items (facts it saved about your preferences)</li>
<li>snippets retrieved from uploaded files or connected tools</li>
</ul>
<p>This is why two people can ask the “same prompt” and get slightly different results.</p>
</section>
<section id="uploading-files-with-your-prompt" class="level3">
<h3 class="anchored" data-anchor-id="uploading-files-with-your-prompt">Uploading files with your prompt</h3>
<p>In most systems, uploading a file does <strong>not</strong> mean the entire file is placed into the context window verbatim.</p>
<p>A common approach is <strong>retrieval</strong> (see “RAG” on <a href="../fundamentals/03_agents.html">Agents</a> page):</p>
<ul>
<li>the file is chunked,</li>
<li>the system builds embeddings (a searchable representation) from each chunk</li>
<li>and only relevant chunks are pulled into the context window when needed (based on what you asked about the file)</li>
</ul>
</section>
<section id="what-happens-when-you-near-the-limit-of-the-context-window" class="level3">
<h3 class="anchored" data-anchor-id="what-happens-when-you-near-the-limit-of-the-context-window">What happens when you near the limit of the context window</h3>
<p>Different providers handle this differently:</p>
<ul>
<li>Some UIs summarize older messages</li>
<li>Some drop the earliest conversation turns (“truncation”)</li>
<li>Some run a compaction step to preserve key details</li>
</ul>
</section>
<section id="output-limits" class="level3">
<h3 class="anchored" data-anchor-id="output-limits">Output limits</h3>
<p>Even with a very large <strong>input</strong> window, models have <strong>output caps</strong>. When you ask for “write 30 pages,” you usually get a truncated response, a refusal, or something like “here is an outline; ask me to expand section by section.”</p>
<p>For long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2 etc. This yields better control and fewer errors.</p>
</section>
</section>
<section id="multimodality" class="level2">
<h2 class="anchored" data-anchor-id="multimodality">Multimodality</h2>
<p>Frontier models like GPT 5.2 and Gemini 3.0 are multimodal: <strong>they can process text, audio, images, and videos.</strong></p>
<section id="its-still-tokens-in-tokens-out" class="level3">
<h3 class="anchored" data-anchor-id="its-still-tokens-in-tokens-out">It’s still tokens in, tokens out</h3>
<p>Even when processing images, audio, or video, these models still work with tokens—just not word tokens. Different modalities get converted into numerical representations:</p>
<ul>
<li><strong>Images</strong> are divided into patches (small grid sections), and each patch becomes one or more tokens</li>
<li><strong>Audio</strong> is converted into spectrograms or waveform segments, then tokenized</li>
<li><strong>Video</strong> combines image tokens (frames) with temporal information</li>
</ul>
<p>The model processes all these token types through the same transformer architecture. This is why you can ask questions about an image in natural language—the image tokens and text tokens flow through the same neural network.</p>
</section>
<section id="specialized-model-orchestration" class="level3">
<h3 class="anchored" data-anchor-id="specialized-model-orchestration">Specialized model orchestration</h3>
<p>The frontier models you interact with often orchestrate multiple specialized models behind the scenes:</p>
<ul>
<li>image generation
<ul>
<li>GPT Image 1.5, Nano Banana Pro</li>
</ul></li>
<li>video generation
<ul>
<li>Sora 2, Veo 3.1</li>
</ul></li>
<li>speach generation (text-to-speach)
<ul>
<li>GPT-4o Mini TTS, Gemini 2.5 Pro Preview TTS</li>
</ul></li>
</ul>
<p>When you ask GPT 5.2 to “generate an image of a sunset over mountains,” the language model interprets your request, formulates a detailed prompt, and hands it off to GPT Image 1.5. The result is passed back and presented as part of the conversation.</p>
<p>This modular architecture allows each component to be optimized for its specific task while providing a unified conversational interface.</p>
</section>
</section>
<section id="main-shortcomings" class="level2">
<h2 class="anchored" data-anchor-id="main-shortcomings">Main Shortcomings</h2>
<p>Despite their impressive capabilities, current LLMs have fundamental limitations:</p>
<section id="knowledge-cutoff" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-cutoff">Knowledge Cutoff</h3>
<p>LLMs are trained on data up to a certain date. They don’t know about recent events, updated policies, or new research.</p>
</section>
<section id="limited-long-term-memory" class="level3">
<h3 class="anchored" data-anchor-id="limited-long-term-memory">Limited long-term memory</h3>
<p>As we discussed above, models have limited context windows (short-term memory). Some systems like ChatGPT implement “memory” features that save key facts about your conversations to a database and inject them into future prompts. But this is bolted-on storage, not true learning—the model’s parameters don’t change based on your interactions.</p>
</section>
<section id="hallucinations" class="level3">
<h3 class="anchored" data-anchor-id="hallucinations">Hallucinations</h3>
<p>LLMs can generate confident, fluent text that is factually incorrect. Models don’t “know” things. They are optimized for producing probable sounding text, not verified truth. This means they can:</p>
<ul>
<li>Invent citations that don’t exist</li>
<li>State incorrect facts with complete confidence</li>
<li>Mix accurate and inaccurate information seamlessly</li>
</ul>
</section>
<section id="inability-to-act" class="level3">
<h3 class="anchored" data-anchor-id="inability-to-act">Inability to act</h3>
<p>GenAI models ingest tokens and output tokens. The can not exert change on external systems (“act”). They cannot send emails, execute code, query databases, browse the web, modify files. The model can <em>describe</em> how to do these things, but it cannot actually perform them without external systems.</p>
<p><strong>This is where agents come in:</strong> By connecting LLMs to tools (APIs, code interpreters, file systems), we can create systems that can take real-world actions. The <a href="../fundamentals/03_agents.html">Agents</a> section describes how modern frameworks address these limitations by giving models the ability to plan, use tools, and affect external systems.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">GenAI in Academia | 2026</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>