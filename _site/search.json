[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Main website\nAI Tools and Applications\nOU AI Policies\n\nAI Governance Principles - OU Login required\nAI Teaching Guidance - OU Login required\nAI Research Guidance - OU Login required\n\nInstructional materials\n\nGenerative AI at the University - OU Canvas login required"
  },
  {
    "objectID": "resources.html#ai-at-university-of-oklahoma",
    "href": "resources.html#ai-at-university-of-oklahoma",
    "title": "Resources",
    "section": "",
    "text": "Main website\nAI Tools and Applications\nOU AI Policies\n\nAI Governance Principles - OU Login required\nAI Teaching Guidance - OU Login required\nAI Research Guidance - OU Login required\n\nInstructional materials\n\nGenerative AI at the University - OU Canvas login required"
  },
  {
    "objectID": "resources.html#ai-in-academia",
    "href": "resources.html#ai-in-academia",
    "title": "Resources",
    "section": "AI in Academia",
    "text": "AI in Academia\n\nFinding Equilibrium - Series of posts on AI in higher ed by Prof.¬†David Hummels and Prof.¬†Jay Akridge\nGen AI For Finance - Post by Prof.¬†Kery Back detailing his approach to incorporating AI into teaching\n\nAlso, check his FMA tutorial"
  },
  {
    "objectID": "resources.html#documentation",
    "href": "resources.html#documentation",
    "title": "Resources",
    "section": "Documentation",
    "text": "Documentation\n\n\n\n\nOpenAI\nAnthropic\nGoogle\nxAI\n\n\n\n\nGUI (Web, Mobile, Desktop) App Documentation\nüìñ\nüìñ\nüìñ\nüìñ\n\n\nAPI Documentation\nüìñ\nüìñ\nüìñ\nüìñ\n\n\nCookbook\nüç≥\nüç≥\nüç≥\nüç≥"
  },
  {
    "objectID": "resources.html#courses-tutorials",
    "href": "resources.html#courses-tutorials",
    "title": "Resources",
    "section": "Courses & Tutorials",
    "text": "Courses & Tutorials\n\nDeepLearning.AI ‚Äî Generative AI for Everyone ‚Äî Excellent intro course by Andrew Ng\nPrompt Engineering Guide ‚Äî Community resource for prompting techniques\n\nI would recommend looking at the ‚ÄúCookbook‚Äù links in the table above first. All of them have prompting guides specific to those models. Come here if you don‚Äôt find what you need there."
  },
  {
    "objectID": "resources.html#ethics-safety",
    "href": "resources.html#ethics-safety",
    "title": "Resources",
    "section": "Ethics & Safety",
    "text": "Ethics & Safety\n\nNIST AI Risk Management Framework\nEU AI Act Overview"
  },
  {
    "objectID": "resources.html#stay-updated",
    "href": "resources.html#stay-updated",
    "title": "Resources",
    "section": "Stay Updated",
    "text": "Stay Updated\n\nThe Batch - Weekly AI newsletter by DeepLearning.AI\nAI News - Daily AI news digest\nThe Innermost Loop - Daily tech news curated by Dr.¬†Alex Wissner-Gross"
  },
  {
    "objectID": "fundamentals/03_agents.html",
    "href": "fundamentals/03_agents.html",
    "title": "Generative AI Agents",
    "section": "",
    "text": "An AI agent is an LLM that can reason through problems, use tools, and take actions autonomously to accomplish goals. While a basic LLM simply generates a response to your prompt, an agent can:\n\nThink through multi-step problems before answering\nUse tools like web search, code execution, or database queries\nTake actions in external systems (send emails, update files, call APIs)\nIterate based on results‚Äîadjusting its approach when something doesn‚Äôt work\n\nThe term ‚Äúagentic‚Äù describes any workflow where the model operates with some degree of autonomy, making decisions about what to do next rather than just responding to a single prompt. This ranges from simple tool-calling (the model decides when to search the web) to fully autonomous agents that can complete complex multi-step tasks with minimal human oversight."
  },
  {
    "objectID": "fundamentals/03_agents.html#what-are-ai-agents",
    "href": "fundamentals/03_agents.html#what-are-ai-agents",
    "title": "Generative AI Agents",
    "section": "",
    "text": "An AI agent is an LLM that can reason through problems, use tools, and take actions autonomously to accomplish goals. While a basic LLM simply generates a response to your prompt, an agent can:\n\nThink through multi-step problems before answering\nUse tools like web search, code execution, or database queries\nTake actions in external systems (send emails, update files, call APIs)\nIterate based on results‚Äîadjusting its approach when something doesn‚Äôt work\n\nThe term ‚Äúagentic‚Äù describes any workflow where the model operates with some degree of autonomy, making decisions about what to do next rather than just responding to a single prompt. This ranges from simple tool-calling (the model decides when to search the web) to fully autonomous agents that can complete complex multi-step tasks with minimal human oversight."
  },
  {
    "objectID": "fundamentals/03_agents.html#thinking-models",
    "href": "fundamentals/03_agents.html#thinking-models",
    "title": "Generative AI Agents",
    "section": "Thinking Models",
    "text": "Thinking Models\nStandard LLMs generate output token-by-token in a single pass‚Äîthey don‚Äôt ‚Äústop and think.‚Äù Thinking models (like GPT 5.2, Gemini 3.0 Pro, Claude Opus 4.5) add an explicit reasoning phase before producing their final answer.\nHow it works:\n\nThe model receives your prompt\nBefore answering, it generates a chain of thought (CoT)‚Äîa series of reasoning steps that break down the problem\n\nIn modern systems, this thinking is visible to you and it‚Äôs a good idea to inspect it and see if you can spot issues with it\nFor some producs like ChatGPT Pro Deep Research, you can pause the thinking/execution loop if you want to correct some reasoning errors or want to add more context\n\n\nThe model can reconsider, catch errors, and refine its approach during this phase\nOnly after thinking does it produce the final response\n\nTrade-offs: Thinking models are better at complex reasoning (math, logic, multi-step problems) but are slower and more expensive because they generate many more tokens internally."
  },
  {
    "objectID": "fundamentals/03_agents.html#tool-use",
    "href": "fundamentals/03_agents.html#tool-use",
    "title": "Generative AI Agents",
    "section": "Tool Use",
    "text": "Tool Use\nLLMs can only generate tokens‚Äîthey can‚Äôt browse the web, run code, or access databases. ‚ÄúTool use‚Äù (also called ‚Äúfunction calling‚Äù) extends models to interact with external systems.\nHow it works:\n\nThe system prompt tells the model what tools are available (e.g., ‚ÄúYou can search the web‚Äù or ‚ÄúYou can run Python code‚Äù)\nInstead of generating text, the model can output a structured tool call (e.g., search(\"current weather in NYC\"), where search is a Python function somewhere on an OpenAI server)\nThe system executes the tool and returns the result to the model\nThe model incorporates the result into its context and continues generating\nThis loop can repeat‚Äîmodels can chain multiple tool calls to accomplish complex tasks\n\nWhy it matters: Tool use lets models access current information (web search), verify calculations (code execution), and take actions in the real world (APIs). It transforms LLMs from text generators into agents that can accomplish tasks.\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\\n+ Available Tools\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph \"Orchestration\"\n        F{\"Output\\nType?\"}\n        T[\"Thinking\\nTokens\"]\n        G[\"Tool Call\"]\n        H[\"Final\\nResponse\"]\n    end\n    \n    subgraph Tools\n        X[\"Execute Tool\"]\n        R[\"Tool Result\"]\n    end\n    \n    A --&gt; D\n    S --&gt; D\n    D --&gt; E --&gt; F\n    F --&gt;|\"Reasoning\"| T\n    T --&gt;|\"Append\"| D\n    F --&gt;|\"Tool\"| G --&gt; X --&gt; R\n    R --&gt;|\"Append\"| D\n    F --&gt;|\"Done\"| H\n    linkStyle 5 stroke-dasharray: 5 5\n    linkStyle 9 stroke-dasharray: 5 5\n\n\n\n\n\nThe key difference from the basic pipeline: thinking models generate reasoning tokens as output that gets appended to the context, creating a loop where the model ‚Äúthinks out loud‚Äù before producing its final answer. Tool-enabled models can similarly pause, call external tools, and incorporate results before continuing."
  },
  {
    "objectID": "fundamentals/03_agents.html#how-agentic-workflows-address-model-shortcomings",
    "href": "fundamentals/03_agents.html#how-agentic-workflows-address-model-shortcomings",
    "title": "Generative AI Agents",
    "section": "How Agentic Workflows Address Model Shortcomings",
    "text": "How Agentic Workflows Address Model Shortcomings\nLLMs on their own have fundamental limitations: they only know what was in their training data, they can confidently generate false information, they can‚Äôt remember past conversations, and they can‚Äôt take actions in the world. Agentic workflows‚Äîcombining thinking, tool use, and orchestration‚Äîgo a long way towards addressing these limitations.\n\nKnowledge cutoff\nLLMs are trained on data up to a certain date. They don‚Äôt know about recent events, updated policies, or new research.\nWeb search tools allow models to retrieve current information in real-time. When you ask ChatGPT about today‚Äôs news, it searches the web and incorporates fresh results into its response. This is called grounding‚Äîanchoring the model‚Äôs output in factual, up-to-date sources.\n\n\nLong-term memory\nThe context window is the model‚Äôs only ‚Äúmemory‚Äù‚Äîonce a conversation exceeds it, earlier content is forgotten. Models also can‚Äôt remember anything from previous sessions.\nExternal memory systems give models persistent storage:\n\nConversation memory: Systems can store summaries or key facts from past sessions and inject them into future conversations\nStructured databases: Tools can query SQL databases, spreadsheets, or CRMs to retrieve specific records\nVector databases and RAG: For unstructured knowledge (documents, policies, research papers), we use Retrieval-Augmented Generation (RAG)\n\n\n\n\n\n\n\nWhat is RAG?\n\n\n\nRetrieval-Augmented Generation (RAG) is a technique that lets models answer questions using your own documents:\n\nIndexing: Documents are split into chunks and converted into numerical vectors (embeddings) that capture semantic meaning. These are stored in a vector database.\nRetrieval: When you ask a question, your query is also converted to a vector. The system finds document chunks with similar vectors (i.e., related content).\nGeneration: The retrieved chunks are inserted into the prompt as context, and the model generates an answer grounded in your documents.\n\n\n\n\n\nHallucinations\nModels sometimes generate plausible-sounding but factually incorrect information. This happens because LLMs are pattern-completion machines‚Äîthey predict likely text, not truthful text.\nSolutions:\n\nWeb search grounding: You can ask models to cite sources, so you can verify claims. Many systems now include inline citations.\nRAG with citations: Answers grounded in retrieved documents can point to specific sources, making verification easy.\nCode execution: For math and logic, models can write and run code rather than ‚Äúreasoning‚Äù their way to an answer.\nVerification loops: Agentic systems can include explicit fact-checking steps where the model (or a separate model) reviews output for consistency and flags uncertain claims.\n\n\n\nAbility to act\nLLMs can only generate text. To actually do things‚Äîsend emails, update databases, control software‚Äîthey need to interact with external systems.\nSolutions:\n\nAPIs (Application Programming Interfaces): APIs are standardized ways for software systems to communicate. A model might request a call to a calendar API to schedule meetings, a Slack API to post messages, or a payment API to process transactions. The model generates the appropriate API call, the system executes it, and results flow back.\nMCP (Model Context Protocol): MCP is an emerging open standard (developed by Anthropic) that provides a uniform way to connect AI models to external tools and data sources. Instead of building custom integrations for each tool, developers can create MCP ‚Äúservers‚Äù that any MCP-compatible model can use.\nBrowser automation: Agentic browsers (like OpenAI Atlas or Perplexity Comet) let models navigate websites, fill forms, click buttons, and extract information‚Äîessentially using the web like a human would."
  },
  {
    "objectID": "fundamentals/04_products.html",
    "href": "fundamentals/04_products.html",
    "title": "Generative AI Products",
    "section": "",
    "text": "OpenAI\nAnthropic\nGoogle\nxAI\n\n\n\n\nWeb platform\nChatGPT\nClaude\nGemini\nGrok\n\n\nFlagship model\nGPT-5.2\nClaude Opus 4.5\nGemini 3 Pro\nGrok 4\n\n\nImage generation model\nGPT Image 1.5\n\nNano Banana Pro\nGrok 2 Image\n\n\nVideo generation model\nSora 2\n\nVeo 3.1\n\n\n\nSpeach generation model\nGPT-4o Mini TTS\n\n\nGemini 2.5 Pro Preview TTS\n\n\nCoding agent\nCodex\nClaude Code\nGemini Code Assist"
  },
  {
    "objectID": "fundamentals/04_products.html#products-from-frontier-model-providers",
    "href": "fundamentals/04_products.html#products-from-frontier-model-providers",
    "title": "Generative AI Products",
    "section": "",
    "text": "OpenAI\nAnthropic\nGoogle\nxAI\n\n\n\n\nWeb platform\nChatGPT\nClaude\nGemini\nGrok\n\n\nFlagship model\nGPT-5.2\nClaude Opus 4.5\nGemini 3 Pro\nGrok 4\n\n\nImage generation model\nGPT Image 1.5\n\nNano Banana Pro\nGrok 2 Image\n\n\nVideo generation model\nSora 2\n\nVeo 3.1\n\n\n\nSpeach generation model\nGPT-4o Mini TTS\n\n\nGemini 2.5 Pro Preview TTS\n\n\nCoding agent\nCodex\nClaude Code\nGemini Code Assist"
  },
  {
    "objectID": "fundamentals/04_products.html#third-party-products",
    "href": "fundamentals/04_products.html#third-party-products",
    "title": "Generative AI Products",
    "section": "Third-Party Products",
    "text": "Third-Party Products\n\nIDEs (integrated development environments)\n\nVS Code\n\nFree, lightweight, massive extension ecosystem\n\nCursor\n\nVS Code fork with native AI chat and code editing built-in\n\nGoogle Antigravity\n\nCloud-based IDE with deep Gemini integration\n\n\nCLI (command line interface) tools\n\nCodex CLI\n\nOpenAI-powered terminal coding assistant; great for shell commands\n\nClaude Code CLI\n\nAnthropic‚Äôs agentic coding tool; excels at complex multi-file edits\n\nGemini CLI\n\nGoogle‚Äôs CLI with built-in web search grounding\n\n\nOther coding tools\n\nGitHub Copilot\n\nCoding agent from Microsoft/GitHub\n\nReplit\n\nAI-assisted app-development platform\n\n\n(Re)search tools\n\nPerplexity\n\nAI search engine with real-time sources and inline citations\n\nConsensus\n\nSearches only peer-reviewed academic papers; ideal for research\n\nOpenEvidence\n\nMedical/clinical evidence search trained on clinical guidelines\n\n\nAgentic browsers\n\nOpenAI Atlas\n\nAI agent that autonomously navigates and interacts with websites\n\nPerplexity Comet\n\nBrowser with native AI search; summarizes pages as you browse"
  },
  {
    "objectID": "fundamentals/02_models.html",
    "href": "fundamentals/02_models.html",
    "title": "Generative AI Models",
    "section": "",
    "text": "A large language model is just a file full of numbers.\nWhen companies like OpenAI, Anthropic, or Google train a model, they‚Äôre creating a very large file‚Äîhundreds of gigabytes for frontier models‚Äîthat contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.\nThat is what the model ‚Äúknows‚Äù‚Äîstatistical patterns about how language works and what tends to follow what.\n\n\nModern frontier models like GPT 5.2 and Claude Opus 4.5 are built through a multi-stage training pipeline. Each stage shapes the model‚Äôs capabilities in different ways.\n\n\nThe foundation of every LLM is next-token prediction on massive text datasets. The model reads text from the internet, books, code repositories, academic papers, and other sources‚Äîa substantial portion of human written output. For each position in the text, it tries to predict what token comes next. When it guesses wrong, the parameters adjust slightly (via stochastic gradient descent) to make a better prediction next time. This process repeats trillions of times.\nPre-training is expensive‚Äîmonths of compute time on thousands of GPUs, costing hundreds of millions of dollars. But the result is a ‚Äúbase model‚Äù that has absorbed vast knowledge about language, facts, reasoning patterns, and code.\n\n\n\nThe base model can complete text, but it doesn‚Äôt know how to be a helpful assistant. Post-training shapes the model‚Äôs behavior:\nSupervised fine-tuning (SFT): Humans write thousands of example conversations demonstrating ideal assistant behavior‚Äîhow to respond to questions, handle ambiguity, refuse harmful requests, and maintain a helpful tone. The model learns to imitate these examples.\nReinforcement learning from human feedback (RLHF): Humans compare pairs of model responses and indicate which is better. These preferences train a ‚Äúreward model‚Äù that can score any response. The LLM then optimizes to produce responses that score highly‚Äîlearning to be more helpful, accurate, and aligned with human values.\n\n\n\nStandard post-training produces capable assistants, but they still struggle with complex reasoning. Models like GPT 5.2 and Claude Opus 4.5 undergo additional training specifically for multi-step reasoning:\nReinforcement learning on reasoning tasks: The model is given problems with verifiable answers (math, logic, coding challenges). It generates chain-of-thought reasoning, and the training process rewards chains that lead to correct answers. Over many iterations, the model learns how to think‚Äîwhich reasoning strategies work, when to reconsider, and how to break down complex problems.\nProcess reward models: Rather than just rewarding correct final answers, these models learn to evaluate each step of the reasoning process. This teaches the model to build sound arguments step-by-step, not just pattern-match to answers.\nSynthetic data and self-play: Models generate their own training data‚Äîproducing reasoning traces, solutions, and even new problems. The best outputs become training examples, creating a flywheel of improvement.\n\n\n\nDeployed models continue to improve through ongoing fine-tuning, bug fixes, and capability additions. The model you use today may be subtly different from last month‚Äôs version."
  },
  {
    "objectID": "fundamentals/02_models.html#what-is-an-llm",
    "href": "fundamentals/02_models.html#what-is-an-llm",
    "title": "Generative AI Models",
    "section": "",
    "text": "A large language model is just a file full of numbers.\nWhen companies like OpenAI, Anthropic, or Google train a model, they‚Äôre creating a very large file‚Äîhundreds of gigabytes for frontier models‚Äîthat contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.\nThat is what the model ‚Äúknows‚Äù‚Äîstatistical patterns about how language works and what tends to follow what.\n\n\nModern frontier models like GPT 5.2 and Claude Opus 4.5 are built through a multi-stage training pipeline. Each stage shapes the model‚Äôs capabilities in different ways.\n\n\nThe foundation of every LLM is next-token prediction on massive text datasets. The model reads text from the internet, books, code repositories, academic papers, and other sources‚Äîa substantial portion of human written output. For each position in the text, it tries to predict what token comes next. When it guesses wrong, the parameters adjust slightly (via stochastic gradient descent) to make a better prediction next time. This process repeats trillions of times.\nPre-training is expensive‚Äîmonths of compute time on thousands of GPUs, costing hundreds of millions of dollars. But the result is a ‚Äúbase model‚Äù that has absorbed vast knowledge about language, facts, reasoning patterns, and code.\n\n\n\nThe base model can complete text, but it doesn‚Äôt know how to be a helpful assistant. Post-training shapes the model‚Äôs behavior:\nSupervised fine-tuning (SFT): Humans write thousands of example conversations demonstrating ideal assistant behavior‚Äîhow to respond to questions, handle ambiguity, refuse harmful requests, and maintain a helpful tone. The model learns to imitate these examples.\nReinforcement learning from human feedback (RLHF): Humans compare pairs of model responses and indicate which is better. These preferences train a ‚Äúreward model‚Äù that can score any response. The LLM then optimizes to produce responses that score highly‚Äîlearning to be more helpful, accurate, and aligned with human values.\n\n\n\nStandard post-training produces capable assistants, but they still struggle with complex reasoning. Models like GPT 5.2 and Claude Opus 4.5 undergo additional training specifically for multi-step reasoning:\nReinforcement learning on reasoning tasks: The model is given problems with verifiable answers (math, logic, coding challenges). It generates chain-of-thought reasoning, and the training process rewards chains that lead to correct answers. Over many iterations, the model learns how to think‚Äîwhich reasoning strategies work, when to reconsider, and how to break down complex problems.\nProcess reward models: Rather than just rewarding correct final answers, these models learn to evaluate each step of the reasoning process. This teaches the model to build sound arguments step-by-step, not just pattern-match to answers.\nSynthetic data and self-play: Models generate their own training data‚Äîproducing reasoning traces, solutions, and even new problems. The best outputs become training examples, creating a flywheel of improvement.\n\n\n\nDeployed models continue to improve through ongoing fine-tuning, bug fixes, and capability additions. The model you use today may be subtly different from last month‚Äôs version."
  },
  {
    "objectID": "fundamentals/02_models.html#what-happens-when-you-prompt",
    "href": "fundamentals/02_models.html#what-happens-when-you-prompt",
    "title": "Generative AI Models",
    "section": "What Happens When You Prompt",
    "text": "What Happens When You Prompt\nWhen you type a question into ChatGPT or Claude, here‚Äôs what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into ‚Äútokens‚Äù‚Äînot exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a ‚Äúsystem prompt‚Äù (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model‚Äôs neural network. Each layer does mathematical operations‚Äîessentially matrix multiplications‚Äîtransforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by ‚Äútemperature‚Äù), adds that token to the sequence, and repeats until it ends up sampling a ‚Äústop‚Äù token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a ‚Äústop‚Äù token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/02_models.html#the-context-window",
    "href": "fundamentals/02_models.html#the-context-window",
    "title": "Generative AI Models",
    "section": "The Context Window",
    "text": "The Context Window\nThe context window is the maximum amount of text (and other inputs) the model can consider at once. This includes more than just your prompt:\n\nsystem instructions\nyour prompt\nchat history\nretrieved snippets\ntool outputs\nfile excerpts\nthe model‚Äôs output\n\nIf the total exceeds the limit:\n\nthe system must drop, summarize, or ‚Äúcompact‚Äù something,\nor it returns an error (depending on provider).\n\n\nWhat may be automatically include in the context window\nMany modern assistants can add extra context such as:\n\nsystem safety instructions\nyour custom instructions\nproject instructions\n‚Äúmemory‚Äù items (facts it saved about your preferences)\nsnippets retrieved from uploaded files or connected tools\n\nThis is why two people can ask the ‚Äúsame prompt‚Äù and get slightly different results.\n\n\nUploading files with your prompt\nIn most systems, uploading a file does not mean the entire file is placed into the context window verbatim.\nA common approach is retrieval (see ‚ÄúRAG‚Äù on Agents page):\n\nthe file is chunked,\nthe system builds embeddings (a searchable representation) from each chunk\nand only relevant chunks are pulled into the context window when needed (based on what you asked about the file)\n\n\n\nWhat happens when you near the limit of the context window\nDifferent providers handle this differently:\n\nSome UIs summarize older messages\nSome drop the earliest conversation turns (‚Äútruncation‚Äù)\nSome run a compaction step to preserve key details\n\n\n\nOutput limits\nEven with a very large input window, models have output caps. When you ask for ‚Äúwrite 30 pages,‚Äù you usually get a truncated response, a refusal, or something like ‚Äúhere is an outline; ask me to expand section by section.‚Äù\nFor long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2 etc. This yields better control and fewer errors."
  },
  {
    "objectID": "fundamentals/02_models.html#multimodality",
    "href": "fundamentals/02_models.html#multimodality",
    "title": "Generative AI Models",
    "section": "Multimodality",
    "text": "Multimodality\nFrontier models like GPT 5.2 and Gemini 3.0 are multimodal: they can process text, audio, images, and videos.\n\nIt‚Äôs still tokens in, tokens out\nEven when processing images, audio, or video, these models still work with tokens‚Äîjust not word tokens. Different modalities get converted into numerical representations:\n\nImages are divided into patches (small grid sections), and each patch becomes one or more tokens\nAudio is converted into spectrograms or waveform segments, then tokenized\nVideo combines image tokens (frames) with temporal information\n\nThe model processes all these token types through the same transformer architecture. This is why you can ask questions about an image in natural language‚Äîthe image tokens and text tokens flow through the same neural network.\n\n\nSpecialized model orchestration\nThe frontier models you interact with often orchestrate multiple specialized models behind the scenes:\n\nimage generation\n\nGPT Image 1.5, Nano Banana Pro\n\nvideo generation\n\nSora 2, Veo 3.1\n\nspeach generation (text-to-speach)\n\nGPT-4o Mini TTS, Gemini 2.5 Pro Preview TTS\n\n\nWhen you ask GPT 5.2 to ‚Äúgenerate an image of a sunset over mountains,‚Äù the language model interprets your request, formulates a detailed prompt, and hands it off to GPT Image 1.5. The result is passed back and presented as part of the conversation.\nThis modular architecture allows each component to be optimized for its specific task while providing a unified conversational interface."
  },
  {
    "objectID": "fundamentals/02_models.html#main-shortcomings",
    "href": "fundamentals/02_models.html#main-shortcomings",
    "title": "Generative AI Models",
    "section": "Main Shortcomings",
    "text": "Main Shortcomings\nDespite their impressive capabilities, current LLMs have fundamental limitations:\n\nKnowledge Cutoff\nLLMs are trained on data up to a certain date. They don‚Äôt know about recent events, updated policies, or new research.\n\n\nLimited long-term memory\nAs we discussed above, models have limited context windows (short-term memory). Some systems like ChatGPT implement ‚Äúmemory‚Äù features that save key facts about your conversations to a database and inject them into future prompts. But this is bolted-on storage, not true learning‚Äîthe model‚Äôs parameters don‚Äôt change based on your interactions.\n\n\nHallucinations\nLLMs can generate confident, fluent text that is factually incorrect. Models don‚Äôt ‚Äúknow‚Äù things. They are optimized for producing probable sounding text, not verified truth. This means they can:\n\nInvent citations that don‚Äôt exist\nState incorrect facts with complete confidence\nMix accurate and inaccurate information seamlessly\n\n\n\nInability to act\nGenAI models ingest tokens and output tokens. The can not exert change on external systems (‚Äúact‚Äù). They cannot send emails, execute code, query databases, browse the web, modify files. The model can describe how to do these things, but it cannot actually perform them without external systems.\nThis is where agents come in: By connecting LLMs to tools (APIs, code interpreters, file systems), we can create systems that can take real-world actions. The Agents section describes how modern frameworks address these limitations by giving models the ability to plan, use tools, and affect external systems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GenAI for Teaching and Research",
    "section": "",
    "text": "This website is designed to provide an introduction to Generative AI (GenAI) technologies, with a focus on practical understanding and effective utilization of these tools for teaching and research. The goal of the website is to provide a repository of resources that academics can use to:\n\nUnderstand the fundamental concepts behind generative AI and large language models\nNavigate the current landscape of GenAI tools and platforms\nIdentify key capabilities and limitations of different GenAI systems\nSelect appropriate tools for various use cases\nApply best practices for effective GenAI utilization"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "GenAI for Teaching and Research",
    "section": "",
    "text": "This website is designed to provide an introduction to Generative AI (GenAI) technologies, with a focus on practical understanding and effective utilization of these tools for teaching and research. The goal of the website is to provide a repository of resources that academics can use to:\n\nUnderstand the fundamental concepts behind generative AI and large language models\nNavigate the current landscape of GenAI tools and platforms\nIdentify key capabilities and limitations of different GenAI systems\nSelect appropriate tools for various use cases\nApply best practices for effective GenAI utilization"
  },
  {
    "objectID": "index.html#safety-and-privacy",
    "href": "index.html#safety-and-privacy",
    "title": "GenAI for Teaching and Research",
    "section": "Safety and privacy",
    "text": "Safety and privacy\nBefore we talk about capabilities, we need shared norms. Some of the biggest risks of GenAI in education and research are misplaced trust and unsafe data handling.\n\nDon‚Äôt upload sensitive data into consumer tools unless your institution has an approved, protected environment (‚ÄúEnterprise‚Äù level subscriptions usually offer this).\n\nExamples of sensitive data: identifiable student info (FERPA), unpublished manuscripts, reviewer comments, proprietary datasets, IRB-protected data, confidential employer partner data.\n\nTools like agentic browsers and conding agents may automatically parse the environment you give them access to (e.g.¬†an open browser window, or a folder on your computer). It is your responsibility to ensure that these environments do not contain sensitive data."
  },
  {
    "objectID": "fundamentals/key-capabilities.html",
    "href": "fundamentals/key-capabilities.html",
    "title": "Key GenAI Capabilities",
    "section": "",
    "text": "Bigger windows help with long syllabi, papers, transcripts, cases, and multi-file projects.\nBut bigger windows do not guarantee accurate long-document reasoning.\n\nExamples of publicly stated context sizes (as of late 2025):\n\nOpenAI‚Äôs GPT‚Äë5.2 Pro API model: 400k context window, 128k max output (API docs).\n\nChatGPT UI (GPT‚Äë5.2): context varies by tier/model (e.g., ‚ÄúThinking‚Äù has a much larger window than ‚ÄúInstant‚Äù).\n\nGoogle Gemini 2.5 Pro: announced 1M token context.\n\nAnthropic Claude long-context tiers include models with up to 1M tokens.\n\n\n\n\nAsk:\n\nCan it read charts, tables, and screenshots?\nCan it generate images or just analyze them?\nIs multimodality native (one model) or stitched (separate encoders + LLM)?\n\nFor finance, multimodality matters for:\n\nreading charts in papers,\ninterpreting slide decks,\nextracting data from PDFs and tables,\nunderstanding UI screenshots from trading or analytics tools.\n\n\n\n\nMany providers now offer a mode that:\n\nspends more compute to deliberate,\nperforms better on multi-step tasks,\nis slower and more expensive.\n\nOpenAI‚Äôs GPT‚Äë5.2 ‚ÄúAuto‚Äù can switch between Instant and Thinking; the UI can show a ‚Äúslimmed-down‚Äù view of chain-of-thought, with an ‚ÄúAnswer now‚Äù option.\n\n\n\nA model with tool access can:\n\nbrowse the web (and cite sources),\nuse a Python environment for calculations,\nanalyze files,\ngenerate spreadsheets / slide decks,\ncall external tools through connectors or ‚Äúactions.‚Äù\n\nWithout tools, models are limited to:\n\ntheir training data,\nand whatever you provide in the prompt.\n\n\n\n\n‚ÄúDeep research‚Äù typically means an agentic workflow:\n\nit searches,\ncollects sources,\nsynthesizes,\nand returns citations and an organized report.\n\nThis is usually not a separate ‚Äúbrain‚Äù; it‚Äôs an agent layer on top of a strong model + tools.\n\n\n\nCoding assistants aren‚Äôt only for programmers. Even if you don‚Äôt write software, they can:\n\ntranslate between Stata/R/Python,\nwrite reproducible scripts,\ngenerate data cleaning code,\nand explain unfamiliar code you inherited.\n\nOpenAI‚Äôs Codex ecosystem is one example (web/CLI/IDE integrations)."
  },
  {
    "objectID": "fundamentals/key-capabilities.html#key-ways-models-differ-what-to-pay-attention-to",
    "href": "fundamentals/key-capabilities.html#key-ways-models-differ-what-to-pay-attention-to",
    "title": "Key GenAI Capabilities",
    "section": "",
    "text": "Bigger windows help with long syllabi, papers, transcripts, cases, and multi-file projects.\nBut bigger windows do not guarantee accurate long-document reasoning.\n\nExamples of publicly stated context sizes (as of late 2025):\n\nOpenAI‚Äôs GPT‚Äë5.2 Pro API model: 400k context window, 128k max output (API docs).\n\nChatGPT UI (GPT‚Äë5.2): context varies by tier/model (e.g., ‚ÄúThinking‚Äù has a much larger window than ‚ÄúInstant‚Äù).\n\nGoogle Gemini 2.5 Pro: announced 1M token context.\n\nAnthropic Claude long-context tiers include models with up to 1M tokens.\n\n\n\n\nAsk:\n\nCan it read charts, tables, and screenshots?\nCan it generate images or just analyze them?\nIs multimodality native (one model) or stitched (separate encoders + LLM)?\n\nFor finance, multimodality matters for:\n\nreading charts in papers,\ninterpreting slide decks,\nextracting data from PDFs and tables,\nunderstanding UI screenshots from trading or analytics tools.\n\n\n\n\nMany providers now offer a mode that:\n\nspends more compute to deliberate,\nperforms better on multi-step tasks,\nis slower and more expensive.\n\nOpenAI‚Äôs GPT‚Äë5.2 ‚ÄúAuto‚Äù can switch between Instant and Thinking; the UI can show a ‚Äúslimmed-down‚Äù view of chain-of-thought, with an ‚ÄúAnswer now‚Äù option.\n\n\n\nA model with tool access can:\n\nbrowse the web (and cite sources),\nuse a Python environment for calculations,\nanalyze files,\ngenerate spreadsheets / slide decks,\ncall external tools through connectors or ‚Äúactions.‚Äù\n\nWithout tools, models are limited to:\n\ntheir training data,\nand whatever you provide in the prompt.\n\n\n\n\n‚ÄúDeep research‚Äù typically means an agentic workflow:\n\nit searches,\ncollects sources,\nsynthesizes,\nand returns citations and an organized report.\n\nThis is usually not a separate ‚Äúbrain‚Äù; it‚Äôs an agent layer on top of a strong model + tools.\n\n\n\nCoding assistants aren‚Äôt only for programmers. Even if you don‚Äôt write software, they can:\n\ntranslate between Stata/R/Python,\nwrite reproducible scripts,\ngenerate data cleaning code,\nand explain unfamiliar code you inherited.\n\nOpenAI‚Äôs Codex ecosystem is one example (web/CLI/IDE integrations)."
  },
  {
    "objectID": "fundamentals/how-llms-work.html",
    "href": "fundamentals/how-llms-work.html",
    "title": "How GenAI Models Work",
    "section": "",
    "text": "When you type a question into ChatGPT or Claude, here‚Äôs what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into ‚Äútokens‚Äù‚Äînot exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a ‚Äúsystem prompt‚Äù (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model‚Äôs neural network. Each layer does mathematical operations‚Äîessentially matrix multiplications‚Äîtransforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by ‚Äútemperature‚Äù), adds that token to the sequence, and repeats until it ends up sampling a ‚Äústop‚Äù token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a ‚Äústop‚Äù token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/how-llms-work.html#what-happens-when-you-prompt",
    "href": "fundamentals/how-llms-work.html#what-happens-when-you-prompt",
    "title": "How GenAI Models Work",
    "section": "",
    "text": "When you type a question into ChatGPT or Claude, here‚Äôs what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into ‚Äútokens‚Äù‚Äînot exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a ‚Äúsystem prompt‚Äù (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model‚Äôs neural network. Each layer does mathematical operations‚Äîessentially matrix multiplications‚Äîtransforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by ‚Äútemperature‚Äù), adds that token to the sequence, and repeats until it ends up sampling a ‚Äústop‚Äù token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a ‚Äústop‚Äù token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/how-llms-work.html#the-model",
    "href": "fundamentals/how-llms-work.html#the-model",
    "title": "How GenAI Models Work",
    "section": "The Model",
    "text": "The Model\nA large language model is just a file full of numbers.\nWhen companies like OpenAI, Anthropic, or Google train a model, they‚Äôre creating a very large file‚Äîhundreds of gigabytes for frontier models‚Äîthat contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.\nThat is what the model ‚Äúknows‚Äù‚Äîstatistical patterns about how language works and what tends to follow what.\n\nHow the Numbers Are Obtained\nThe training process happens in stages:\nPre-training: The model reads massive amounts of text from the internet, books, code repositories, academic papers‚Äîessentially a substantial portion of human written output. During this phase, it learns to predict ‚Äúwhat word comes next?‚Äù billions of times. The parameters adjust to get better at prediction. This takes months and costs tens to hundreds of millions of dollars in compute.\nPost-training (Fine-tuning): The base model is then adjusted using:\n\nSupervised fine-tuning (SFT): Humans write example conversations showing how the model should respond\nReinforcement learning from human feedback (RLHF): Humans rate model outputs, and the model learns to generate outputs that get higher ratings\nConstitutional AI (CAI): Used by Anthropic, where the model learns to critique and revise its own outputs based on principles\n\nContinuous refinement: Models are updated regularly to improve performance, fix issues, and add capabilities.\nKey terms you might hear: ‚Äútransformer architecture‚Äù (the mathematical structure these models use), ‚Äúattention mechanism‚Äù (how the model decides what parts of the input to focus on), ‚Äúparameters‚Äù or ‚Äúweights‚Äù (the numbers in the file)."
  },
  {
    "objectID": "fundamentals/how-llms-work.html#the-context-window",
    "href": "fundamentals/how-llms-work.html#the-context-window",
    "title": "How GenAI Models Work",
    "section": "The Context Window",
    "text": "The Context Window\nThe context window is the maximum amount of text (and other inputs) the model can consider at once:\n(system instructions + your prompt + chat history + retrieved snippets + tool outputs + file excerpts) + the model‚Äôs output.\nIf the total exceeds the limit:\n\nthe system must drop, summarize, or ‚Äúcompact‚Äù something,\nor it returns an error (depending on provider).\n\n\nWhat providers may automatically include in the context window (even if you don‚Äôt see it)\nMany modern assistants can add extra context such as:\n\nsystem safety instructions,\nyour custom instructions,\nproject instructions,\n‚Äúmemory‚Äù items (facts it saved about your preferences),\nsnippets retrieved from uploaded files or connected tools.\n\nThis is why two people can ask the ‚Äúsame prompt‚Äù and get slightly different results.\n\n\nUploading files with your prompt\nIn most systems, uploading a file does not mean the entire file is placed into the context window verbatim.\nA common approach is retrieval:\n\nthe file is chunked,\nthe system builds embeddings (a searchable representation) from each chunk\nand only relevant chunks are pulled into the context window when needed (based on what you asked about the file)\n\n\n\nWhat happens when you near the limit of the context window\nDifferent providers handle this differently:\n\nSome UIs summarize older messages.\nSome drop the earliest conversation turns (‚Äútruncation‚Äù).\nSome run a compaction step to preserve key details.\n\n\n\nOutput limits\nEven with a very large input window, models have output caps. When you ask for ‚Äúwrite 30 pages,‚Äù you usually get:\n\na truncated response,\nor a refusal,\nor ‚Äúhere is an outline; ask me to expand section by section.‚Äù\n\nRecommendation:\nFor long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2‚Ä¶\nThis yields better control and fewer errors."
  },
  {
    "objectID": "fundamentals/how-llms-work.html#thinking-planning-models-and-tool-use",
    "href": "fundamentals/how-llms-work.html#thinking-planning-models-and-tool-use",
    "title": "How GenAI Models Work",
    "section": "‚ÄúThinking‚Äù (Planning) Models and Tool Use",
    "text": "‚ÄúThinking‚Äù (Planning) Models and Tool Use\n\nThinking Models\nStandard LLMs generate output token-by-token in a single pass‚Äîthey don‚Äôt ‚Äústop and think.‚Äù Thinking models (like OpenAI‚Äôs o1/o3, Claude with extended thinking, or DeepSeek R1) add an explicit reasoning phase before producing their final answer.\nHow it works:\n\nThe model receives your prompt\nBefore answering, it generates a chain of thought‚Äîa series of reasoning steps that break down the problem\n\nIn modern systems, this thinking is visible to you and it‚Äôs a good idea to inspect it and see if you can spot issues with it\nFor some producs like ChatGPT Pro Deep Research, you can pause the thinking/execution loop if you want to correct some reasoning errors or want to add more context\n\n\nThe model can reconsider, catch errors, and refine its approach during this phase\nOnly after thinking does it produce the final response\n\nTrade-offs: Thinking models are better at complex reasoning (math, logic, multi-step problems) but are slower and more expensive because they generate many more tokens internally.\n\n\nTool Use\nLLMs can only generate tokens‚Äîthey can‚Äôt browse the web, run code, or access databases. ‚ÄúTool use‚Äù (also called ‚Äúfunction calling‚Äù) extends models to interact with external systems.\nHow it works:\n\nThe system prompt tells the model what tools are available (e.g., ‚ÄúYou can search the web‚Äù or ‚ÄúYou can run Python code‚Äù)\nInstead of generating text, the model can output a structured tool call (e.g., search(\"current weather in NYC\"), where search is a Python function somewhere on an OpenAI server)\nThe system executes the tool and returns the result to the model\nThe model incorporates the result into its context and continues generating\nThis loop can repeat‚Äîmodels can chain multiple tool calls to accomplish complex tasks\n\nWhy it matters: Tool use lets models access current information (web search), verify calculations (code execution), and take actions in the real world (APIs). It transforms LLMs from text generators into agents that can accomplish tasks.\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\\n+ Available Tools\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph \"LLM Output\"\n        F{\"Output\\nType?\"}\n        T[\"Thinking\\nTokens\"]\n        G[\"Tool Call\"]\n        H[\"Final\\nResponse\"]\n    end\n    \n    subgraph Tools\n        X[\"Execute Tool\"]\n        R[\"Tool Result\"]\n    end\n    \n    A --&gt; D\n    S --&gt; D\n    D --&gt; E --&gt; F\n    F --&gt;|\"Reasoning\"| T\n    T --&gt;|\"Append\"| D\n    F --&gt;|\"Tool\"| G --&gt; X --&gt; R\n    R --&gt;|\"Append\"| D\n    F --&gt;|\"Done\"| H\n    linkStyle 5 stroke-dasharray: 5 5\n    linkStyle 9 stroke-dasharray: 5 5\n\n\n\n\n\nThe key difference from the basic pipeline (at the top of this page): thinking models generate reasoning tokens as output that gets appended to the context, creating a loop where the model ‚Äúthinks out loud‚Äù before producing its final answer. Tool-enabled models can similarly pause, call external tools, and incorporate results before continuing."
  },
  {
    "objectID": "fundamentals/choosing-tools.html",
    "href": "fundamentals/choosing-tools.html",
    "title": "Choosing the Right Tool",
    "section": "",
    "text": "With dozens of GenAI tools available, selecting the right one for your needs can be overwhelming. This guide provides a framework for making informed choices."
  },
  {
    "objectID": "fundamentals/choosing-tools.html#introduction",
    "href": "fundamentals/choosing-tools.html#introduction",
    "title": "Choosing the Right Tool",
    "section": "",
    "text": "With dozens of GenAI tools available, selecting the right one for your needs can be overwhelming. This guide provides a framework for making informed choices."
  },
  {
    "objectID": "fundamentals/choosing-tools.html#decision-framework",
    "href": "fundamentals/choosing-tools.html#decision-framework",
    "title": "Choosing the Right Tool",
    "section": "Decision Framework",
    "text": "Decision Framework\n\n1. Define Your Use Case\nStart by clearly identifying what you need:\n\nTask type: Writing, coding, analysis, research?\nInput/output: Text only, or images/files too?\nFrequency: One-off task or ongoing usage?\nIntegration: Standalone or part of a workflow?\n\n\n\n2. Evaluate Key Factors\n\nCapability Match\n\n\n\nUse Case\nRecommended Tools\n\n\n\n\nGeneral writing\nChatGPT, Claude, Gemini\n\n\nCode development\nGitHub Copilot, Cursor, Claude\n\n\nResearch & search\nPerplexity, Gemini\n\n\nImage generation\nNano Banana Pro, Midjourney, Stable Diffusion\n\n\nDocument analysis\nClaude (long context), Gemini\n\n\n\n\n\nCost Considerations\n\n\n\n\n\n\nPricing Models\n\n\n\n\nFree tiers: Limited usage, good for exploration\nSubscriptions: $20/month typical for premium access\nAPI pricing: Pay per token, scales with usage\nEnterprise: Custom pricing with additional features\n\n\n\n\n\nPrivacy & Security\nConsider data handling requirements:\n\nSensitive data: May require local/on-premise solutions\nCompliance: Check vendor certifications (SOC 2, etc.)\nData retention: Understand how inputs are stored/used\nTraining opt-out: Some providers allow opting out of training"
  },
  {
    "objectID": "fundamentals/choosing-tools.html#tool-categories",
    "href": "fundamentals/choosing-tools.html#tool-categories",
    "title": "Choosing the Right Tool",
    "section": "Tool Categories",
    "text": "Tool Categories\n\nConversational Assistants\nBest for: General tasks, brainstorming, Q&A\n\nChatGPT (OpenAI)\nClaude (Anthropic)\nGemini (Google)\n\n\n\nCode Assistants\nBest for: Development workflows\n\nGitHub Copilot ‚Äî IDE integration\nCursor ‚Äî AI-native editor\nCodeium ‚Äî Free alternative\n\n\n\nSearch & Research\nBest for: Information retrieval, citations\n\nPerplexity ‚Äî AI search with sources\nGemini ‚Äî Google integration\nConsensus ‚Äî Academic research"
  },
  {
    "objectID": "fundamentals/choosing-tools.html#making-the-choice",
    "href": "fundamentals/choosing-tools.html#making-the-choice",
    "title": "Choosing the Right Tool",
    "section": "Making the Choice",
    "text": "Making the Choice\n\n\n\n\n\n\nRecommendation\n\n\n\nStart with a general-purpose tool (ChatGPT or Claude) to understand your needs, then specialize if necessary. Most users find 2-3 tools cover their needs.\n\n\n\nQuick Selection Guide\nNeed general assistance? ‚Üí ChatGPT or Claude\nNeed to search/research? ‚Üí Perplexity\nNeed code help in IDE? ‚Üí GitHub Copilot or Cursor\nNeed image generation? ‚Üí Midjourney or DALL-E\nNeed document analysis? ‚Üí Claude (long context)\nNeed privacy/local? ‚Üí Llama-based solutions"
  },
  {
    "objectID": "fundamentals/01_ecosystem.html",
    "href": "fundamentals/01_ecosystem.html",
    "title": "The GenAI Ecosystem",
    "section": "",
    "text": "When people say ‚ÄúAI,‚Äù or ‚ÄúGenAI,‚Äù they often mean ‚ÄúChatGPT‚Äù or a similar chat interface. It‚Äôs more useful to think of GenAI in layers:\n\nModels -&gt; the ‚Äúbrains‚Äù:\n\nGPT‚Äë5.2, Claude Opus 4.5, Gemini 3.0 Pro, Grok 4.1, etc.\n\nThese models are frequently referred to as LLMs (large language models). This is somewhat of a misnomer these days since the latest iterations of these models can handle more than just language (text) as an input. They can also process audio, image, and video inputs.\n\n\n\nTools -&gt; capabilities around the model:\n\nWeb search\nCode execution\nDatabase queries\nConnectors to other software/platforms like file systems, email clients, browsers, IDEs, CRMs, messaging apps etc\n\nFacilitated through APIs (application programming interfaces), and MCP (model context protocol) servers\n\n\nProducts (interfaces):\n\nWeb, desktop, and mobile apps from model providers: ChatGPT, Claude, Gemini, Grok\nCoding agents: Codex, Claude Code, Gemini Code Assist, GitHub Copilot\nThird-party apps: Perplexity, Replit, Cursor\n\n\nMany of these products use agentic workflows (agents) in the background (workflows orchestrated by a model) - The model plans steps, uses tools, analyzes the output of those tools, and iterates until it produces the requested output\n\n\n\nImage generated with Nano Banana Pro"
  },
  {
    "objectID": "fundamentals/01_ecosystem.html#framing-what-i-mean-by-genai",
    "href": "fundamentals/01_ecosystem.html#framing-what-i-mean-by-genai",
    "title": "The GenAI Ecosystem",
    "section": "",
    "text": "When people say ‚ÄúAI,‚Äù or ‚ÄúGenAI,‚Äù they often mean ‚ÄúChatGPT‚Äù or a similar chat interface. It‚Äôs more useful to think of GenAI in layers:\n\nModels -&gt; the ‚Äúbrains‚Äù:\n\nGPT‚Äë5.2, Claude Opus 4.5, Gemini 3.0 Pro, Grok 4.1, etc.\n\nThese models are frequently referred to as LLMs (large language models). This is somewhat of a misnomer these days since the latest iterations of these models can handle more than just language (text) as an input. They can also process audio, image, and video inputs.\n\n\n\nTools -&gt; capabilities around the model:\n\nWeb search\nCode execution\nDatabase queries\nConnectors to other software/platforms like file systems, email clients, browsers, IDEs, CRMs, messaging apps etc\n\nFacilitated through APIs (application programming interfaces), and MCP (model context protocol) servers\n\n\nProducts (interfaces):\n\nWeb, desktop, and mobile apps from model providers: ChatGPT, Claude, Gemini, Grok\nCoding agents: Codex, Claude Code, Gemini Code Assist, GitHub Copilot\nThird-party apps: Perplexity, Replit, Cursor\n\n\nMany of these products use agentic workflows (agents) in the background (workflows orchestrated by a model) - The model plans steps, uses tools, analyzes the output of those tools, and iterates until it produces the requested output\n\n\n\nImage generated with Nano Banana Pro"
  },
  {
    "objectID": "fundamentals/01_ecosystem.html#why-the-genai-as-a-stack-framing-matters",
    "href": "fundamentals/01_ecosystem.html#why-the-genai-as-a-stack-framing-matters",
    "title": "The GenAI Ecosystem",
    "section": "Why the ‚ÄúGenAI as a stack‚Äù framing matters",
    "text": "Why the ‚ÄúGenAI as a stack‚Äù framing matters\nIt reminds us that:\n\nJumps in capability are not just about better models, but better models + better tools + better agentic workflows.\nDifferent products may be better at different tasks (sometimes in a way that is not necessarily related to the models they use underneath)\n\nFor example, Claude (product) may be better than ChatGPT (product) at generating Excel sheets because of better integration with Microsoft Office (tool), not because Claude Opus 4.5 (model) is better than GPT 5.2 (model)\n\nYou should not be judging the current capabilities of ‚ÄúGenAI‚Äù based on your experiences with a single product (ChatGPT for most people)"
  }
]