[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Main website\nAI Tools and Applications\nOU AI Policies\n\nAI Governance Principles - OU Login required\nAI Teaching Guidance - OU Login required\nAI Research Guidance - OU Login required\n\nInstructional materials\n\nGenerative AI at the University - OU Canvas login required"
  },
  {
    "objectID": "resources.html#ai-at-university-of-oklahoma",
    "href": "resources.html#ai-at-university-of-oklahoma",
    "title": "Resources",
    "section": "",
    "text": "Main website\nAI Tools and Applications\nOU AI Policies\n\nAI Governance Principles - OU Login required\nAI Teaching Guidance - OU Login required\nAI Research Guidance - OU Login required\n\nInstructional materials\n\nGenerative AI at the University - OU Canvas login required"
  },
  {
    "objectID": "resources.html#ai-in-academia",
    "href": "resources.html#ai-in-academia",
    "title": "Resources",
    "section": "AI in Academia",
    "text": "AI in Academia\n\nFinding Equilibrium - Series of posts on AI in higher ed by Prof.¬†David Hummels and Prof.¬†Jay Akridge\nGen AI For Finance - Post by Prof.¬†Kery Back detailing his approach to incorporating AI into teaching\n\nAlso, check his FMA tutorial"
  },
  {
    "objectID": "resources.html#documentation",
    "href": "resources.html#documentation",
    "title": "Resources",
    "section": "Documentation",
    "text": "Documentation\n\n\n\n\nOpenAI\nAnthropic\nGoogle\nxAI\n\n\n\n\nGUI (Web, Mobile, Desktop) App Documentation\nüìñ\nüìñ\nüìñ\nüìñ\n\n\nAPI Documentation\nüìñ\nüìñ\nüìñ\nüìñ\n\n\nCookbook\nüç≥\nüç≥\nüç≥\nüç≥"
  },
  {
    "objectID": "resources.html#courses-tutorials",
    "href": "resources.html#courses-tutorials",
    "title": "Resources",
    "section": "Courses & Tutorials",
    "text": "Courses & Tutorials\n\nDeepLearning.AI ‚Äî Generative AI for Everyone ‚Äî Excellent intro course by Andrew Ng\nPrompt Engineering Guide ‚Äî Community resource for prompting techniques\n\nI would recommend looking at the ‚ÄúCookbook‚Äù links in the table above first. All of them have prompting guides specific to those models. Come here if you don‚Äôt find what you need there."
  },
  {
    "objectID": "resources.html#ethics-safety",
    "href": "resources.html#ethics-safety",
    "title": "Resources",
    "section": "Ethics & Safety",
    "text": "Ethics & Safety\n\nNIST AI Risk Management Framework\nEU AI Act Overview"
  },
  {
    "objectID": "resources.html#stay-updated",
    "href": "resources.html#stay-updated",
    "title": "Resources",
    "section": "Stay Updated",
    "text": "Stay Updated\n\nThe Batch - Weekly AI newsletter by DeepLearning.AI\nAI News - Daily AI news digest\nThe Innermost Loop - Daily tech news curated by Dr.¬†Alex Wissner-Gross"
  },
  {
    "objectID": "fundamentals/03_agents.html",
    "href": "fundamentals/03_agents.html",
    "title": "Generative AI Agents",
    "section": "",
    "text": "An AI agent is an LLM that can reason through problems, use tools, and take actions autonomously to accomplish goals. While a basic LLM simply generates a response to your prompt, an agent can:\n\nThink through multi-step problems before answering\nUse tools like web search, code execution, or database queries\nTake actions in external systems (send emails, update files, call APIs)\nIterate based on results‚Äîadjusting its approach when something doesn‚Äôt work\n\nThe term ‚Äúagentic‚Äù describes any workflow where the model operates with some degree of autonomy, making decisions about what to do next rather than just responding to a single prompt. This ranges from simple tool-calling (the model decides when to search the web) to fully autonomous agents that can complete complex multi-step tasks with minimal human oversight."
  },
  {
    "objectID": "fundamentals/03_agents.html#what-are-ai-agents",
    "href": "fundamentals/03_agents.html#what-are-ai-agents",
    "title": "Generative AI Agents",
    "section": "",
    "text": "An AI agent is an LLM that can reason through problems, use tools, and take actions autonomously to accomplish goals. While a basic LLM simply generates a response to your prompt, an agent can:\n\nThink through multi-step problems before answering\nUse tools like web search, code execution, or database queries\nTake actions in external systems (send emails, update files, call APIs)\nIterate based on results‚Äîadjusting its approach when something doesn‚Äôt work\n\nThe term ‚Äúagentic‚Äù describes any workflow where the model operates with some degree of autonomy, making decisions about what to do next rather than just responding to a single prompt. This ranges from simple tool-calling (the model decides when to search the web) to fully autonomous agents that can complete complex multi-step tasks with minimal human oversight."
  },
  {
    "objectID": "fundamentals/03_agents.html#thinking-models",
    "href": "fundamentals/03_agents.html#thinking-models",
    "title": "Generative AI Agents",
    "section": "Thinking Models",
    "text": "Thinking Models\nStandard LLMs generate output token-by-token in a single pass‚Äîthey don‚Äôt ‚Äústop and think.‚Äù Thinking models (like GPT 5.2, Gemini 3.0 Pro, Claude Opus 4.5) add an explicit reasoning phase before producing their final answer.\nHow it works:\n\nThe model receives your prompt\nBefore answering, it generates a chain of thought (CoT)‚Äîa series of reasoning steps that break down the problem\n\nIn modern systems, this thinking is visible to you and it‚Äôs a good idea to inspect it and see if you can spot issues with it\nFor some producs like ChatGPT Pro Deep Research, you can pause the thinking/execution loop if you want to correct some reasoning errors or want to add more context\n\n\nThe model can reconsider, catch errors, and refine its approach during this phase\nOnly after thinking does it produce the final response\n\nTrade-offs: Thinking models are better at complex reasoning (math, logic, multi-step problems) but are slower and more expensive because they generate many more tokens internally."
  },
  {
    "objectID": "fundamentals/03_agents.html#tool-use",
    "href": "fundamentals/03_agents.html#tool-use",
    "title": "Generative AI Agents",
    "section": "Tool Use",
    "text": "Tool Use\nLLMs can only generate tokens‚Äîthey can‚Äôt browse the web, run code, or access databases. ‚ÄúTool use‚Äù (also called ‚Äúfunction calling‚Äù) extends models to interact with external systems.\nHow it works:\n\nThe system prompt tells the model what tools are available (e.g., ‚ÄúYou can search the web‚Äù or ‚ÄúYou can run Python code‚Äù)\nInstead of generating text, the model can output a structured tool call (e.g., search(\"current weather in NYC\"), where search is a Python function somewhere on an OpenAI server)\nThe system executes the tool and returns the result to the model\nThe model incorporates the result into its context and continues generating\nThis loop can repeat‚Äîmodels can chain multiple tool calls to accomplish complex tasks\n\nWhy it matters: Tool use lets models access current information (web search), verify calculations (code execution), and take actions in the real world (APIs). It transforms LLMs from text generators into agents that can accomplish tasks.\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\\n+ Available Tools\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph \"Orchestration\"\n        F{\"Output\\nType?\"}\n        T[\"Thinking\\nTokens\"]\n        G[\"Tool Call\"]\n        H[\"Final\\nResponse\"]\n    end\n    \n    subgraph Tools\n        X[\"Execute Tool\"]\n        R[\"Tool Result\"]\n    end\n    \n    A --&gt; D\n    S --&gt; D\n    D --&gt; E --&gt; F\n    F --&gt;|\"Reasoning\"| T\n    T --&gt;|\"Append\"| D\n    F --&gt;|\"Tool\"| G --&gt; X --&gt; R\n    R --&gt;|\"Append\"| D\n    F --&gt;|\"Done\"| H\n    linkStyle 5 stroke-dasharray: 5 5\n    linkStyle 9 stroke-dasharray: 5 5\n\n\n\n\n\nThe key difference from the basic pipeline: thinking models generate reasoning tokens as output that gets appended to the context, creating a loop where the model ‚Äúthinks out loud‚Äù before producing its final answer. Tool-enabled models can similarly pause, call external tools, and incorporate results before continuing."
  },
  {
    "objectID": "fundamentals/03_agents.html#how-agentic-workflows-address-model-shortcomings",
    "href": "fundamentals/03_agents.html#how-agentic-workflows-address-model-shortcomings",
    "title": "Generative AI Agents",
    "section": "How Agentic Workflows Address Model Shortcomings",
    "text": "How Agentic Workflows Address Model Shortcomings\nLLMs on their own have fundamental limitations: they only know what was in their training data, they can confidently generate false information, they can‚Äôt remember past conversations, and they can‚Äôt take actions in the world. Agentic workflows‚Äîcombining thinking, tool use, and orchestration‚Äîgo a long way towards addressing these limitations.\n\nKnowledge cutoff\nLLMs are trained on data up to a certain date. They don‚Äôt know about recent events, updated policies, or new research.\nWeb search tools allow models to retrieve current information in real-time. When you ask ChatGPT about today‚Äôs news, it searches the web and incorporates fresh results into its response. This is called grounding‚Äîanchoring the model‚Äôs output in factual, up-to-date sources.\n\n\nLong-term memory\nThe context window is the model‚Äôs only ‚Äúmemory‚Äù‚Äîonce a conversation exceeds it, earlier content is forgotten. Models also can‚Äôt remember anything from previous sessions.\nExternal memory systems give models persistent storage:\n\nConversation memory: Systems can store summaries or key facts from past sessions and inject them into future conversations\nStructured databases: Tools can query SQL databases, spreadsheets, or CRMs to retrieve specific records\nVector databases and RAG: For unstructured knowledge (documents, policies, research papers), we use Retrieval-Augmented Generation (RAG)\n\n\n\n\n\n\n\nWhat is RAG?\n\n\n\nRetrieval-Augmented Generation (RAG) is a technique that lets models answer questions using your own documents:\n\nIndexing: Documents are split into chunks and converted into numerical vectors (embeddings) that capture semantic meaning. These are stored in a vector database.\nRetrieval: When you ask a question, your query is also converted to a vector. The system finds document chunks with similar vectors (i.e., related content).\nGeneration: The retrieved chunks are inserted into the prompt as context, and the model generates an answer grounded in your documents.\n\n\n\n\n\nHallucinations\nModels sometimes generate plausible-sounding but factually incorrect information. This happens because LLMs are pattern-completion machines‚Äîthey predict likely text, not truthful text.\nSolutions:\n\nWeb search grounding: You can ask models to cite sources, so you can verify claims. Many systems now include inline citations.\nRAG with citations: Answers grounded in retrieved documents can point to specific sources, making verification easy.\nCode execution: For math and logic, models can write and run code rather than ‚Äúreasoning‚Äù their way to an answer.\nVerification loops: Agentic systems can include explicit fact-checking steps where the model (or a separate model) reviews output for consistency and flags uncertain claims.\n\n\n\nAbility to act\nLLMs can only generate text. To actually do things‚Äîsend emails, update databases, control software‚Äîthey need to interact with external systems.\nSolutions:\n\nAPIs (Application Programming Interfaces): APIs are standardized ways for software systems to communicate. A model might request a call to a calendar API to schedule meetings, a Slack API to post messages, or a payment API to process transactions. The model generates the appropriate API call, the system executes it, and results flow back.\nMCP (Model Context Protocol): MCP is an emerging open standard (developed by Anthropic) that provides a uniform way to connect AI models to external tools and data sources. Instead of building custom integrations for each tool, developers can create MCP ‚Äúservers‚Äù that any MCP-compatible model can use.\nBrowser automation: Agentic browsers (like OpenAI Atlas or Perplexity Comet) let models navigate websites, fill forms, click buttons, and extract information‚Äîessentially using the web like a human would."
  },
  {
    "objectID": "fundamentals/04_products.html",
    "href": "fundamentals/04_products.html",
    "title": "Generative AI Products",
    "section": "",
    "text": "OpenAI\nAnthropic\nGoogle\nxAI\n\n\n\n\nWeb platform\nChatGPT\nClaude\nGemini\nGrok\n\n\nFlagship model\nGPT-5.2\nClaude Opus 4.5\nGemini 3 Pro\nGrok 4\n\n\nImage generation model\nGPT Image 1.5\n\nNano Banana Pro\nGrok 2 Image\n\n\nVideo generation model\nSora 2\n\nVeo 3.1\n\n\n\nSpeach generation model\nGPT-4o Mini TTS\n\n\nGemini 2.5 Pro Preview TTS\n\n\nCoding agent\nCodex\nClaude Code\nGemini Code Assist"
  },
  {
    "objectID": "fundamentals/04_products.html#products-from-frontier-model-providers",
    "href": "fundamentals/04_products.html#products-from-frontier-model-providers",
    "title": "Generative AI Products",
    "section": "",
    "text": "OpenAI\nAnthropic\nGoogle\nxAI\n\n\n\n\nWeb platform\nChatGPT\nClaude\nGemini\nGrok\n\n\nFlagship model\nGPT-5.2\nClaude Opus 4.5\nGemini 3 Pro\nGrok 4\n\n\nImage generation model\nGPT Image 1.5\n\nNano Banana Pro\nGrok 2 Image\n\n\nVideo generation model\nSora 2\n\nVeo 3.1\n\n\n\nSpeach generation model\nGPT-4o Mini TTS\n\n\nGemini 2.5 Pro Preview TTS\n\n\nCoding agent\nCodex\nClaude Code\nGemini Code Assist"
  },
  {
    "objectID": "fundamentals/04_products.html#third-party-products",
    "href": "fundamentals/04_products.html#third-party-products",
    "title": "Generative AI Products",
    "section": "Third-Party Products",
    "text": "Third-Party Products\n\nIDEs (integrated development environments)\n\nVS Code\n\nFree, lightweight, massive extension ecosystem\n\nCursor\n\nVS Code fork with native AI chat and code editing built-in\n\nGoogle Antigravity\n\nCloud-based IDE with deep Gemini integration\n\n\nCLI (command line interface) tools\n\nCodex CLI\n\nOpenAI-powered terminal coding assistant; great for shell commands\n\nClaude Code CLI\n\nAnthropic‚Äôs agentic coding tool; excels at complex multi-file edits\n\nGemini CLI\n\nGoogle‚Äôs CLI with built-in web search grounding\n\n\nOther coding tools\n\nGitHub Copilot\n\nCoding agent from Microsoft/GitHub\n\nReplit\n\nAI-assisted app-development platform\n\nAnaconda\n\nComprehensive Python distribution and environment management system.\n\n\n(Re)search tools\n\nPerplexity\n\nAI search engine with real-time sources and inline citations\n\nConsensus\n\nSearches only peer-reviewed academic papers; ideal for research\n\nOpenEvidence\n\nMedical/clinical evidence search trained on clinical guidelines\n\n\nAgentic browsers\n\nOpenAI Atlas\n\nAI agent that autonomously navigates and interacts with websites\n\nPerplexity Comet\n\nBrowser with native AI search; summarizes pages as you browse\n\n\nLaTex\n\nOpenAI Prism\n\nAI-enabled LaTex workspace (think Overleaf with ChatGPT embedded)"
  },
  {
    "objectID": "fundamentals/02_models.html",
    "href": "fundamentals/02_models.html",
    "title": "Generative AI Models",
    "section": "",
    "text": "A large language model is just a file full of numbers.\nWhen companies like OpenAI, Anthropic, or Google train a model, they‚Äôre creating a very large file‚Äîhundreds of gigabytes for frontier models‚Äîthat contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.\nThat is what the model ‚Äúknows‚Äù‚Äîstatistical patterns about how language works and what tends to follow what.\n\n\nModern frontier models like GPT 5.2 and Claude Opus 4.5 are built through a multi-stage training pipeline. Each stage shapes the model‚Äôs capabilities in different ways.\n\n\nThe foundation of every LLM is next-token prediction on massive text datasets. The model reads text from the internet, books, code repositories, academic papers, and other sources‚Äîa substantial portion of human written output. For each position in the text, it tries to predict what token comes next. When it guesses wrong, the parameters adjust slightly (via stochastic gradient descent) to make a better prediction next time. This process repeats trillions of times.\nPre-training is expensive‚Äîmonths of compute time on thousands of GPUs, costing hundreds of millions of dollars. But the result is a ‚Äúbase model‚Äù that has absorbed vast knowledge about language, facts, reasoning patterns, and code.\n\n\n\nThe base model can complete text, but it doesn‚Äôt know how to be a helpful assistant. Post-training shapes the model‚Äôs behavior:\nSupervised fine-tuning (SFT): Humans write thousands of example conversations demonstrating ideal assistant behavior‚Äîhow to respond to questions, handle ambiguity, refuse harmful requests, and maintain a helpful tone. The model learns to imitate these examples.\nReinforcement learning from human feedback (RLHF): Humans compare pairs of model responses and indicate which is better. These preferences train a ‚Äúreward model‚Äù that can score any response. The LLM then optimizes to produce responses that score highly‚Äîlearning to be more helpful, accurate, and aligned with human values.\n\n\n\nStandard post-training produces capable assistants, but they still struggle with complex reasoning. Models like GPT 5.2 and Claude Opus 4.5 undergo additional training specifically for multi-step reasoning:\nReinforcement learning on reasoning tasks: The model is given problems with verifiable answers (math, logic, coding challenges). It generates chain-of-thought reasoning, and the training process rewards chains that lead to correct answers. Over many iterations, the model learns how to think‚Äîwhich reasoning strategies work, when to reconsider, and how to break down complex problems.\nProcess reward models: Rather than just rewarding correct final answers, these models learn to evaluate each step of the reasoning process. This teaches the model to build sound arguments step-by-step, not just pattern-match to answers.\nSynthetic data and self-play: Models generate their own training data‚Äîproducing reasoning traces, solutions, and even new problems. The best outputs become training examples, creating a flywheel of improvement.\n\n\n\nDeployed models continue to improve through ongoing fine-tuning, bug fixes, and capability additions. The model you use today may be subtly different from last month‚Äôs version."
  },
  {
    "objectID": "fundamentals/02_models.html#what-is-an-llm",
    "href": "fundamentals/02_models.html#what-is-an-llm",
    "title": "Generative AI Models",
    "section": "",
    "text": "A large language model is just a file full of numbers.\nWhen companies like OpenAI, Anthropic, or Google train a model, they‚Äôre creating a very large file‚Äîhundreds of gigabytes for frontier models‚Äîthat contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.\nThat is what the model ‚Äúknows‚Äù‚Äîstatistical patterns about how language works and what tends to follow what.\n\n\nModern frontier models like GPT 5.2 and Claude Opus 4.5 are built through a multi-stage training pipeline. Each stage shapes the model‚Äôs capabilities in different ways.\n\n\nThe foundation of every LLM is next-token prediction on massive text datasets. The model reads text from the internet, books, code repositories, academic papers, and other sources‚Äîa substantial portion of human written output. For each position in the text, it tries to predict what token comes next. When it guesses wrong, the parameters adjust slightly (via stochastic gradient descent) to make a better prediction next time. This process repeats trillions of times.\nPre-training is expensive‚Äîmonths of compute time on thousands of GPUs, costing hundreds of millions of dollars. But the result is a ‚Äúbase model‚Äù that has absorbed vast knowledge about language, facts, reasoning patterns, and code.\n\n\n\nThe base model can complete text, but it doesn‚Äôt know how to be a helpful assistant. Post-training shapes the model‚Äôs behavior:\nSupervised fine-tuning (SFT): Humans write thousands of example conversations demonstrating ideal assistant behavior‚Äîhow to respond to questions, handle ambiguity, refuse harmful requests, and maintain a helpful tone. The model learns to imitate these examples.\nReinforcement learning from human feedback (RLHF): Humans compare pairs of model responses and indicate which is better. These preferences train a ‚Äúreward model‚Äù that can score any response. The LLM then optimizes to produce responses that score highly‚Äîlearning to be more helpful, accurate, and aligned with human values.\n\n\n\nStandard post-training produces capable assistants, but they still struggle with complex reasoning. Models like GPT 5.2 and Claude Opus 4.5 undergo additional training specifically for multi-step reasoning:\nReinforcement learning on reasoning tasks: The model is given problems with verifiable answers (math, logic, coding challenges). It generates chain-of-thought reasoning, and the training process rewards chains that lead to correct answers. Over many iterations, the model learns how to think‚Äîwhich reasoning strategies work, when to reconsider, and how to break down complex problems.\nProcess reward models: Rather than just rewarding correct final answers, these models learn to evaluate each step of the reasoning process. This teaches the model to build sound arguments step-by-step, not just pattern-match to answers.\nSynthetic data and self-play: Models generate their own training data‚Äîproducing reasoning traces, solutions, and even new problems. The best outputs become training examples, creating a flywheel of improvement.\n\n\n\nDeployed models continue to improve through ongoing fine-tuning, bug fixes, and capability additions. The model you use today may be subtly different from last month‚Äôs version."
  },
  {
    "objectID": "fundamentals/02_models.html#what-happens-when-you-prompt",
    "href": "fundamentals/02_models.html#what-happens-when-you-prompt",
    "title": "Generative AI Models",
    "section": "What Happens When You Prompt",
    "text": "What Happens When You Prompt\nWhen you type a question into ChatGPT or Claude, here‚Äôs what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into ‚Äútokens‚Äù‚Äînot exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a ‚Äúsystem prompt‚Äù (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model‚Äôs neural network. Each layer does mathematical operations‚Äîessentially matrix multiplications‚Äîtransforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by ‚Äútemperature‚Äù), adds that token to the sequence, and repeats until it ends up sampling a ‚Äústop‚Äù token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a ‚Äústop‚Äù token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/02_models.html#the-context-window",
    "href": "fundamentals/02_models.html#the-context-window",
    "title": "Generative AI Models",
    "section": "The Context Window",
    "text": "The Context Window\nThe context window is the maximum amount of text (and other inputs) the model can consider at once. This includes more than just your prompt:\n\nsystem instructions\nyour prompt\nchat history\nretrieved snippets\ntool outputs\nfile excerpts\nthe model‚Äôs output\n\nIf the total exceeds the limit:\n\nthe system must drop, summarize, or ‚Äúcompact‚Äù something,\nor it returns an error (depending on provider).\n\n\nWhat may be automatically include in the context window\nMany modern assistants can add extra context such as:\n\nsystem safety instructions\nyour custom instructions\nproject instructions\n‚Äúmemory‚Äù items (facts it saved about your preferences)\nsnippets retrieved from uploaded files or connected tools\n\nThis is why two people can ask the ‚Äúsame prompt‚Äù and get slightly different results.\n\n\nUploading files with your prompt\nIn most systems, uploading a file does not mean the entire file is placed into the context window verbatim.\nA common approach is retrieval (see ‚ÄúRAG‚Äù on Agents page):\n\nthe file is chunked,\nthe system builds embeddings (a searchable representation) from each chunk\nand only relevant chunks are pulled into the context window when needed (based on what you asked about the file)\n\n\n\nWhat happens when you near the limit of the context window\nDifferent providers handle this differently:\n\nSome UIs summarize older messages\nSome drop the earliest conversation turns (‚Äútruncation‚Äù)\nSome run a compaction step to preserve key details\n\n\n\nOutput limits\nEven with a very large input window, models have output caps. When you ask for ‚Äúwrite 30 pages,‚Äù you usually get a truncated response, a refusal, or something like ‚Äúhere is an outline; ask me to expand section by section.‚Äù\nFor long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2 etc. This yields better control and fewer errors."
  },
  {
    "objectID": "fundamentals/02_models.html#multimodality",
    "href": "fundamentals/02_models.html#multimodality",
    "title": "Generative AI Models",
    "section": "Multimodality",
    "text": "Multimodality\nFrontier models like GPT 5.2 and Gemini 3.0 are multimodal: they can process text, audio, images, and videos.\n\nIt‚Äôs still tokens in, tokens out\nEven when processing images, audio, or video, these models still work with tokens‚Äîjust not word tokens. Different modalities get converted into numerical representations:\n\nImages are divided into patches (small grid sections), and each patch becomes one or more tokens\nAudio is converted into spectrograms or waveform segments, then tokenized\nVideo combines image tokens (frames) with temporal information\n\nThe model processes all these token types through the same transformer architecture. This is why you can ask questions about an image in natural language‚Äîthe image tokens and text tokens flow through the same neural network.\n\n\nSpecialized model orchestration\nThe frontier models you interact with often orchestrate multiple specialized models behind the scenes:\n\nimage generation\n\nGPT Image 1.5, Nano Banana Pro\n\nvideo generation\n\nSora 2, Veo 3.1\n\nspeach generation (text-to-speach)\n\nGPT-4o Mini TTS, Gemini 2.5 Pro Preview TTS\n\n\nWhen you ask GPT 5.2 to ‚Äúgenerate an image of a sunset over mountains,‚Äù the language model interprets your request, formulates a detailed prompt, and hands it off to GPT Image 1.5. The result is passed back and presented as part of the conversation.\nThis modular architecture allows each component to be optimized for its specific task while providing a unified conversational interface."
  },
  {
    "objectID": "fundamentals/02_models.html#main-shortcomings",
    "href": "fundamentals/02_models.html#main-shortcomings",
    "title": "Generative AI Models",
    "section": "Main Shortcomings",
    "text": "Main Shortcomings\nDespite their impressive capabilities, current LLMs have fundamental limitations:\n\nKnowledge Cutoff\nLLMs are trained on data up to a certain date. They don‚Äôt know about recent events, updated policies, or new research.\n\n\nLimited long-term memory\nAs we discussed above, models have limited context windows (short-term memory). Some systems like ChatGPT implement ‚Äúmemory‚Äù features that save key facts about your conversations to a database and inject them into future prompts. But this is bolted-on storage, not true learning‚Äîthe model‚Äôs parameters don‚Äôt change based on your interactions.\n\n\nHallucinations\nLLMs can generate confident, fluent text that is factually incorrect. Models don‚Äôt ‚Äúknow‚Äù things. They are optimized for producing probable sounding text, not verified truth. This means they can:\n\nInvent citations that don‚Äôt exist\nState incorrect facts with complete confidence\nMix accurate and inaccurate information seamlessly\n\n\n\nInability to act\nGenAI models ingest tokens and output tokens. The can not exert change on external systems (‚Äúact‚Äù). They cannot send emails, execute code, query databases, browse the web, modify files. The model can describe how to do these things, but it cannot actually perform them without external systems.\nThis is where agents come in: By connecting LLMs to tools (APIs, code interpreters, file systems), we can create systems that can take real-world actions. The Agents section describes how modern frameworks address these limitations by giving models the ability to plan, use tools, and affect external systems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GenAI for Teaching and Research",
    "section": "",
    "text": "This website is designed to provide an introduction to Generative AI (GenAI) technologies, with a focus on practical understanding and effective utilization of these tools for teaching and research. The website will be periodically updated to provide an up to date repository of resources that academics can use to:\n\nUnderstand the fundamental concepts behind generative AI and large language models\nNavigate the current landscape of GenAI tools and platforms\nIdentify key capabilities and limitations of different GenAI systems\nSelect appropriate tools for various use cases\nApply best practices for effective GenAI utilization"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "GenAI for Teaching and Research",
    "section": "",
    "text": "This website is designed to provide an introduction to Generative AI (GenAI) technologies, with a focus on practical understanding and effective utilization of these tools for teaching and research. The website will be periodically updated to provide an up to date repository of resources that academics can use to:\n\nUnderstand the fundamental concepts behind generative AI and large language models\nNavigate the current landscape of GenAI tools and platforms\nIdentify key capabilities and limitations of different GenAI systems\nSelect appropriate tools for various use cases\nApply best practices for effective GenAI utilization"
  },
  {
    "objectID": "index.html#safety-and-privacy",
    "href": "index.html#safety-and-privacy",
    "title": "GenAI for Teaching and Research",
    "section": "Safety and privacy",
    "text": "Safety and privacy\nBefore we talk about capabilities, we need shared norms. Some of the biggest risks of GenAI in education and research are misplaced trust and unsafe data handling.\n\nDon‚Äôt upload sensitive data into consumer tools unless your institution has an approved, protected environment (‚ÄúEnterprise‚Äù level subscriptions usually offer this).\n\nExamples of sensitive data: identifiable student info (FERPA), unpublished manuscripts, reviewer comments, proprietary datasets, IRB-protected data, confidential employer partner data.\n\nTools like agentic browsers and conding agents may automatically parse the environment you give them access to (e.g.¬†an open browser window, or a folder on your computer). It is your responsibility to ensure that these environments do not contain sensitive data."
  },
  {
    "objectID": "teaching/tools.html",
    "href": "teaching/tools.html",
    "title": "GenAI Tools for Teaching",
    "section": "",
    "text": "NotebookLM is Google‚Äôs AI-powered research and note-taking tool. Upload your course materials (PDFs, docs, slides, websites) and NotebookLM can generate:\n\nAudio Overviews (‚ÄúPodcasts‚Äù): Two AI hosts discuss your content in a conversational style. Choose from Deep Dive, Brief (1-2 min), Critique, or Debate formats.\nFlashcards: Customizable study cards for key terms, dates, and concepts with adjustable difficulty levels.\nQuizzes: Multiple-choice questions with optional hints and explanations.\n\nTeaching use: Convert dense lecture notes or readings into engaging podcast-style summaries students can listen to on the go, or auto-generate study aids for exam prep."
  },
  {
    "objectID": "teaching/tools.html#notebooklm",
    "href": "teaching/tools.html#notebooklm",
    "title": "GenAI Tools for Teaching",
    "section": "",
    "text": "NotebookLM is Google‚Äôs AI-powered research and note-taking tool. Upload your course materials (PDFs, docs, slides, websites) and NotebookLM can generate:\n\nAudio Overviews (‚ÄúPodcasts‚Äù): Two AI hosts discuss your content in a conversational style. Choose from Deep Dive, Brief (1-2 min), Critique, or Debate formats.\nFlashcards: Customizable study cards for key terms, dates, and concepts with adjustable difficulty levels.\nQuizzes: Multiple-choice questions with optional hints and explanations.\n\nTeaching use: Convert dense lecture notes or readings into engaging podcast-style summaries students can listen to on the go, or auto-generate study aids for exam prep."
  },
  {
    "objectID": "teaching/tools.html#chatgpt-projects",
    "href": "teaching/tools.html#chatgpt-projects",
    "title": "GenAI Tools for Teaching",
    "section": "ChatGPT Projects",
    "text": "ChatGPT Projects\nChatGPT Projects are smart workspaces that organize chats, files, and custom instructions together. Upload up to 20-40 files (depending on plan) including PDFs, spreadsheets, images, and documents‚ÄîChatGPT references these across all conversations in the project.\n\nSupports files up to 512MB and 2M tokens\nBuilt-in memory retains context across sessions\nCan synthesize, transform, and extract information from complex documents\n\nTeaching use: Upload teaching materials into a ‚ÄúProject‚Äù and ChatGPT can then answer questions with citations, explain derivations, or (for teachers) help you identify gaps in your materials. Particularly strong for STEM content with equations and figures that other tools struggle to parse (NotebookLM is not as good at this)."
  },
  {
    "objectID": "teaching/tools.html#gemini-in-chrome",
    "href": "teaching/tools.html#gemini-in-chrome",
    "title": "GenAI Tools for Teaching",
    "section": "Gemini in Chrome",
    "text": "Gemini in Chrome\nGemini in Chrome is Google‚Äôs new browser-native AI integration. Access Gemini directly from Chrome‚Äôs sidebar to interact with web content.\n\nAsk Gemini to clarify complex information across up to 10 open tabs simultaneously\nGet key takeaways and summaries from webpages\nUse Gemini Live to have natural voice conversations while browsing\nAI Mode in the address bar for quick questions\n\nTeaching use: If your lecture notes, readings, or course materials are hosted online (Canvas pages, course websites, online textbooks), students can ask Gemini questions directly about the content without copy-pasting. Useful for navigating and understanding multi-page web-based course materials. Make sure students understand that this should not be used on webpages that contain private information"
  },
  {
    "objectID": "teaching/tools.html#claude-in-excel",
    "href": "teaching/tools.html#claude-in-excel",
    "title": "GenAI Tools for Teaching",
    "section": "Claude in Excel",
    "text": "Claude in Excel\nClaude in Excel is Anthropic‚Äôs native Excel add-in (currently in beta for Claude Pro, Max, Team, and Enterprise users). Access Claude from a sidebar within Excel or through Microsoft 365 Copilot‚Äôs Agent Mode.\n\nReads multi-tab spreadsheets with cell-level citations\nDebugs and fixes formulas while preserving dependencies\nCreates pivot tables and charts\nBuilds new spreadsheets from scratch or populates templates\nTracks and explains all changes with full transparency\n\nTeaching use: For courses involving data analysis, Claude can help students understand complex spreadsheet logic, debug formula errors, and learn Excel workflows interactively. Instructors can use it to quickly build example datasets or grade spreadsheet-based assignments."
  },
  {
    "objectID": "teaching/tools.html#openai-prism",
    "href": "teaching/tools.html#openai-prism",
    "title": "GenAI Tools for Teaching",
    "section": "OpenAI Prism",
    "text": "OpenAI Prism\nOpenAI Prism is a free AI-native workspace for scientific writing. It‚Äôs a LaTeX-native editor powered by GPT-5.2 designed for researchers and educators.\n\nDraft, compile, and collaborate in real-time with unlimited collaborators\nAI-assisted writing with full document context\nSearch and incorporate literature from arXiv directly\nConvert whiteboard equations or diagrams into LaTeX\nAutomated proofreading, citation management, and equation formatting\n\nTeaching use: Create professional lecture slides and course materials with proper mathematical typesetting. The AI can help refine explanations, suggest relevant literature, and ensure consistent formatting. Free for anyone with a ChatGPT personal account‚Äîmaking it accessible for students writing technical reports or theses."
  },
  {
    "objectID": "teaching/tools.html#vs-code-codex-extension",
    "href": "teaching/tools.html#vs-code-codex-extension",
    "title": "GenAI Tools for Teaching",
    "section": "VS Code + Codex Extension",
    "text": "VS Code + Codex Extension\nOpenAI Codex is a VS Code extension that functions as an AI coding agent with full access to your workspace files. Included with ChatGPT Plus, Pro, Business, Edu, and Enterprise plans.\n\nSide-by-side mode: Chat with Codex in a panel while it reads and edits your files with full context\nCloud delegation: Offload larger tasks to run in the background\nAccepts text, screenshots, or diagrams as input\nGenerates, debugs, explains, and refactors code\n\nTeaching use: Coding agents like Codex are good at more than just coding. This gives LLMs direct access to your local files so you don‚Äôt have to copy-paste anything into ChatGPT. You can just ask Codex to read, edit, create new files on your computer. Students can use it as a tutoring tool that understands their entire project context."
  },
  {
    "objectID": "research/discussion.html",
    "href": "research/discussion.html",
    "title": "Generative AI and Academic Research",
    "section": "",
    "text": "Submission policy: The journal does not have explicity AI guidelines for authors, but submissions are governed by Wiley‚Äôs policies. The publishers‚Äôs AI guidelines are here\n\nDisclosure requirement for Authors: ‚ÄúAuthors should maintain documentation of all AI Technology used, including its purpose, whether it impacted key arguments or conclusions, and how they personally reviewed and verified AI-generated content. Authors must disclose the use of AI Technologies when submitting their Material to a Wiley-published journal. If not provided, the journal may request this documentation. Transparency is essential to Wiley‚Äôs commitment to integrity in publishing. For information on how to declare your use of AI Technologies, review the Disclosure and declaration of AI use author FAQs.‚Äù\n\nReviewer policy: Journal does not have an explicit ‚ÄúReferee AI Statement‚Äù, but the he AFA Code explicitly forbids reviewers from using their ‚Äúadvanced access to unpublished research for their own purposes‚Äù which should cover sharing the manuscript with an LLM provider.\n\n\n\n\n\nSubmission policy: Submissions are governed by Elsevier‚Äôs policies. The publisher‚Äôs AI guidelines are here\n\nDisclosure requirement for Authors: ‚ÄúAuthors should disclose the use of AI Tools for manuscript preparation in a separate AI declaration statement in their manuscript upon submission and a statement will appear in the published work. Authors should document their use of AI, including the name of the AI Tool used, the purpose of the use, and the extent of their oversight. Declaring the use of AI Tools supports transparency and trust between authors, readers, reviewers, editors and contributors and facilitates compliance with the terms of use of the relevant AI Tool. Basic checks of grammar, spelling and punctuation need no declaration. AI use in the research process should be declared and described in detail in the methods section.‚Äù\n\nReviewer policy: Journal does not have an explicit ‚ÄúReferee AI Statement‚Äù but the Elsevier policy explicitly prohibits it!\n\n‚ÄúReviewers should not upload a submitted manuscript or any part of it into a generative AI tool as this may violate the authors‚Äô confidentiality and proprietary rights and, where the paper contains personally identifiable information, may breach data privacy rights.‚Äù\n\n\n\n\n\n\nSubmission policy: GenAI is discussed in the journal‚Äôs author guidelines. Its use is not explicitly prohibited:\n\n‚ÄúBy submitting the paper, the authors take full responsibility for the contents, and ensure they comply with the AFA Code of Professional Conduct and Ethics https://afajof.org/code-of-professional-conduct-and-ethics/, which includes plagiarism and broader academic‚Äëintegrity standards. The use of AI assistance does not absolve the authors from this responsibility. As this is a rapidly evolving space, these submission guidelines may also evolve going forward.‚Äù\n\nReviewer policy: GenAI use explicitly prohibited\n\n‚ÄúReviewers must not feed unpublished manuscript text, data, figures, or any supplementary files or materials into any tool that does not guarantee strict confidentiality, including an opt-out from model training to be utilized by the reviewer.‚Äù"
  },
  {
    "objectID": "research/discussion.html#journal-ai-policies",
    "href": "research/discussion.html#journal-ai-policies",
    "title": "Generative AI and Academic Research",
    "section": "",
    "text": "Submission policy: The journal does not have explicity AI guidelines for authors, but submissions are governed by Wiley‚Äôs policies. The publishers‚Äôs AI guidelines are here\n\nDisclosure requirement for Authors: ‚ÄúAuthors should maintain documentation of all AI Technology used, including its purpose, whether it impacted key arguments or conclusions, and how they personally reviewed and verified AI-generated content. Authors must disclose the use of AI Technologies when submitting their Material to a Wiley-published journal. If not provided, the journal may request this documentation. Transparency is essential to Wiley‚Äôs commitment to integrity in publishing. For information on how to declare your use of AI Technologies, review the Disclosure and declaration of AI use author FAQs.‚Äù\n\nReviewer policy: Journal does not have an explicit ‚ÄúReferee AI Statement‚Äù, but the he AFA Code explicitly forbids reviewers from using their ‚Äúadvanced access to unpublished research for their own purposes‚Äù which should cover sharing the manuscript with an LLM provider.\n\n\n\n\n\nSubmission policy: Submissions are governed by Elsevier‚Äôs policies. The publisher‚Äôs AI guidelines are here\n\nDisclosure requirement for Authors: ‚ÄúAuthors should disclose the use of AI Tools for manuscript preparation in a separate AI declaration statement in their manuscript upon submission and a statement will appear in the published work. Authors should document their use of AI, including the name of the AI Tool used, the purpose of the use, and the extent of their oversight. Declaring the use of AI Tools supports transparency and trust between authors, readers, reviewers, editors and contributors and facilitates compliance with the terms of use of the relevant AI Tool. Basic checks of grammar, spelling and punctuation need no declaration. AI use in the research process should be declared and described in detail in the methods section.‚Äù\n\nReviewer policy: Journal does not have an explicit ‚ÄúReferee AI Statement‚Äù but the Elsevier policy explicitly prohibits it!\n\n‚ÄúReviewers should not upload a submitted manuscript or any part of it into a generative AI tool as this may violate the authors‚Äô confidentiality and proprietary rights and, where the paper contains personally identifiable information, may breach data privacy rights.‚Äù\n\n\n\n\n\n\nSubmission policy: GenAI is discussed in the journal‚Äôs author guidelines. Its use is not explicitly prohibited:\n\n‚ÄúBy submitting the paper, the authors take full responsibility for the contents, and ensure they comply with the AFA Code of Professional Conduct and Ethics https://afajof.org/code-of-professional-conduct-and-ethics/, which includes plagiarism and broader academic‚Äëintegrity standards. The use of AI assistance does not absolve the authors from this responsibility. As this is a rapidly evolving space, these submission guidelines may also evolve going forward.‚Äù\n\nReviewer policy: GenAI use explicitly prohibited\n\n‚ÄúReviewers must not feed unpublished manuscript text, data, figures, or any supplementary files or materials into any tool that does not guarantee strict confidentiality, including an opt-out from model training to be utilized by the reviewer.‚Äù"
  },
  {
    "objectID": "research/tools.html",
    "href": "research/tools.html",
    "title": "GenAI Tools for Research",
    "section": "",
    "text": "DeepResearch tools are AI-powered research agents that automate complex literature review workflows. Available from multiple providers including OpenAI, Google Gemini, and Perplexity. These tools can interpret research questions, semantically search millions of papers, evaluate sources for relevance and quality, extract data, and synthesize findings into fully-cited reports.\nResearch use cases: Literature reviews, copy editing, double-checking proofs, and creating referee reports on your own papers before submission. Current limitations: though hallucinations are a lot less common, human oversight remains essential."
  },
  {
    "objectID": "research/tools.html#deepresearch",
    "href": "research/tools.html#deepresearch",
    "title": "GenAI Tools for Research",
    "section": "",
    "text": "DeepResearch tools are AI-powered research agents that automate complex literature review workflows. Available from multiple providers including OpenAI, Google Gemini, and Perplexity. These tools can interpret research questions, semantically search millions of papers, evaluate sources for relevance and quality, extract data, and synthesize findings into fully-cited reports.\nResearch use cases: Literature reviews, copy editing, double-checking proofs, and creating referee reports on your own papers before submission. Current limitations: though hallucinations are a lot less common, human oversight remains essential."
  },
  {
    "objectID": "research/tools.html#openai-prism",
    "href": "research/tools.html#openai-prism",
    "title": "GenAI Tools for Research",
    "section": "OpenAI Prism",
    "text": "OpenAI Prism\nOpenAI Prism is a free, LaTeX-native AI workspace for scientific writing (think Overleaf with ChatGPT embedded in it). Powered by GPT-5.2, Prism provides AI-assisted drafting and revision with full document context (equations, citations, figures), real-time collaboration with unlimited co-authors, built-in literature search from sources like arXiv, and voice-based editing. It can convert whiteboard sketches to LaTeX and provides automated proofreading and citation management.\nResearch use cases: Collaborate with co-authors on manuscripts with AI assistance embedded throughout the writing process. Free for anyone with a ChatGPT personal account; coming soon to Business, Enterprise, and Education plans."
  },
  {
    "objectID": "research/tools.html#vs-code-codex-extension",
    "href": "research/tools.html#vs-code-codex-extension",
    "title": "GenAI Tools for Research",
    "section": "VS Code + Codex Extension",
    "text": "VS Code + Codex Extension\nOpenAI Codex integrates with VS Code as an AI coding agent that provides real-time code suggestions, autonomous implementation of complex development tasks, natural language interaction with your codebase, and automated code review. Included with ChatGPT Plus, Pro, Business, and Enterprise plans.\nResearch use cases: Code generation, testing, debugging, and version control for computational research projects. Excellent alternatives include Cursor ($20/month) and Google‚Äôs Project Antigravity (included with Gemini Pro subscription, also $20/month)."
  },
  {
    "objectID": "research/tools.html#anaconda",
    "href": "research/tools.html#anaconda",
    "title": "GenAI Tools for Research",
    "section": "Anaconda",
    "text": "Anaconda\nThe easiest way to get started with Python programming. Anaconda is a comprehensive Python distribution and environment management system. It provides isolated environments with defined packages and dependencies, ensuring reproducible configurations across different projects without conflicts.\nResearch use cases: Give you Python and most commonly used packages for data science. The Anaconda Navigator GUI makes it accessible without command-line expertise, while the conda package manager provides access to over 33,000 AI, data science, and ML packages."
  },
  {
    "objectID": "research/tools.html#api-credits",
    "href": "research/tools.html#api-credits",
    "title": "GenAI Tools for Research",
    "section": "API Credits",
    "text": "API Credits\nProgrammatic access to LLMs allows you to integrate AI capabilities directly into your research workflows, scripts, and applications. Major providers (OpenAI, Anthropic, Google) offer their own APIs with pay-as-you-go pricing. Alternatively, OpenRouter provides a unified API interface to 300+ models from 60+ providers through a single endpoint, compatible with the OpenAI SDK format.\nResearch use cases: Building custom research tools, automating repetitive tasks (data cleaning, coding, text analysis), running batch experiments, and integrating AI into computational pipelines. Use your favorite model provider directly, or OpenRouter if you want flexibility to switch between models without changing your code."
  },
  {
    "objectID": "artifacts/ai_universities_strategies.html",
    "href": "artifacts/ai_universities_strategies.html",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "",
    "text": "This is a summary of a December 2025 blog post by Jay Akridge and David Hummels (Purdue University), the fifth in their series on AI, labor markets, and the university. The post synthesizes their earlier findings and offers concrete recommendations for how universities should respond to generative AI."
  },
  {
    "objectID": "artifacts/ai_universities_strategies.html#overview",
    "href": "artifacts/ai_universities_strategies.html#overview",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "",
    "text": "This is a summary of a December 2025 blog post by Jay Akridge and David Hummels (Purdue University), the fifth in their series on AI, labor markets, and the university. The post synthesizes their earlier findings and offers concrete recommendations for how universities should respond to generative AI."
  },
  {
    "objectID": "artifacts/ai_universities_strategies.html#key-findings-from-the-series",
    "href": "artifacts/ai_universities_strategies.html#key-findings-from-the-series",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "Key Findings from the Series",
    "text": "Key Findings from the Series\nThe authors distill six key insights from their research:\n\nGenAI predicts but doesn‚Äôt understand causality. It cannot form ‚Äúworld models‚Äù that describe causal mechanisms. Deep disciplinary knowledge is required to extract real value from AI.\nGenAI output has serious quality verification problems. It draws on the entire internet (authoritative sources and Reddit threads alike), produces fluent but meaningless text (‚Äúworkslop‚Äù), and hallucinates plausible-sounding but false content.\nFirm adoption is widespread but experimental. Companies don‚Äôt yet know how to use these tools effectively, and time/cost savings have been modest so far.\nTwo clear use patterns are emerging: information retrieval and tasks requiring explicit, codified knowledge. Unfortunately, many university assignments fall squarely in these categories.\nMost jobs are bundles of tasks. Even where AI can replace specific tasks, integrating those into workflows takes time. Workers who can solve well-defined problems and integrate solutions into larger contexts will be valuable.\nThe real value isn‚Äôt cost reduction‚Äîit‚Äôs creating new opportunities. The question for employers shifts from ‚Äúwhich workers can I lay off?‚Äù to ‚Äúwhat new value can these workers create with AI?‚Äù Universities should prepare students to answer that second question."
  },
  {
    "objectID": "artifacts/ai_universities_strategies.html#where-students-stand",
    "href": "artifacts/ai_universities_strategies.html#where-students-stand",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "Where Students Stand",
    "text": "Where Students Stand\nStudents are anxious. About 61% of the Class of 2026 are pessimistic about job prospects, with generative AI being a fast-growing reason for that pessimism.\nAI use among Gen Z is widespread (79% use GenAI tools, 47% regularly), but frequent use doesn‚Äôt mean comfort. Students see AI as helping them find information and complete work faster, but they‚Äôre genuinely worried about its impact on their own cognitive abilities‚Äîparticularly their capacity to search for information, evaluate sources, and generate new ideas."
  },
  {
    "objectID": "artifacts/ai_universities_strategies.html#curricular-recommendations",
    "href": "artifacts/ai_universities_strategies.html#curricular-recommendations",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "Curricular Recommendations",
    "text": "Curricular Recommendations\n\n1. More Experiential Learning\nThe core challenge: how do we make a 22-year-old graduate look like an experienced 27-year-old to employers, given that AI can now handle much of what entry-level workers used to do?\nStudents need repeated opportunities to:\n\nDefine problems (not just solve pre-defined ones)\nHypothesize potential solutions\nEvaluate and choose among alternatives\nCommunicate solutions and interpret feedback\nIntegrate multiple sources into a coherent ‚Äúproduct‚Äù\n\n\n\n2. Generating New Knowledge/New Value\nPablo Picasso: ‚ÄúComputers are useless. They only give you answers.‚Äù\nUniversities spend too little time getting students to ask their own questions and defend their solutions. GenAI might actually accelerate the process of moving students toward the knowledge frontier, enabling more ambitious independent work.\n\n\n3. Evaluating Source Validity\nGenAI puts information literacy problems ‚Äúon steroids‚Äù for three reasons:\n\nThe predictive process for generating text obscures underlying sources\nWhen prompted for sources, GenAI mixes authoritative and non-authoritative citations\nOften there is no real ‚Äúsource‚Äù‚Äîthe output is a statistical amalgamation or outright fabrication\n\nStudents need to become ‚Äúsource detectives.‚Äù\n\n\n4. Epistemology: What Counts as Evidence?\nEach discipline has its own standards of evidence. Universities need to be much more explicit about teaching students how we know what we know‚Äîhow questions are defined, how answers are arrived at, how claims are evaluated. Most students, if asked, would struggle to articulate this process.\n\n\n5. Show Students the Path from Day One\nStudents will ask: ‚ÄúJust let us use the AI tools already!‚Äù But using GenAI effectively without foundational knowledge is like getting into a cockpit for your first solo flight without ground training.\nUniversities must help students see from the start why building a worldview, understanding a discipline, and being information-literate are prerequisites for realizing GenAI‚Äôs potential."
  },
  {
    "objectID": "artifacts/ai_universities_strategies.html#implementation-challenges",
    "href": "artifacts/ai_universities_strategies.html#implementation-challenges",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "Implementation Challenges",
    "text": "Implementation Challenges\n\n1. Creating AI-Proof Assessments\nThis is a moving target. Last fall, asking students to draw graphs was a workaround; now GenAI handles graphs well. The deeper question: what experiences and assessments build learning under the assumption that students will always have these tools?\n\n\n2. Experiential Learning is Expensive\n500 students in a lecture hall with computer-graded exams is cheap. Experiential learning is not. Active learning approaches and Course-Based Undergraduate Research Experiences (CUREs) offer scalable alternatives.\n\n\n3. Student Readiness\nMany students are accustomed to regurgitating information, not open-ended inquiry. Honors students may thrive; helping the broader student body adapt will require careful planning and support.\n\n\n4. Centralized vs.¬†Decentralized AI Policies\nThe authors disagree here:\n\nAkridge (Provost perspective): Universities need some campus-wide AI guidance‚Äîbasic skills, ethical use, clear expectations‚Äîwhile preserving local latitude.\nHummels (Dean perspective): Course-level policies are sufficient. Students adapt to different rules in different contexts all the time. ‚ÄúJust read the damn syllabus!‚Äù\n\n\n\n5. Supporting Faculty\nWith GenAI still early in adoption, universities should encourage experimentation, extract lessons, and then systematically scale what works‚Äîusing stage-gate innovation models borrowed from industry.\n\n\n6. Staying Current with Employers\nUniversities can‚Äôt wait for a steady state. They need ongoing engagement with employers to understand how AI is changing what firms want from entry-level hires.\n\n\n7. Cost and Access to AI Tools\nFree access to powerful AI tools may not last. Developers are hemorrhaging money; monetization is coming. The result could be a two-tier system: some students on ‚Äúlow-rent chatbots,‚Äù others on ‚Äúgold-plated premium bots‚Äù‚Äîunless universities absorb the cost.\n\n\n8. AI and Student Mental Health\nSome users show evidence of AI-related psychoses and suicidal ideation. Once universities actively encourage AI use, student mental health implications need to be on the radar."
  },
  {
    "objectID": "artifacts/ai_universities_strategies.html#bottom-line",
    "href": "artifacts/ai_universities_strategies.html#bottom-line",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "Bottom Line",
    "text": "Bottom Line\nUniversities can fully engage in AI adoption‚Äîincluding helping firms figure out how to use AI productively‚Äîor they can watch from the sidelines. The authors strongly recommend the former. The latter is ‚Äúone more step toward being declared irrelevant.‚Äù"
  },
  {
    "objectID": "teaching/discussion.html",
    "href": "teaching/discussion.html",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "",
    "text": "This is a summary of a December 2025 blog post by Jay Akridge and David Hummels (Purdue University), the fifth in their series on AI, labor markets, and the university. The post synthesizes their earlier findings and offers concrete recommendations for how universities should respond to generative AI. If you have time, I strongly encourage you to read the original article and the posts linked therein."
  },
  {
    "objectID": "teaching/discussion.html#overview",
    "href": "teaching/discussion.html#overview",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "",
    "text": "This is a summary of a December 2025 blog post by Jay Akridge and David Hummels (Purdue University), the fifth in their series on AI, labor markets, and the university. The post synthesizes their earlier findings and offers concrete recommendations for how universities should respond to generative AI. If you have time, I strongly encourage you to read the original article and the posts linked therein."
  },
  {
    "objectID": "teaching/discussion.html#key-findings-from-the-series",
    "href": "teaching/discussion.html#key-findings-from-the-series",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "Key Findings from the Series",
    "text": "Key Findings from the Series\nThe authors distill six key insights from their research:\n\nGenAI can predict but doesn‚Äôt understand causality. It cannot form ‚Äúworld models‚Äù that describe causal mechanisms. Deep disciplinary knowledge is required to extract real value from AI.\nGenAI output has serious quality verification problems. It draws on the entire internet (authoritative sources and Reddit threads alike), can produce fluent but meaningless text (‚Äúworkslop‚Äù), and can hallucinate plausible-sounding but false content.\nFirm adoption is widespread but experimental. Companies don‚Äôt yet know how to use these tools effectively, and time/cost savings have been modest so far.\nTwo clear use patterns for GenAI are emerging: information retrieval and tasks requiring explicit, codified knowledge. Unfortunately, many university assignments fall squarely in these categories.\nMost jobs are bundles of tasks. Even where AI can replace specific tasks, integrating those into workflows takes time. Workers who can solve well-defined problems and integrate solutions into larger contexts will be valuable.\nThe real value isn‚Äôt cost reduction‚Äîit‚Äôs creating new opportunities. The question for employers shifts from ‚Äúwhich workers can I lay off?‚Äù to ‚Äúwhat new value can these workers create with AI?‚Äù Universities should prepare students to answer that second question."
  },
  {
    "objectID": "teaching/discussion.html#curricular-recommendations",
    "href": "teaching/discussion.html#curricular-recommendations",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "Curricular Recommendations",
    "text": "Curricular Recommendations\n\n1. More Experiential Learning\nThe core challenge: how do we make a 22-year-old graduate look like an experienced 27-year-old to employers, given that AI can now handle much of what entry-level workers used to do?\nStudents need repeated opportunities to:\n\nDefine problems (not just solve pre-defined ones)\nHypothesize potential solutions\nEvaluate and choose among alternatives\nCommunicate solutions and interpret feedback\nIntegrate multiple sources into a coherent ‚Äúproduct‚Äù\n\n\n\n2. Generating New Knowledge/New Value\nPablo Picasso: ‚ÄúComputers are useless. They only give you answers.‚Äù\nUniversities spend too little time getting students to ask their own questions and defend their solutions. GenAI might actually accelerate the process of moving students toward the knowledge frontier, enabling more ambitious independent work.\n\n\n3. Evaluating Source Validity\nGenAI puts information literacy problems ‚Äúon steroids‚Äù for three reasons:\n\nThe predictive process for generating text obscures underlying sources\nWhen prompted for sources, GenAI mixes authoritative and non-authoritative citations\nOften there is no real ‚Äúsource‚Äù‚Äîthe output is a statistical amalgamation or outright fabrication\n\nStudents need to become ‚Äúsource detectives.‚Äù\n\n\n4. Epistemology: What Counts as Evidence?\nEach discipline has its own standards of evidence. Universities need to be much more explicit about teaching students how we know what we know‚Äîhow questions are defined, how answers are arrived at, how claims are evaluated. Most students, if asked, would struggle to articulate this process.\n\n\n5. Show Students the Path from Day One\nStudents will ask: ‚ÄúJust let us use the AI tools already!‚Äù But using GenAI effectively without foundational knowledge is like getting into a cockpit for your first solo flight without ground training.\nUniversities must help students see from the start why building a worldview, understanding a discipline, and being information-literate are prerequisites for realizing GenAI‚Äôs potential."
  },
  {
    "objectID": "teaching/discussion.html#implementation-challenges",
    "href": "teaching/discussion.html#implementation-challenges",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "Implementation Challenges",
    "text": "Implementation Challenges\n\n1. Creating AI-Proof Assessments\nThis is a moving target. Last fall, asking students to draw graphs was a workaround; now GenAI handles graphs well. The deeper question: what experiences and assessments build learning under the assumption that students will always have these tools?\n\n\n2. Experiential Learning is Expensive\n500 students in a lecture hall with computer-graded exams is cheap. Experiential learning is not. Active learning approaches and Course-Based Undergraduate Research Experiences (CUREs) offer scalable alternatives.\n\n\n3. Student Readiness\nMany students are accustomed to regurgitating information, not open-ended inquiry. Honors students may thrive; helping the broader student body adapt will require careful planning and support.\n\n\n4. Centralized vs.¬†Decentralized AI Policies\nThe authors disagree here:\n\nAkridge (Provost perspective): Universities need some campus-wide AI guidance‚Äîbasic skills, ethical use, clear expectations‚Äîwhile preserving local latitude.\nHummels (Dean perspective): Course-level policies are sufficient. Students adapt to different rules in different contexts all the time. ‚ÄúJust read the damn syllabus!‚Äù\n\n\n\n5. Supporting Faculty\nWith GenAI still early in adoption, universities should encourage experimentation, extract lessons, and then systematically scale what works‚Äîusing stage-gate innovation models borrowed from industry.\n\n\n6. Staying Current with Employers\nUniversities can‚Äôt wait for a steady state. They need ongoing engagement with employers to understand how AI is changing what firms want from entry-level hires.\n\n\n7. Cost and Access to AI Tools\nFree access to powerful AI tools may not last. Developers are hemorrhaging money; monetization is coming. The result could be a two-tier system: some students on ‚Äúlow-rent chatbots,‚Äù others on ‚Äúgold-plated premium bots‚Äù‚Äîunless universities absorb the cost.\n\n\n8. AI and Student Mental Health\nSome users show evidence of AI-related psychoses and suicidal ideation. Once universities actively encourage AI use, student mental health implications need to be on the radar."
  },
  {
    "objectID": "teaching/discussion.html#bottom-line",
    "href": "teaching/discussion.html#bottom-line",
    "title": "AI and Universities: Strategies for a Time of Rapid Change",
    "section": "Bottom Line",
    "text": "Bottom Line\nUniversities can fully engage in AI adoption‚Äîincluding helping firms figure out how to use AI productively‚Äîor they can watch from the sidelines. The authors strongly recommend the former. The latter is ‚Äúone more step toward being declared irrelevant.‚Äù"
  },
  {
    "objectID": "fundamentals/key-capabilities.html",
    "href": "fundamentals/key-capabilities.html",
    "title": "Key GenAI Capabilities",
    "section": "",
    "text": "Bigger windows help with long syllabi, papers, transcripts, cases, and multi-file projects.\nBut bigger windows do not guarantee accurate long-document reasoning.\n\nExamples of publicly stated context sizes (as of late 2025):\n\nOpenAI‚Äôs GPT‚Äë5.2 Pro API model: 400k context window, 128k max output (API docs).\n\nChatGPT UI (GPT‚Äë5.2): context varies by tier/model (e.g., ‚ÄúThinking‚Äù has a much larger window than ‚ÄúInstant‚Äù).\n\nGoogle Gemini 2.5 Pro: announced 1M token context.\n\nAnthropic Claude long-context tiers include models with up to 1M tokens.\n\n\n\n\nAsk:\n\nCan it read charts, tables, and screenshots?\nCan it generate images or just analyze them?\nIs multimodality native (one model) or stitched (separate encoders + LLM)?\n\nFor finance, multimodality matters for:\n\nreading charts in papers,\ninterpreting slide decks,\nextracting data from PDFs and tables,\nunderstanding UI screenshots from trading or analytics tools.\n\n\n\n\nMany providers now offer a mode that:\n\nspends more compute to deliberate,\nperforms better on multi-step tasks,\nis slower and more expensive.\n\nOpenAI‚Äôs GPT‚Äë5.2 ‚ÄúAuto‚Äù can switch between Instant and Thinking; the UI can show a ‚Äúslimmed-down‚Äù view of chain-of-thought, with an ‚ÄúAnswer now‚Äù option.\n\n\n\nA model with tool access can:\n\nbrowse the web (and cite sources),\nuse a Python environment for calculations,\nanalyze files,\ngenerate spreadsheets / slide decks,\ncall external tools through connectors or ‚Äúactions.‚Äù\n\nWithout tools, models are limited to:\n\ntheir training data,\nand whatever you provide in the prompt.\n\n\n\n\n‚ÄúDeep research‚Äù typically means an agentic workflow:\n\nit searches,\ncollects sources,\nsynthesizes,\nand returns citations and an organized report.\n\nThis is usually not a separate ‚Äúbrain‚Äù; it‚Äôs an agent layer on top of a strong model + tools.\n\n\n\nCoding assistants aren‚Äôt only for programmers. Even if you don‚Äôt write software, they can:\n\ntranslate between Stata/R/Python,\nwrite reproducible scripts,\ngenerate data cleaning code,\nand explain unfamiliar code you inherited.\n\nOpenAI‚Äôs Codex ecosystem is one example (web/CLI/IDE integrations)."
  },
  {
    "objectID": "fundamentals/key-capabilities.html#key-ways-models-differ-what-to-pay-attention-to",
    "href": "fundamentals/key-capabilities.html#key-ways-models-differ-what-to-pay-attention-to",
    "title": "Key GenAI Capabilities",
    "section": "",
    "text": "Bigger windows help with long syllabi, papers, transcripts, cases, and multi-file projects.\nBut bigger windows do not guarantee accurate long-document reasoning.\n\nExamples of publicly stated context sizes (as of late 2025):\n\nOpenAI‚Äôs GPT‚Äë5.2 Pro API model: 400k context window, 128k max output (API docs).\n\nChatGPT UI (GPT‚Äë5.2): context varies by tier/model (e.g., ‚ÄúThinking‚Äù has a much larger window than ‚ÄúInstant‚Äù).\n\nGoogle Gemini 2.5 Pro: announced 1M token context.\n\nAnthropic Claude long-context tiers include models with up to 1M tokens.\n\n\n\n\nAsk:\n\nCan it read charts, tables, and screenshots?\nCan it generate images or just analyze them?\nIs multimodality native (one model) or stitched (separate encoders + LLM)?\n\nFor finance, multimodality matters for:\n\nreading charts in papers,\ninterpreting slide decks,\nextracting data from PDFs and tables,\nunderstanding UI screenshots from trading or analytics tools.\n\n\n\n\nMany providers now offer a mode that:\n\nspends more compute to deliberate,\nperforms better on multi-step tasks,\nis slower and more expensive.\n\nOpenAI‚Äôs GPT‚Äë5.2 ‚ÄúAuto‚Äù can switch between Instant and Thinking; the UI can show a ‚Äúslimmed-down‚Äù view of chain-of-thought, with an ‚ÄúAnswer now‚Äù option.\n\n\n\nA model with tool access can:\n\nbrowse the web (and cite sources),\nuse a Python environment for calculations,\nanalyze files,\ngenerate spreadsheets / slide decks,\ncall external tools through connectors or ‚Äúactions.‚Äù\n\nWithout tools, models are limited to:\n\ntheir training data,\nand whatever you provide in the prompt.\n\n\n\n\n‚ÄúDeep research‚Äù typically means an agentic workflow:\n\nit searches,\ncollects sources,\nsynthesizes,\nand returns citations and an organized report.\n\nThis is usually not a separate ‚Äúbrain‚Äù; it‚Äôs an agent layer on top of a strong model + tools.\n\n\n\nCoding assistants aren‚Äôt only for programmers. Even if you don‚Äôt write software, they can:\n\ntranslate between Stata/R/Python,\nwrite reproducible scripts,\ngenerate data cleaning code,\nand explain unfamiliar code you inherited.\n\nOpenAI‚Äôs Codex ecosystem is one example (web/CLI/IDE integrations)."
  },
  {
    "objectID": "fundamentals/how-llms-work.html",
    "href": "fundamentals/how-llms-work.html",
    "title": "How GenAI Models Work",
    "section": "",
    "text": "When you type a question into ChatGPT or Claude, here‚Äôs what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into ‚Äútokens‚Äù‚Äînot exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a ‚Äúsystem prompt‚Äù (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model‚Äôs neural network. Each layer does mathematical operations‚Äîessentially matrix multiplications‚Äîtransforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by ‚Äútemperature‚Äù), adds that token to the sequence, and repeats until it ends up sampling a ‚Äústop‚Äù token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a ‚Äústop‚Äù token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/how-llms-work.html#what-happens-when-you-prompt",
    "href": "fundamentals/how-llms-work.html#what-happens-when-you-prompt",
    "title": "How GenAI Models Work",
    "section": "",
    "text": "When you type a question into ChatGPT or Claude, here‚Äôs what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into ‚Äútokens‚Äù‚Äînot exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a ‚Äúsystem prompt‚Äù (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model‚Äôs neural network. Each layer does mathematical operations‚Äîessentially matrix multiplications‚Äîtransforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by ‚Äútemperature‚Äù), adds that token to the sequence, and repeats until it ends up sampling a ‚Äústop‚Äù token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a ‚Äústop‚Äù token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/how-llms-work.html#the-model",
    "href": "fundamentals/how-llms-work.html#the-model",
    "title": "How GenAI Models Work",
    "section": "The Model",
    "text": "The Model\nA large language model is just a file full of numbers.\nWhen companies like OpenAI, Anthropic, or Google train a model, they‚Äôre creating a very large file‚Äîhundreds of gigabytes for frontier models‚Äîthat contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.\nThat is what the model ‚Äúknows‚Äù‚Äîstatistical patterns about how language works and what tends to follow what.\n\nHow the Numbers Are Obtained\nThe training process happens in stages:\nPre-training: The model reads massive amounts of text from the internet, books, code repositories, academic papers‚Äîessentially a substantial portion of human written output. During this phase, it learns to predict ‚Äúwhat word comes next?‚Äù billions of times. The parameters adjust to get better at prediction. This takes months and costs tens to hundreds of millions of dollars in compute.\nPost-training (Fine-tuning): The base model is then adjusted using:\n\nSupervised fine-tuning (SFT): Humans write example conversations showing how the model should respond\nReinforcement learning from human feedback (RLHF): Humans rate model outputs, and the model learns to generate outputs that get higher ratings\nConstitutional AI (CAI): Used by Anthropic, where the model learns to critique and revise its own outputs based on principles\n\nContinuous refinement: Models are updated regularly to improve performance, fix issues, and add capabilities.\nKey terms you might hear: ‚Äútransformer architecture‚Äù (the mathematical structure these models use), ‚Äúattention mechanism‚Äù (how the model decides what parts of the input to focus on), ‚Äúparameters‚Äù or ‚Äúweights‚Äù (the numbers in the file)."
  },
  {
    "objectID": "fundamentals/how-llms-work.html#the-context-window",
    "href": "fundamentals/how-llms-work.html#the-context-window",
    "title": "How GenAI Models Work",
    "section": "The Context Window",
    "text": "The Context Window\nThe context window is the maximum amount of text (and other inputs) the model can consider at once:\n(system instructions + your prompt + chat history + retrieved snippets + tool outputs + file excerpts) + the model‚Äôs output.\nIf the total exceeds the limit:\n\nthe system must drop, summarize, or ‚Äúcompact‚Äù something,\nor it returns an error (depending on provider).\n\n\nWhat providers may automatically include in the context window (even if you don‚Äôt see it)\nMany modern assistants can add extra context such as:\n\nsystem safety instructions,\nyour custom instructions,\nproject instructions,\n‚Äúmemory‚Äù items (facts it saved about your preferences),\nsnippets retrieved from uploaded files or connected tools.\n\nThis is why two people can ask the ‚Äúsame prompt‚Äù and get slightly different results.\n\n\nUploading files with your prompt\nIn most systems, uploading a file does not mean the entire file is placed into the context window verbatim.\nA common approach is retrieval:\n\nthe file is chunked,\nthe system builds embeddings (a searchable representation) from each chunk\nand only relevant chunks are pulled into the context window when needed (based on what you asked about the file)\n\n\n\nWhat happens when you near the limit of the context window\nDifferent providers handle this differently:\n\nSome UIs summarize older messages.\nSome drop the earliest conversation turns (‚Äútruncation‚Äù).\nSome run a compaction step to preserve key details.\n\n\n\nOutput limits\nEven with a very large input window, models have output caps. When you ask for ‚Äúwrite 30 pages,‚Äù you usually get:\n\na truncated response,\nor a refusal,\nor ‚Äúhere is an outline; ask me to expand section by section.‚Äù\n\nRecommendation:\nFor long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2‚Ä¶\nThis yields better control and fewer errors."
  },
  {
    "objectID": "fundamentals/how-llms-work.html#thinking-planning-models-and-tool-use",
    "href": "fundamentals/how-llms-work.html#thinking-planning-models-and-tool-use",
    "title": "How GenAI Models Work",
    "section": "‚ÄúThinking‚Äù (Planning) Models and Tool Use",
    "text": "‚ÄúThinking‚Äù (Planning) Models and Tool Use\n\nThinking Models\nStandard LLMs generate output token-by-token in a single pass‚Äîthey don‚Äôt ‚Äústop and think.‚Äù Thinking models (like OpenAI‚Äôs o1/o3, Claude with extended thinking, or DeepSeek R1) add an explicit reasoning phase before producing their final answer.\nHow it works:\n\nThe model receives your prompt\nBefore answering, it generates a chain of thought‚Äîa series of reasoning steps that break down the problem\n\nIn modern systems, this thinking is visible to you and it‚Äôs a good idea to inspect it and see if you can spot issues with it\nFor some producs like ChatGPT Pro Deep Research, you can pause the thinking/execution loop if you want to correct some reasoning errors or want to add more context\n\n\nThe model can reconsider, catch errors, and refine its approach during this phase\nOnly after thinking does it produce the final response\n\nTrade-offs: Thinking models are better at complex reasoning (math, logic, multi-step problems) but are slower and more expensive because they generate many more tokens internally.\n\n\nTool Use\nLLMs can only generate tokens‚Äîthey can‚Äôt browse the web, run code, or access databases. ‚ÄúTool use‚Äù (also called ‚Äúfunction calling‚Äù) extends models to interact with external systems.\nHow it works:\n\nThe system prompt tells the model what tools are available (e.g., ‚ÄúYou can search the web‚Äù or ‚ÄúYou can run Python code‚Äù)\nInstead of generating text, the model can output a structured tool call (e.g., search(\"current weather in NYC\"), where search is a Python function somewhere on an OpenAI server)\nThe system executes the tool and returns the result to the model\nThe model incorporates the result into its context and continues generating\nThis loop can repeat‚Äîmodels can chain multiple tool calls to accomplish complex tasks\n\nWhy it matters: Tool use lets models access current information (web search), verify calculations (code execution), and take actions in the real world (APIs). It transforms LLMs from text generators into agents that can accomplish tasks.\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\\n+ Available Tools\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph \"LLM Output\"\n        F{\"Output\\nType?\"}\n        T[\"Thinking\\nTokens\"]\n        G[\"Tool Call\"]\n        H[\"Final\\nResponse\"]\n    end\n    \n    subgraph Tools\n        X[\"Execute Tool\"]\n        R[\"Tool Result\"]\n    end\n    \n    A --&gt; D\n    S --&gt; D\n    D --&gt; E --&gt; F\n    F --&gt;|\"Reasoning\"| T\n    T --&gt;|\"Append\"| D\n    F --&gt;|\"Tool\"| G --&gt; X --&gt; R\n    R --&gt;|\"Append\"| D\n    F --&gt;|\"Done\"| H\n    linkStyle 5 stroke-dasharray: 5 5\n    linkStyle 9 stroke-dasharray: 5 5\n\n\n\n\n\nThe key difference from the basic pipeline (at the top of this page): thinking models generate reasoning tokens as output that gets appended to the context, creating a loop where the model ‚Äúthinks out loud‚Äù before producing its final answer. Tool-enabled models can similarly pause, call external tools, and incorporate results before continuing."
  },
  {
    "objectID": "fundamentals/choosing-tools.html",
    "href": "fundamentals/choosing-tools.html",
    "title": "Choosing the Right Tool",
    "section": "",
    "text": "With dozens of GenAI tools available, selecting the right one for your needs can be overwhelming. This guide provides a framework for making informed choices."
  },
  {
    "objectID": "fundamentals/choosing-tools.html#introduction",
    "href": "fundamentals/choosing-tools.html#introduction",
    "title": "Choosing the Right Tool",
    "section": "",
    "text": "With dozens of GenAI tools available, selecting the right one for your needs can be overwhelming. This guide provides a framework for making informed choices."
  },
  {
    "objectID": "fundamentals/choosing-tools.html#decision-framework",
    "href": "fundamentals/choosing-tools.html#decision-framework",
    "title": "Choosing the Right Tool",
    "section": "Decision Framework",
    "text": "Decision Framework\n\n1. Define Your Use Case\nStart by clearly identifying what you need:\n\nTask type: Writing, coding, analysis, research?\nInput/output: Text only, or images/files too?\nFrequency: One-off task or ongoing usage?\nIntegration: Standalone or part of a workflow?\n\n\n\n2. Evaluate Key Factors\n\nCapability Match\n\n\n\nUse Case\nRecommended Tools\n\n\n\n\nGeneral writing\nChatGPT, Claude, Gemini\n\n\nCode development\nGitHub Copilot, Cursor, Claude\n\n\nResearch & search\nPerplexity, Gemini\n\n\nImage generation\nNano Banana Pro, Midjourney, Stable Diffusion\n\n\nDocument analysis\nClaude (long context), Gemini\n\n\n\n\n\nCost Considerations\n\n\n\n\n\n\nPricing Models\n\n\n\n\nFree tiers: Limited usage, good for exploration\nSubscriptions: $20/month typical for premium access\nAPI pricing: Pay per token, scales with usage\nEnterprise: Custom pricing with additional features\n\n\n\n\n\nPrivacy & Security\nConsider data handling requirements:\n\nSensitive data: May require local/on-premise solutions\nCompliance: Check vendor certifications (SOC 2, etc.)\nData retention: Understand how inputs are stored/used\nTraining opt-out: Some providers allow opting out of training"
  },
  {
    "objectID": "fundamentals/choosing-tools.html#tool-categories",
    "href": "fundamentals/choosing-tools.html#tool-categories",
    "title": "Choosing the Right Tool",
    "section": "Tool Categories",
    "text": "Tool Categories\n\nConversational Assistants\nBest for: General tasks, brainstorming, Q&A\n\nChatGPT (OpenAI)\nClaude (Anthropic)\nGemini (Google)\n\n\n\nCode Assistants\nBest for: Development workflows\n\nGitHub Copilot ‚Äî IDE integration\nCursor ‚Äî AI-native editor\nCodeium ‚Äî Free alternative\n\n\n\nSearch & Research\nBest for: Information retrieval, citations\n\nPerplexity ‚Äî AI search with sources\nGemini ‚Äî Google integration\nConsensus ‚Äî Academic research"
  },
  {
    "objectID": "fundamentals/choosing-tools.html#making-the-choice",
    "href": "fundamentals/choosing-tools.html#making-the-choice",
    "title": "Choosing the Right Tool",
    "section": "Making the Choice",
    "text": "Making the Choice\n\n\n\n\n\n\nRecommendation\n\n\n\nStart with a general-purpose tool (ChatGPT or Claude) to understand your needs, then specialize if necessary. Most users find 2-3 tools cover their needs.\n\n\n\nQuick Selection Guide\nNeed general assistance? ‚Üí ChatGPT or Claude\nNeed to search/research? ‚Üí Perplexity\nNeed code help in IDE? ‚Üí GitHub Copilot or Cursor\nNeed image generation? ‚Üí Midjourney or DALL-E\nNeed document analysis? ‚Üí Claude (long context)\nNeed privacy/local? ‚Üí Llama-based solutions"
  },
  {
    "objectID": "fundamentals/01_ecosystem.html",
    "href": "fundamentals/01_ecosystem.html",
    "title": "The GenAI Ecosystem",
    "section": "",
    "text": "When people say ‚ÄúAI,‚Äù or ‚ÄúGenAI,‚Äù they often mean ‚ÄúChatGPT‚Äù or a similar chat interface. It‚Äôs more useful to think of GenAI in layers:\n\nModels -&gt; the ‚Äúbrains‚Äù:\n\nGPT‚Äë5.2, Claude Opus 4.5, Gemini 3.0 Pro, Grok 4.1, etc.\n\nThese models are frequently referred to as LLMs (large language models). This is somewhat of a misnomer these days since the latest iterations of these models can handle more than just language (text) as an input. They can also process audio, image, and video inputs.\n\n\n\nTools -&gt; capabilities around the model:\n\nWeb search\nCode execution\nDatabase queries\nConnectors to other software/platforms like file systems, email clients, browsers, IDEs, CRMs, messaging apps etc\n\nFacilitated through APIs (application programming interfaces), and MCP (model context protocol) servers\n\n\nProducts (interfaces):\n\nWeb, desktop, and mobile apps from model providers: ChatGPT, Claude, Gemini, Grok\nCoding agents: Codex, Claude Code, Gemini Code Assist, GitHub Copilot\nThird-party apps: Perplexity, Replit, Cursor\n\n\nMany of these products use agentic workflows (agents) in the background (workflows orchestrated by a model) - The model plans steps, uses tools, analyzes the output of those tools, and iterates until it produces the requested output\n\n\n\nImage generated with Nano Banana Pro"
  },
  {
    "objectID": "fundamentals/01_ecosystem.html#framing-what-i-mean-by-genai",
    "href": "fundamentals/01_ecosystem.html#framing-what-i-mean-by-genai",
    "title": "The GenAI Ecosystem",
    "section": "",
    "text": "When people say ‚ÄúAI,‚Äù or ‚ÄúGenAI,‚Äù they often mean ‚ÄúChatGPT‚Äù or a similar chat interface. It‚Äôs more useful to think of GenAI in layers:\n\nModels -&gt; the ‚Äúbrains‚Äù:\n\nGPT‚Äë5.2, Claude Opus 4.5, Gemini 3.0 Pro, Grok 4.1, etc.\n\nThese models are frequently referred to as LLMs (large language models). This is somewhat of a misnomer these days since the latest iterations of these models can handle more than just language (text) as an input. They can also process audio, image, and video inputs.\n\n\n\nTools -&gt; capabilities around the model:\n\nWeb search\nCode execution\nDatabase queries\nConnectors to other software/platforms like file systems, email clients, browsers, IDEs, CRMs, messaging apps etc\n\nFacilitated through APIs (application programming interfaces), and MCP (model context protocol) servers\n\n\nProducts (interfaces):\n\nWeb, desktop, and mobile apps from model providers: ChatGPT, Claude, Gemini, Grok\nCoding agents: Codex, Claude Code, Gemini Code Assist, GitHub Copilot\nThird-party apps: Perplexity, Replit, Cursor\n\n\nMany of these products use agentic workflows (agents) in the background (workflows orchestrated by a model) - The model plans steps, uses tools, analyzes the output of those tools, and iterates until it produces the requested output\n\n\n\nImage generated with Nano Banana Pro"
  },
  {
    "objectID": "fundamentals/01_ecosystem.html#why-the-genai-as-a-stack-framing-matters",
    "href": "fundamentals/01_ecosystem.html#why-the-genai-as-a-stack-framing-matters",
    "title": "The GenAI Ecosystem",
    "section": "Why the ‚ÄúGenAI as a stack‚Äù framing matters",
    "text": "Why the ‚ÄúGenAI as a stack‚Äù framing matters\nIt reminds us that:\n\nJumps in capability are not just about better models, but better models + better tools + better agentic workflows.\nDifferent products may be better at different tasks (sometimes in a way that is not necessarily related to the models they use underneath)\n\nFor example, Claude (product) may be better than ChatGPT (product) at generating Excel sheets because of better integration with Microsoft Office (tool), not because Claude Opus 4.5 (model) is better than GPT 5.2 (model)\n\nYou should not be judging the current capabilities of ‚ÄúGenAI‚Äù based on your experiences with a single product (ChatGPT for most people)"
  }
]