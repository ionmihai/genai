[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "DeepLearning.AI — Generative AI for Everyone — Excellent intro course by Andrew Ng\nGoogle — Generative AI Learning Path — Comprehensive Google Cloud training\nPrompt Engineering Guide — Community resource for prompting techniques\n\n\n\n\n\n\n\nProvider\nDocumentation\n\n\n\n\nOpenAI\nplatform.openai.com/docs\n\n\nAnthropic\ndocs.anthropic.com\n\n\nGoogle\nai.google.dev"
  },
  {
    "objectID": "resources.html#learning-resources",
    "href": "resources.html#learning-resources",
    "title": "Resources",
    "section": "",
    "text": "DeepLearning.AI — Generative AI for Everyone — Excellent intro course by Andrew Ng\nGoogle — Generative AI Learning Path — Comprehensive Google Cloud training\nPrompt Engineering Guide — Community resource for prompting techniques\n\n\n\n\n\n\n\nProvider\nDocumentation\n\n\n\n\nOpenAI\nplatform.openai.com/docs\n\n\nAnthropic\ndocs.anthropic.com\n\n\nGoogle\nai.google.dev"
  },
  {
    "objectID": "resources.html#tools-platforms",
    "href": "resources.html#tools-platforms",
    "title": "Resources",
    "section": "2 Tools & Platforms",
    "text": "2 Tools & Platforms\n\n2.1 Conversational AI\n\nChatGPT — OpenAI’s flagship interface\nClaude — Anthropic’s assistant\nGemini — Google’s AI assistant\nPerplexity — AI-powered search\n\n\n\n2.2 Development Tools\n\nGitHub Copilot — AI pair programmer\nCursor — AI-first code editor\nReplit — AI-assisted coding platform\n\n\n\n2.3 Image Generation\n\nMidjourney — High-quality image generation\nDALL-E — OpenAI’s image model\nStable Diffusion — Open-source image generation"
  },
  {
    "objectID": "resources.html#research-news",
    "href": "resources.html#research-news",
    "title": "Resources",
    "section": "3 Research & News",
    "text": "3 Research & News\n\n3.1 Stay Updated\n\nThe Batch — Weekly AI newsletter by DeepLearning.AI\nImport AI — Weekly AI developments newsletter\nAI News — Daily AI news digest\n\n\n\n3.2 Research\n\narXiv AI — Latest AI research papers\nPapers With Code — Research with implementations\nHugging Face — Models, datasets, and community"
  },
  {
    "objectID": "resources.html#ethics-safety",
    "href": "resources.html#ethics-safety",
    "title": "Resources",
    "section": "4 Ethics & Safety",
    "text": "4 Ethics & Safety\n\n4.1 Guidelines & Frameworks\n\nNIST AI Risk Management Framework\nEU AI Act Overview\nResponsible AI Practices"
  },
  {
    "objectID": "fundamentals/04_interfaces.html",
    "href": "fundamentals/04_interfaces.html",
    "title": "Generative AI Interfaces",
    "section": "",
    "text": "ChatGPT\nClaude\nGemini"
  },
  {
    "objectID": "fundamentals/04_interfaces.html#web-interfaces-from-model-provides",
    "href": "fundamentals/04_interfaces.html#web-interfaces-from-model-provides",
    "title": "Generative AI Interfaces",
    "section": "",
    "text": "ChatGPT\nClaude\nGemini"
  },
  {
    "objectID": "fundamentals/04_interfaces.html#application-programming-interfaces-apis",
    "href": "fundamentals/04_interfaces.html#application-programming-interfaces-apis",
    "title": "Generative AI Interfaces",
    "section": "2 Application Programming Interfaces (APIs)",
    "text": "2 Application Programming Interfaces (APIs)\n\nLinks to API docs"
  },
  {
    "objectID": "fundamentals/04_interfaces.html#third-party-applications",
    "href": "fundamentals/04_interfaces.html#third-party-applications",
    "title": "Generative AI Interfaces",
    "section": "3 Third-Party Applications",
    "text": "3 Third-Party Applications\n\nCursor\nPerplexity"
  },
  {
    "objectID": "fundamentals/choosing-tools.html",
    "href": "fundamentals/choosing-tools.html",
    "title": "Choosing the Right Tool",
    "section": "",
    "text": "With dozens of GenAI tools available, selecting the right one for your needs can be overwhelming. This guide provides a framework for making informed choices."
  },
  {
    "objectID": "fundamentals/choosing-tools.html#introduction",
    "href": "fundamentals/choosing-tools.html#introduction",
    "title": "Choosing the Right Tool",
    "section": "",
    "text": "With dozens of GenAI tools available, selecting the right one for your needs can be overwhelming. This guide provides a framework for making informed choices."
  },
  {
    "objectID": "fundamentals/choosing-tools.html#decision-framework",
    "href": "fundamentals/choosing-tools.html#decision-framework",
    "title": "Choosing the Right Tool",
    "section": "2 Decision Framework",
    "text": "2 Decision Framework\n\n2.1 1. Define Your Use Case\nStart by clearly identifying what you need:\n\nTask type: Writing, coding, analysis, research?\nInput/output: Text only, or images/files too?\nFrequency: One-off task or ongoing usage?\nIntegration: Standalone or part of a workflow?\n\n\n\n2.2 2. Evaluate Key Factors\n\n2.2.1 Capability Match\n\n\n\nUse Case\nRecommended Tools\n\n\n\n\nGeneral writing\nChatGPT, Claude, Gemini\n\n\nCode development\nGitHub Copilot, Cursor, Claude\n\n\nResearch & search\nPerplexity, Gemini\n\n\nImage generation\nNano Banana Pro, Midjourney, Stable Diffusion\n\n\nDocument analysis\nClaude (long context), Gemini\n\n\n\n\n\n2.2.2 Cost Considerations\n\n\n\n\n\n\nPricing Models\n\n\n\n\nFree tiers: Limited usage, good for exploration\nSubscriptions: $20/month typical for premium access\nAPI pricing: Pay per token, scales with usage\nEnterprise: Custom pricing with additional features\n\n\n\n\n\n2.2.3 Privacy & Security\nConsider data handling requirements:\n\nSensitive data: May require local/on-premise solutions\nCompliance: Check vendor certifications (SOC 2, etc.)\nData retention: Understand how inputs are stored/used\nTraining opt-out: Some providers allow opting out of training"
  },
  {
    "objectID": "fundamentals/choosing-tools.html#tool-categories",
    "href": "fundamentals/choosing-tools.html#tool-categories",
    "title": "Choosing the Right Tool",
    "section": "3 Tool Categories",
    "text": "3 Tool Categories\n\n3.1 Conversational Assistants\nBest for: General tasks, brainstorming, Q&A\n\nChatGPT (OpenAI)\nClaude (Anthropic)\nGemini (Google)\n\n\n\n3.2 Code Assistants\nBest for: Development workflows\n\nGitHub Copilot — IDE integration\nCursor — AI-native editor\nCodeium — Free alternative\n\n\n\n3.3 Search & Research\nBest for: Information retrieval, citations\n\nPerplexity — AI search with sources\nGemini — Google integration\nConsensus — Academic research"
  },
  {
    "objectID": "fundamentals/choosing-tools.html#making-the-choice",
    "href": "fundamentals/choosing-tools.html#making-the-choice",
    "title": "Choosing the Right Tool",
    "section": "4 Making the Choice",
    "text": "4 Making the Choice\n\n\n\n\n\n\nRecommendation\n\n\n\nStart with a general-purpose tool (ChatGPT or Claude) to understand your needs, then specialize if necessary. Most users find 2-3 tools cover their needs.\n\n\n\n4.1 Quick Selection Guide\nNeed general assistance? → ChatGPT or Claude\nNeed to search/research? → Perplexity\nNeed code help in IDE? → GitHub Copilot or Cursor\nNeed image generation? → Midjourney or DALL-E\nNeed document analysis? → Claude (long context)\nNeed privacy/local? → Llama-based solutions"
  },
  {
    "objectID": "fundamentals/02_models.html",
    "href": "fundamentals/02_models.html",
    "title": "Generative AI Models",
    "section": "",
    "text": "When you type a question into ChatGPT or Claude, here’s what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into “tokens”—not exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a “system prompt” (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model’s neural network. Each layer does mathematical operations—essentially matrix multiplications—transforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by “temperature”), adds that token to the sequence, and repeats until it ends up sampling a “stop” token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a “stop” token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/02_models.html#what-happens-when-you-prompt",
    "href": "fundamentals/02_models.html#what-happens-when-you-prompt",
    "title": "Generative AI Models",
    "section": "",
    "text": "When you type a question into ChatGPT or Claude, here’s what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into “tokens”—not exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a “system prompt” (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model’s neural network. Each layer does mathematical operations—essentially matrix multiplications—transforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by “temperature”), adds that token to the sequence, and repeats until it ends up sampling a “stop” token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a “stop” token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/02_models.html#what-is-an-llm",
    "href": "fundamentals/02_models.html#what-is-an-llm",
    "title": "Generative AI Models",
    "section": "2 What is an LLM?",
    "text": "2 What is an LLM?\nA large language model is just a file full of numbers.\nWhen companies like OpenAI, Anthropic, or Google train a model, they’re creating a very large file—hundreds of gigabytes for frontier models—that contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.\nThat is what the model “knows”—statistical patterns about how language works and what tends to follow what.\n\n2.1 How the Numbers Are Obtained\nThe training process happens in stages:\nPre-training: The model reads massive amounts of text from the internet, books, code repositories, academic papers—essentially a substantial portion of human written output. During this phase, it learns to predict “what word comes next?” billions of times. The parameters adjust to get better at prediction. This takes months and costs tens to hundreds of millions of dollars in compute.\nPost-training (Fine-tuning): The base model is then adjusted using:\n\nSupervised fine-tuning (SFT): Humans write example conversations showing how the model should respond\nReinforcement learning from human feedback (RLHF): Humans rate model outputs, and the model learns to generate outputs that get higher ratings\nConstitutional AI (CAI): Used by Anthropic, where the model learns to critique and revise its own outputs based on principles\n\nContinuous refinement: Models are updated regularly to improve performance, fix issues, and add capabilities.\nKey terms you might hear: “transformer architecture” (the mathematical structure these models use), “attention mechanism” (how the model decides what parts of the input to focus on), “parameters” or “weights” (the numbers in the file)."
  },
  {
    "objectID": "fundamentals/02_models.html#the-context-window",
    "href": "fundamentals/02_models.html#the-context-window",
    "title": "Generative AI Models",
    "section": "3 The Context Window",
    "text": "3 The Context Window\nThe context window is the maximum amount of text (and other inputs) the model can consider at once:\n(system instructions + your prompt + chat history + retrieved snippets + tool outputs + file excerpts) + the model’s output.\nIf the total exceeds the limit:\n\nthe system must drop, summarize, or “compact” something,\nor it returns an error (depending on provider).\n\n\n3.1 What providers may automatically include in the context window (even if you don’t see it)\nMany modern assistants can add extra context such as:\n\nsystem safety instructions,\nyour custom instructions,\nproject instructions,\n“memory” items (facts it saved about your preferences),\nsnippets retrieved from uploaded files or connected tools.\n\nThis is why two people can ask the “same prompt” and get slightly different results.\n\n\n3.2 Uploading files with your prompt\nIn most systems, uploading a file does not mean the entire file is placed into the context window verbatim.\nA common approach is retrieval:\n\nthe file is chunked,\nthe system builds embeddings (a searchable representation) from each chunk\nand only relevant chunks are pulled into the context window when needed (based on what you asked about the file)\n\n\n\n3.3 What happens when you near the limit of the context window\nDifferent providers handle this differently:\n\nSome UIs summarize older messages.\nSome drop the earliest conversation turns (“truncation”).\nSome run a compaction step to preserve key details.\n\n\n\n3.4 Output limits\nEven with a very large input window, models have output caps. When you ask for “write 30 pages,” you usually get:\n\na truncated response,\nor a refusal,\nor “here is an outline; ask me to expand section by section.”\n\nRecommendation:\nFor long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2…\nThis yields better control and fewer errors."
  },
  {
    "objectID": "fundamentals/02_models.html#frontier-genai-models-are-multimodal",
    "href": "fundamentals/02_models.html#frontier-genai-models-are-multimodal",
    "title": "Generative AI Models",
    "section": "4 Frontier GenAI Models are Multimodal",
    "text": "4 Frontier GenAI Models are Multimodal\nModels like GPT 5.2 and Gemini 3.0 are multimodal: they can input and output text, audio, images, and videos.\nTODO:\n\nexplain that they still input and output tokens (just with a different representation)\nmention names of specialized models like Nano Banana Pro, and Sora (anything for speach?) and how they are integrated with the main models (GPT 5.2, Gemini 3.0 etc.)"
  },
  {
    "objectID": "fundamentals/02_models.html#main-shortcomings",
    "href": "fundamentals/02_models.html#main-shortcomings",
    "title": "Generative AI Models",
    "section": "5 Main Shortcomings",
    "text": "5 Main Shortcomings\n\nLong-term memory (context window is short-term memory)\nHalucinations\nInability to act (exert change on external systems)\n\nThe “Agents” pages describes modern advances in addressing these shortcomings."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GenAI for Teaching and Research",
    "section": "",
    "text": "This website is designed to provide an introduction to Generative AI (GenAI) technologies, with a focus on practical understanding and effective utilization of these tools for teaching and research. The goal of the website is to provide a repository of resources that academics can use to:\n\nUnderstand the fundamental concepts behind generative AI and large language models\nNavigate the current landscape of GenAI tools and platforms\nIdentify key capabilities and limitations of different GenAI systems\nSelect appropriate tools for various use cases\nApply best practices for effective GenAI utilization"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "GenAI for Teaching and Research",
    "section": "",
    "text": "This website is designed to provide an introduction to Generative AI (GenAI) technologies, with a focus on practical understanding and effective utilization of these tools for teaching and research. The goal of the website is to provide a repository of resources that academics can use to:\n\nUnderstand the fundamental concepts behind generative AI and large language models\nNavigate the current landscape of GenAI tools and platforms\nIdentify key capabilities and limitations of different GenAI systems\nSelect appropriate tools for various use cases\nApply best practices for effective GenAI utilization"
  },
  {
    "objectID": "index.html#safety-and-privacy",
    "href": "index.html#safety-and-privacy",
    "title": "GenAI for Teaching and Research",
    "section": "2 Safety and privacy",
    "text": "2 Safety and privacy\nBefore we talk about capabilities, we need shared norms. The biggest risks of GenAI in education and research are not “the models are too powerful,” but rather misplaced trust and unsafe data handling.\n\nDon’t upload sensitive data into consumer tools unless your institution has an approved, protected environment (“Enterprise” level subscriptions usually offer this).\n\nExamples of sensitive data: identifiable student info (FERPA), unpublished manuscripts, reviewer comments, proprietary datasets, IRB-protected data, confidential employer partner data.\n\nVerification is a skill, not an afterthought.\n\nTreat the model like a smart research/teaching assistant: helpful, fast, and occasionally wrong.\n\nWe will not debate whether students “should” use AI. They already are. The more productive conversation revolves around questions like:\n\nWhat learning outcomes do we want, in a world where GenAI can perform many of the tasks we traditionally teach our students?\nHow do we design assessments that reward thinking/understanding?"
  },
  {
    "objectID": "fundamentals/key-capabilities.html",
    "href": "fundamentals/key-capabilities.html",
    "title": "Key GenAI Capabilities",
    "section": "",
    "text": "Bigger windows help with long syllabi, papers, transcripts, cases, and multi-file projects.\nBut bigger windows do not guarantee accurate long-document reasoning.\n\nExamples of publicly stated context sizes (as of late 2025):\n\nOpenAI’s GPT‑5.2 Pro API model: 400k context window, 128k max output (API docs).\n\nChatGPT UI (GPT‑5.2): context varies by tier/model (e.g., “Thinking” has a much larger window than “Instant”).\n\nGoogle Gemini 2.5 Pro: announced 1M token context.\n\nAnthropic Claude long-context tiers include models with up to 1M tokens.\n\n\n\n\nAsk:\n\nCan it read charts, tables, and screenshots?\nCan it generate images or just analyze them?\nIs multimodality native (one model) or stitched (separate encoders + LLM)?\n\nFor finance, multimodality matters for:\n\nreading charts in papers,\ninterpreting slide decks,\nextracting data from PDFs and tables,\nunderstanding UI screenshots from trading or analytics tools.\n\n\n\n\nMany providers now offer a mode that:\n\nspends more compute to deliberate,\nperforms better on multi-step tasks,\nis slower and more expensive.\n\nOpenAI’s GPT‑5.2 “Auto” can switch between Instant and Thinking; the UI can show a “slimmed-down” view of chain-of-thought, with an “Answer now” option.\n\n\n\nA model with tool access can:\n\nbrowse the web (and cite sources),\nuse a Python environment for calculations,\nanalyze files,\ngenerate spreadsheets / slide decks,\ncall external tools through connectors or “actions.”\n\nWithout tools, models are limited to:\n\ntheir training data,\nand whatever you provide in the prompt.\n\n\n\n\n“Deep research” typically means an agentic workflow:\n\nit searches,\ncollects sources,\nsynthesizes,\nand returns citations and an organized report.\n\nThis is usually not a separate “brain”; it’s an agent layer on top of a strong model + tools.\n\n\n\nCoding assistants aren’t only for programmers. Even if you don’t write software, they can:\n\ntranslate between Stata/R/Python,\nwrite reproducible scripts,\ngenerate data cleaning code,\nand explain unfamiliar code you inherited.\n\nOpenAI’s Codex ecosystem is one example (web/CLI/IDE integrations)."
  },
  {
    "objectID": "fundamentals/key-capabilities.html#key-ways-models-differ-what-to-pay-attention-to",
    "href": "fundamentals/key-capabilities.html#key-ways-models-differ-what-to-pay-attention-to",
    "title": "Key GenAI Capabilities",
    "section": "",
    "text": "Bigger windows help with long syllabi, papers, transcripts, cases, and multi-file projects.\nBut bigger windows do not guarantee accurate long-document reasoning.\n\nExamples of publicly stated context sizes (as of late 2025):\n\nOpenAI’s GPT‑5.2 Pro API model: 400k context window, 128k max output (API docs).\n\nChatGPT UI (GPT‑5.2): context varies by tier/model (e.g., “Thinking” has a much larger window than “Instant”).\n\nGoogle Gemini 2.5 Pro: announced 1M token context.\n\nAnthropic Claude long-context tiers include models with up to 1M tokens.\n\n\n\n\nAsk:\n\nCan it read charts, tables, and screenshots?\nCan it generate images or just analyze them?\nIs multimodality native (one model) or stitched (separate encoders + LLM)?\n\nFor finance, multimodality matters for:\n\nreading charts in papers,\ninterpreting slide decks,\nextracting data from PDFs and tables,\nunderstanding UI screenshots from trading or analytics tools.\n\n\n\n\nMany providers now offer a mode that:\n\nspends more compute to deliberate,\nperforms better on multi-step tasks,\nis slower and more expensive.\n\nOpenAI’s GPT‑5.2 “Auto” can switch between Instant and Thinking; the UI can show a “slimmed-down” view of chain-of-thought, with an “Answer now” option.\n\n\n\nA model with tool access can:\n\nbrowse the web (and cite sources),\nuse a Python environment for calculations,\nanalyze files,\ngenerate spreadsheets / slide decks,\ncall external tools through connectors or “actions.”\n\nWithout tools, models are limited to:\n\ntheir training data,\nand whatever you provide in the prompt.\n\n\n\n\n“Deep research” typically means an agentic workflow:\n\nit searches,\ncollects sources,\nsynthesizes,\nand returns citations and an organized report.\n\nThis is usually not a separate “brain”; it’s an agent layer on top of a strong model + tools.\n\n\n\nCoding assistants aren’t only for programmers. Even if you don’t write software, they can:\n\ntranslate between Stata/R/Python,\nwrite reproducible scripts,\ngenerate data cleaning code,\nand explain unfamiliar code you inherited.\n\nOpenAI’s Codex ecosystem is one example (web/CLI/IDE integrations)."
  },
  {
    "objectID": "fundamentals/how-llms-work.html",
    "href": "fundamentals/how-llms-work.html",
    "title": "How GenAI Models Work",
    "section": "",
    "text": "When you type a question into ChatGPT or Claude, here’s what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into “tokens”—not exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a “system prompt” (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model’s neural network. Each layer does mathematical operations—essentially matrix multiplications—transforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by “temperature”), adds that token to the sequence, and repeats until it ends up sampling a “stop” token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a “stop” token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/how-llms-work.html#what-happens-when-you-prompt",
    "href": "fundamentals/how-llms-work.html#what-happens-when-you-prompt",
    "title": "How GenAI Models Work",
    "section": "",
    "text": "When you type a question into ChatGPT or Claude, here’s what actually happens:\n\nTokenization: Your prompt is converted into a sequence of numbers. Text is split into “tokens”—not exactly words, but chunks. Common words are single tokens; rare words get split into pieces. Each token maps to a number (ID) the model can process. Each model has its own vocabulary of tokens.\nContext assembly: Your prompt is combined with any previous messages in the conversation, plus a “system prompt” (instructions the model provider includes automatically).\n\nContext Window: The model sees all tokens in the context at once (no memory between subsequent prompts). Window size limits how much text fits in a single interaction. Larger windows = more capability but higher cost.\n\nForward pass: The tokenized input goes through the model’s neural network. Each layer does mathematical operations—essentially matrix multiplications—transforming the input through the billions of parameters.\nOutput generation: The model produces a probability distribution over all possible next tokens in its vocabulary. It samples from this distribution (with some randomness controlled by “temperature”), adds that token to the sequence, and repeats until it ends up sampling a “stop” token.\n\nTemperature: Controls randomness in selection:\n\nTemperature = 0: Always pick the highest-probability token (deterministic)\nTemperature = 1: Sample proportionally to probabilities\n\nTemperature &gt; 1: Flatten the distribution (more random/creative)\n\n\nDetokenization: Once a “stop” token is reached, the output tokens are converted back to human-readable text.\n\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\"]\n    end\n    \n    subgraph Tokenization\n        B[\"Tokenizer\"]\n        C[\"Token IDs\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph Output\n        F{\"Probability\\nDistribution\"}\n        G[\"Temperature\"]\n        H[\"Selected Token\"]\n    end\n    \n    A --&gt; B\n    S --&gt; B\n    B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H\n    H --&gt;|\"Append & repeat\"| D\n    linkStyle 8 stroke-dasharray: 5 5"
  },
  {
    "objectID": "fundamentals/how-llms-work.html#the-model",
    "href": "fundamentals/how-llms-work.html#the-model",
    "title": "How GenAI Models Work",
    "section": "2 The Model",
    "text": "2 The Model\nA large language model is just a file full of numbers.\nWhen companies like OpenAI, Anthropic, or Google train a model, they’re creating a very large file—hundreds of gigabytes for frontier models—that contains billions of numerical parameters. These parameters encode patterns learned from reading enormous amounts of text.\nThat is what the model “knows”—statistical patterns about how language works and what tends to follow what.\n\n2.1 How the Numbers Are Obtained\nThe training process happens in stages:\nPre-training: The model reads massive amounts of text from the internet, books, code repositories, academic papers—essentially a substantial portion of human written output. During this phase, it learns to predict “what word comes next?” billions of times. The parameters adjust to get better at prediction. This takes months and costs tens to hundreds of millions of dollars in compute.\nPost-training (Fine-tuning): The base model is then adjusted using:\n\nSupervised fine-tuning (SFT): Humans write example conversations showing how the model should respond\nReinforcement learning from human feedback (RLHF): Humans rate model outputs, and the model learns to generate outputs that get higher ratings\nConstitutional AI (CAI): Used by Anthropic, where the model learns to critique and revise its own outputs based on principles\n\nContinuous refinement: Models are updated regularly to improve performance, fix issues, and add capabilities.\nKey terms you might hear: “transformer architecture” (the mathematical structure these models use), “attention mechanism” (how the model decides what parts of the input to focus on), “parameters” or “weights” (the numbers in the file)."
  },
  {
    "objectID": "fundamentals/how-llms-work.html#the-context-window",
    "href": "fundamentals/how-llms-work.html#the-context-window",
    "title": "How GenAI Models Work",
    "section": "3 The Context Window",
    "text": "3 The Context Window\nThe context window is the maximum amount of text (and other inputs) the model can consider at once:\n(system instructions + your prompt + chat history + retrieved snippets + tool outputs + file excerpts) + the model’s output.\nIf the total exceeds the limit:\n\nthe system must drop, summarize, or “compact” something,\nor it returns an error (depending on provider).\n\n\n3.1 What providers may automatically include in the context window (even if you don’t see it)\nMany modern assistants can add extra context such as:\n\nsystem safety instructions,\nyour custom instructions,\nproject instructions,\n“memory” items (facts it saved about your preferences),\nsnippets retrieved from uploaded files or connected tools.\n\nThis is why two people can ask the “same prompt” and get slightly different results.\n\n\n3.2 Uploading files with your prompt\nIn most systems, uploading a file does not mean the entire file is placed into the context window verbatim.\nA common approach is retrieval:\n\nthe file is chunked,\nthe system builds embeddings (a searchable representation) from each chunk\nand only relevant chunks are pulled into the context window when needed (based on what you asked about the file)\n\n\n\n3.3 What happens when you near the limit of the context window\nDifferent providers handle this differently:\n\nSome UIs summarize older messages.\nSome drop the earliest conversation turns (“truncation”).\nSome run a compaction step to preserve key details.\n\n\n\n3.4 Output limits\nEven with a very large input window, models have output caps. When you ask for “write 30 pages,” you usually get:\n\na truncated response,\nor a refusal,\nor “here is an outline; ask me to expand section by section.”\n\nRecommendation:\nFor long outputs, ask the model to 1) write an outline, 2) draft section 1, 3) draft section 2…\nThis yields better control and fewer errors."
  },
  {
    "objectID": "fundamentals/how-llms-work.html#thinking-planning-models-and-tool-use",
    "href": "fundamentals/how-llms-work.html#thinking-planning-models-and-tool-use",
    "title": "How GenAI Models Work",
    "section": "4 “Thinking” (Planning) Models and Tool Use",
    "text": "4 “Thinking” (Planning) Models and Tool Use\n\n4.1 Thinking Models\nStandard LLMs generate output token-by-token in a single pass—they don’t “stop and think.” Thinking models (like OpenAI’s o1/o3, Claude with extended thinking, or DeepSeek R1) add an explicit reasoning phase before producing their final answer.\nHow it works:\n\nThe model receives your prompt\nBefore answering, it generates a chain of thought—a series of reasoning steps that break down the problem\n\nIn modern systems, this thinking is visible to you and it’s a good idea to inspect it and see if you can spot issues with it\nFor some producs like ChatGPT Pro Deep Research, you can pause the thinking/execution loop if you want to correct some reasoning errors or want to add more context\n\n\nThe model can reconsider, catch errors, and refine its approach during this phase\nOnly after thinking does it produce the final response\n\nTrade-offs: Thinking models are better at complex reasoning (math, logic, multi-step problems) but are slower and more expensive because they generate many more tokens internally.\n\n\n4.2 Tool Use\nLLMs can only generate tokens—they can’t browse the web, run code, or access databases. “Tool use” (also called “function calling”) extends models to interact with external systems.\nHow it works:\n\nThe system prompt tells the model what tools are available (e.g., “You can search the web” or “You can run Python code”)\nInstead of generating text, the model can output a structured tool call (e.g., search(\"current weather in NYC\"), where search is a Python function somewhere on an OpenAI server)\nThe system executes the tool and returns the result to the model\nThe model incorporates the result into its context and continues generating\nThis loop can repeat—models can chain multiple tool calls to accomplish complex tasks\n\nWhy it matters: Tool use lets models access current information (web search), verify calculations (code execution), and take actions in the real world (APIs). It transforms LLMs from text generators into agents that can accomplish tasks.\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\\n+ Available Tools\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph \"LLM Output\"\n        F{\"Output\\nType?\"}\n        T[\"Thinking\\nTokens\"]\n        G[\"Tool Call\"]\n        H[\"Final\\nResponse\"]\n    end\n    \n    subgraph Tools\n        X[\"Execute Tool\"]\n        R[\"Tool Result\"]\n    end\n    \n    A --&gt; D\n    S --&gt; D\n    D --&gt; E --&gt; F\n    F --&gt;|\"Reasoning\"| T\n    T --&gt;|\"Append\"| D\n    F --&gt;|\"Tool\"| G --&gt; X --&gt; R\n    R --&gt;|\"Append\"| D\n    F --&gt;|\"Done\"| H\n    linkStyle 5 stroke-dasharray: 5 5\n    linkStyle 9 stroke-dasharray: 5 5\n\n\n\n\n\nThe key difference from the basic pipeline (at the top of this page): thinking models generate reasoning tokens as output that gets appended to the context, creating a loop where the model “thinks out loud” before producing its final answer. Tool-enabled models can similarly pause, call external tools, and incorporate results before continuing."
  },
  {
    "objectID": "fundamentals/03_agents.html",
    "href": "fundamentals/03_agents.html",
    "title": "Generative AI Agents",
    "section": "",
    "text": "Standard LLMs generate output token-by-token in a single pass—they don’t “stop and think.” Thinking models (like OpenAI’s o1/o3, Claude with extended thinking, or DeepSeek R1) add an explicit reasoning phase before producing their final answer.\nHow it works:\n\nThe model receives your prompt\nBefore answering, it generates a chain of thought—a series of reasoning steps that break down the problem\n\nIn modern systems, this thinking is visible to you and it’s a good idea to inspect it and see if you can spot issues with it\nFor some producs like ChatGPT Pro Deep Research, you can pause the thinking/execution loop if you want to correct some reasoning errors or want to add more context\n\n\nThe model can reconsider, catch errors, and refine its approach during this phase\nOnly after thinking does it produce the final response\n\nTrade-offs: Thinking models are better at complex reasoning (math, logic, multi-step problems) but are slower and more expensive because they generate many more tokens internally.\n\n\n\nLLMs can only generate tokens—they can’t browse the web, run code, or access databases. “Tool use” (also called “function calling”) extends models to interact with external systems.\nHow it works:\n\nThe system prompt tells the model what tools are available (e.g., “You can search the web” or “You can run Python code”)\nInstead of generating text, the model can output a structured tool call (e.g., search(\"current weather in NYC\"), where search is a Python function somewhere on an OpenAI server)\nThe system executes the tool and returns the result to the model\nThe model incorporates the result into its context and continues generating\nThis loop can repeat—models can chain multiple tool calls to accomplish complex tasks\n\nWhy it matters: Tool use lets models access current information (web search), verify calculations (code execution), and take actions in the real world (APIs). It transforms LLMs from text generators into agents that can accomplish tasks.\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\\n+ Available Tools\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph \"LLM Output\"\n        F{\"Output\\nType?\"}\n        T[\"Thinking\\nTokens\"]\n        G[\"Tool Call\"]\n        H[\"Final\\nResponse\"]\n    end\n    \n    subgraph Tools\n        X[\"Execute Tool\"]\n        R[\"Tool Result\"]\n    end\n    \n    A --&gt; D\n    S --&gt; D\n    D --&gt; E --&gt; F\n    F --&gt;|\"Reasoning\"| T\n    T --&gt;|\"Append\"| D\n    F --&gt;|\"Tool\"| G --&gt; X --&gt; R\n    R --&gt;|\"Append\"| D\n    F --&gt;|\"Done\"| H\n    linkStyle 5 stroke-dasharray: 5 5\n    linkStyle 9 stroke-dasharray: 5 5\n\n\n\n\n\nThe key difference from the basic pipeline: thinking models generate reasoning tokens as output that gets appended to the context, creating a loop where the model “thinks out loud” before producing its final answer. Tool-enabled models can similarly pause, call external tools, and incorporate results before continuing."
  },
  {
    "objectID": "fundamentals/03_agents.html#thinking-planning-models-and-tool-use",
    "href": "fundamentals/03_agents.html#thinking-planning-models-and-tool-use",
    "title": "Generative AI Agents",
    "section": "",
    "text": "Standard LLMs generate output token-by-token in a single pass—they don’t “stop and think.” Thinking models (like OpenAI’s o1/o3, Claude with extended thinking, or DeepSeek R1) add an explicit reasoning phase before producing their final answer.\nHow it works:\n\nThe model receives your prompt\nBefore answering, it generates a chain of thought—a series of reasoning steps that break down the problem\n\nIn modern systems, this thinking is visible to you and it’s a good idea to inspect it and see if you can spot issues with it\nFor some producs like ChatGPT Pro Deep Research, you can pause the thinking/execution loop if you want to correct some reasoning errors or want to add more context\n\n\nThe model can reconsider, catch errors, and refine its approach during this phase\nOnly after thinking does it produce the final response\n\nTrade-offs: Thinking models are better at complex reasoning (math, logic, multi-step problems) but are slower and more expensive because they generate many more tokens internally.\n\n\n\nLLMs can only generate tokens—they can’t browse the web, run code, or access databases. “Tool use” (also called “function calling”) extends models to interact with external systems.\nHow it works:\n\nThe system prompt tells the model what tools are available (e.g., “You can search the web” or “You can run Python code”)\nInstead of generating text, the model can output a structured tool call (e.g., search(\"current weather in NYC\"), where search is a Python function somewhere on an OpenAI server)\nThe system executes the tool and returns the result to the model\nThe model incorporates the result into its context and continues generating\nThis loop can repeat—models can chain multiple tool calls to accomplish complex tasks\n\nWhy it matters: Tool use lets models access current information (web search), verify calculations (code execution), and take actions in the real world (APIs). It transforms LLMs from text generators into agents that can accomplish tasks.\n\n\n\n\nflowchart LR\n    subgraph Input\n        A[\"User Prompt\"]\n        S[\"System Prompt\\n+ Available Tools\"]\n    end\n    \n    subgraph Processing\n        D[\"Context Window\"]\n        E[\"LLM\"]\n    end\n    \n    subgraph \"LLM Output\"\n        F{\"Output\\nType?\"}\n        T[\"Thinking\\nTokens\"]\n        G[\"Tool Call\"]\n        H[\"Final\\nResponse\"]\n    end\n    \n    subgraph Tools\n        X[\"Execute Tool\"]\n        R[\"Tool Result\"]\n    end\n    \n    A --&gt; D\n    S --&gt; D\n    D --&gt; E --&gt; F\n    F --&gt;|\"Reasoning\"| T\n    T --&gt;|\"Append\"| D\n    F --&gt;|\"Tool\"| G --&gt; X --&gt; R\n    R --&gt;|\"Append\"| D\n    F --&gt;|\"Done\"| H\n    linkStyle 5 stroke-dasharray: 5 5\n    linkStyle 9 stroke-dasharray: 5 5\n\n\n\n\n\nThe key difference from the basic pipeline: thinking models generate reasoning tokens as output that gets appended to the context, creating a loop where the model “thinks out loud” before producing its final answer. Tool-enabled models can similarly pause, call external tools, and incorporate results before continuing."
  },
  {
    "objectID": "fundamentals/03_agents.html#how-agentic-workflows-address-model-shortcomings",
    "href": "fundamentals/03_agents.html#how-agentic-workflows-address-model-shortcomings",
    "title": "Generative AI Agents",
    "section": "2 How Agentic Workflows Address Model Shortcomings",
    "text": "2 How Agentic Workflows Address Model Shortcomings\n\n2.1 Long-term memory\n\nTools to access databases (including vector databases)\n\n\n\n2.2 Halucinations\n\nWeb search\nRAG\n\n\n\n2.3 Ability to act\n\nConnectors to external systems (e.g. via function calling or MCP servers)"
  },
  {
    "objectID": "fundamentals/01_ecosystem.html",
    "href": "fundamentals/01_ecosystem.html",
    "title": "The GenAI Ecosystem",
    "section": "",
    "text": "When people say “AI,” or “GenAI,” they often mean “ChatGPT” or a similar chat interface. It’s more useful to think of GenAI in layers:\n\nModels -&gt; the “brains”:\n\nGPT‑5.2, Claude Opus 4.5, Gemini 3.0 Pro, Grok 4.1, etc.\n\nThese models are frequently referred to as LLMs (large language models). This is somewhat of a misnomer these days since the latest iterations of these models can handle more than just language (text) as an input. They can also process audio, image, and video inputs.\n\n\n\nTools -&gt; capabilities around the model:\n\nweb search\nPython interpreter\nfile analysis\nspreadsheet/slides generation\nconnectors to other software/platforms like Outlook, Gmail, Dropbox, Google Drive, GitHub, etc.\n\nProducts (interfaces):\n\nWeb: ChatGPT, Claude, Gemini, Grok\nCoding agents: Codex, Claude Code, Gemini Code Assist, GitHub Copilot\nApps: Perplexity, Replit, Cursor, NotebookLM\n\nMany of these products use agents in the background (workflows orchestrated by a model)\n\nThe model plans steps, uses tools, analyzes the output of those tools, and iterates until it produces the requested output\n\n\n\n\n\n\n\nImage generated with Nano Banana Pro"
  },
  {
    "objectID": "fundamentals/01_ecosystem.html#framing-what-i-mean-by-genai",
    "href": "fundamentals/01_ecosystem.html#framing-what-i-mean-by-genai",
    "title": "The GenAI Ecosystem",
    "section": "",
    "text": "When people say “AI,” or “GenAI,” they often mean “ChatGPT” or a similar chat interface. It’s more useful to think of GenAI in layers:\n\nModels -&gt; the “brains”:\n\nGPT‑5.2, Claude Opus 4.5, Gemini 3.0 Pro, Grok 4.1, etc.\n\nThese models are frequently referred to as LLMs (large language models). This is somewhat of a misnomer these days since the latest iterations of these models can handle more than just language (text) as an input. They can also process audio, image, and video inputs.\n\n\n\nTools -&gt; capabilities around the model:\n\nweb search\nPython interpreter\nfile analysis\nspreadsheet/slides generation\nconnectors to other software/platforms like Outlook, Gmail, Dropbox, Google Drive, GitHub, etc.\n\nProducts (interfaces):\n\nWeb: ChatGPT, Claude, Gemini, Grok\nCoding agents: Codex, Claude Code, Gemini Code Assist, GitHub Copilot\nApps: Perplexity, Replit, Cursor, NotebookLM\n\nMany of these products use agents in the background (workflows orchestrated by a model)\n\nThe model plans steps, uses tools, analyzes the output of those tools, and iterates until it produces the requested output\n\n\n\n\n\n\n\nImage generated with Nano Banana Pro"
  },
  {
    "objectID": "fundamentals/01_ecosystem.html#why-the-genai-as-a-stack-framing-matters",
    "href": "fundamentals/01_ecosystem.html#why-the-genai-as-a-stack-framing-matters",
    "title": "The GenAI Ecosystem",
    "section": "2 Why the “GenAI as a stack” framing matters",
    "text": "2 Why the “GenAI as a stack” framing matters\nIt reminds us that:\n\nJumps in capability are not just about better models, but better models + better tools + better agentic workflows.\nDifferent products may be better at different tasks (sometimes in a way that is not necessarily related to the models they use underneath)\n\nFor example, Claude (product) may be better than ChatGPT (product) at generating Excel sheets because of better integration with Microsoft Office (tool), not because Claude Opus 4.5 (model) is better than GPT 5.2 (model)\n\nYou should not be judging the current capabilities of “GenAI” based on your experiences with a single product (ChatGPT for most people)"
  }
]